{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb673dc",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f57e85",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ba6a74-1033-44ad-ac91-930843f720db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models import Encoder, Decoder\n",
    "\n",
    "from pathlib import Path\n",
    "from DatasetInterface import MSCOCOInterface, get_loader\n",
    "from data_prep_utils import *\n",
    "from utils import train, save_model, load_model, plot_loss\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "###### download the data we need\n",
    "# !cd ~/INM706-image-captioning/Datasets/coco/images/\n",
    "# !wget http://images.cocodataset.org/zips/train2017.zip\n",
    "# !wget http://images.cocodataset.org/zips/val2017.zip\n",
    "# !unzip train2017.zip\n",
    "# !unzip val2017.zip\n",
    "# !rm train2017.zip\n",
    "# !rm val2017.zip\n",
    "\n",
    "##### run code below if nltk hasn't been set up in clound instance yet\n",
    "# !python -m nltk.downloader -d /usr/local/share/nltk_data all\n",
    "\n",
    "###### run code below to save pre-trained weights if needed\n",
    "# cd ~/INM706-image-captioning/model\n",
    "# !wget https://download.pytorch.org/models/resnet152-394f9c45.pth\n",
    "# !mv resnet152-394f9c45.pth resnet152_model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaa079",
   "metadata": {},
   "source": [
    "## Load Dataset Interface and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2472cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('Datasets/coco')\n",
    "imgs_path = root/'images'/'train2017'\n",
    "imgs_path_test = root/'images'/'val2017'\n",
    "\n",
    "\n",
    "# prepare_datasets(train_percent = 0.87, super_categories=None,\n",
    "#                  max_train=45000, max_val=9000, max_test=3000)\n",
    "\n",
    "#### build vocab using full original coco train. Uncomment to run\n",
    "# build_vocab(freq_threshold=2, sequence_length=40,\n",
    "#             captions_file='captions_train2017.json')\n",
    "\n",
    "train_captions_path = root/'annotations'/'custom_captions_train.json'\n",
    "val_captions_path = root/'annotations'/'custom_captions_val.json'\n",
    "test_captions_path = root/'annotations'/'custom_captions_test.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab72e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to boost the performence of CUDA use:\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8143132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interface_params = {\n",
    "    'imgs_path': imgs_path,\n",
    "    'captions_path': train_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    # 'sequence_length': 20,\n",
    "    'caps_per_img': 5,\n",
    "    # 'stage': \"train\",\n",
    "    'vocab_from_file': True\n",
    "}\n",
    "\n",
    "val_interface_params = {\n",
    "    'imgs_path': imgs_path,\n",
    "    'captions_path': val_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    # 'sequence_length': 20,\n",
    "    'caps_per_img': 1,\n",
    "    # 'stage': \"validation\",\n",
    "    'vocab_from_file': True\n",
    "}\n",
    "\n",
    "test_interface_params = {\n",
    "    'imgs_path': imgs_path_test,\n",
    "    'captions_path': test_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    # 'sequence_length': 20,\n",
    "    'caps_per_img': 1,\n",
    "    # 'stage': \"test\",\n",
    "    'vocab_from_file': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8237cb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vocab size is 16232\n",
      "####################\n",
      "\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225000/225000 [00:22<00:00, 10006.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vocab size is 16232\n",
      "####################\n",
      "\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [00:01<00:00, 8710.12it/s] \n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# # Training Interface\n",
    "# coco_interface_train = MSCOCOInterface(**train_interface_params)\n",
    "\n",
    "# # Validation Interface\n",
    "# coco_interface_val = MSCOCOInterface(**val_interface_params)\n",
    "\n",
    "train_loader = get_loader(**train_interface_params, batch_size=batch_size)\n",
    "val_loader = get_loader(**val_interface_params, batch_size=batch_size)\n",
    "# train_loader = data.DataLoader(coco_interface_train, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = data.DataLoader(coco_interface_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f38c73a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training captions: 225000\n",
      "Validation captions: 9000\n"
     ]
    }
   ],
   "source": [
    "print(\"training captions: {}\\nValidation captions: {}\"\n",
    "      .format(len(train_loader.dataset), len(val_loader.dataset)))\n",
    "\n",
    "# print(f\"Length of vocabulary: {len(coco_interface_train.idx_to_string)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b144ce97-baa5-4af7-b9ce-cd9615874272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32]) torch.Size([32, 3, 256, 256]) torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "idx, im, cap = next(iter(train_loader))\n",
    "print(idx.shape, im.shape, cap.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a85bb9",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46e041f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "hidden_size = 512\n",
    "vocab_size = len(train_loader.dataset.idx_to_string)\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52f24c",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc0b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################READY########################################\n"
     ]
    }
   ],
   "source": [
    "# pretrained = False does use a pretrained resnet but loads \n",
    "# from local .pth file \n",
    "encoder = Encoder(embed_size=embed_size, pretrained=True, model_weight_path=\"./model/resnet152_model.pth\")\n",
    "decoder = Decoder(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size, num_layers=num_layers)\n",
    "print(\"########################################READY########################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f6a4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "# commenting out ignore pad as we are not padding now\n",
    "criterion = nn.CrossEntropyLoss(#ignore_index=train_loader.dataset.string_to_index[\"<PAD>\"]\n",
    "                                )\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436dff8a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c8e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, encoder, decoder, training_loss, validation_loss, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'training_loss': training_loss,\n",
    "        'validation_loss': validation_loss\n",
    "    }, checkpoint_path)\n",
    "\n",
    "def load_model(encoder, decoder, checkpoint_path):\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    training_loss = checkpoint['training_loss']\n",
    "    validation_loss = checkpoint['validation_loss']\n",
    "\n",
    "    return encoder, decoder, training_loss, validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de69f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/image_captioning_model_v8.pth file does not exist, training startging from scratch\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = './model/image_captioning_model_v8.pth'\n",
    "if Path(CHECKPOINT).exists():\n",
    "    encoder, decoder, training_loss, validation_loss = load_model(encoder, decoder, CHECKPOINT)\n",
    "else:\n",
    "    print(f'{CHECKPOINT} file does not exist, training startging from scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67994682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train_step = math.ceil(len(train_loader.dataset.caption_lengths) / train_loader.batch_sampler.batch_size)\n",
    "val_step = math.ceil(len(val_loader.dataset.caption_lengths) / val_loader.batch_sampler.batch_size)\n",
    "\n",
    "def train(encoder, decoder, criterion, optimizer, train_loader, val_loader, total_epoch, checkpoint_path):\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(total_epoch):\n",
    "        train_epoch_loss = 0\n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        # Training phase\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        for i_step in range(train_step):\n",
    "            # obtain a sample where all captions have same length\n",
    "            indices = train_loader.dataset.get_train_indices()\n",
    "            # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "            train_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "            # Obtain the batch.\n",
    "            idx, images, captions = next(iter(train_loader))\n",
    "\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            \n",
    "            # Zero the gradients.\n",
    "            encoder.zero_grad()\n",
    "            decoder.zero_grad()\n",
    "            \n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_epoch_loss += loss.item()\n",
    "            if i_step % 100 == 0:\n",
    "                print('Training: {}  :{:.4f}'.format(i_step,loss.item()))\n",
    "                \n",
    "        train_epoch_loss /= train_step\n",
    "        training_loss.append(train_epoch_loss)\n",
    "        \n",
    "        # validation phase\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        for i_step in range(val_step):\n",
    "            # obtain a sample where all captions have same length\n",
    "            indices = val_loader.dataset.get_train_indices()\n",
    "            # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "            val_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "            # Obtain the batch.\n",
    "            idx, images, captions = next(iter(val_loader))\n",
    "\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            \n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "            \n",
    "            val_epoch_loss += loss.item()\n",
    "            if i_step % 100 == 0:\n",
    "                print('Validation: {}  :{:.4f}'.format(i_step,loss.item()))\n",
    "            \n",
    "        val_epoch_loss /= val_step\n",
    "        validation_loss.append(val_epoch_loss)\n",
    "    \n",
    "        epoch_time = (time.time() - start_time) /60**1\n",
    "\n",
    "        save_model(epoch, encoder, decoder, training_loss, validation_loss, checkpoint_path)\n",
    "\n",
    "        print(\"Epoch: {:d}. Training Loss = {:.4f}, Training Perplexity: {:.4f}. Validation Loss: {:.4f}, Validation Perplexity: {:.4f}. Time: {:f}\" \\\n",
    "          .format(epoch, train_epoch_loss, np.exp(train_epoch_loss), val_epoch_loss, np.exp(val_epoch_loss), epoch_time))\n",
    "    \n",
    "    return training_loss, validation_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0  :5.1558\n",
      "Training: 100  :6.8887\n",
      "Training: 200  :4.7070\n",
      "Training: 300  :5.3139\n",
      "Training: 400  :4.2956\n",
      "Training: 500  :5.6933\n",
      "Training: 600  :5.2031\n",
      "Training: 700  :5.7740\n",
      "Training: 800  :3.8395\n",
      "Training: 900  :4.6064\n",
      "Training: 1000  :3.4492\n",
      "Training: 1100  :5.4491\n",
      "Training: 1200  :6.1125\n",
      "Training: 1300  :5.4897\n",
      "Training: 1400  :6.6075\n",
      "Training: 1500  :4.3259\n",
      "Training: 1600  :4.6551\n",
      "Training: 1700  :3.8328\n",
      "Training: 1800  :3.4387\n",
      "Training: 1900  :4.1924\n",
      "Training: 2000  :5.0335\n",
      "Training: 2100  :5.2689\n",
      "Training: 2200  :3.6711\n",
      "Training: 2300  :5.5258\n",
      "Training: 2400  :4.7051\n",
      "Training: 2500  :3.8897\n",
      "Training: 2600  :5.2689\n",
      "Training: 2700  :6.3200\n",
      "Training: 2800  :5.6591\n",
      "Training: 2900  :3.6919\n",
      "Training: 3000  :6.2831\n",
      "Training: 3100  :5.1877\n",
      "Training: 3200  :6.0560\n",
      "Training: 3300  :3.8607\n",
      "Training: 3400  :5.2743\n",
      "Training: 3500  :6.6732\n",
      "Training: 3600  :7.4037\n",
      "Training: 3700  :4.6636\n",
      "Training: 3800  :5.5909\n",
      "Training: 3900  :3.8991\n",
      "Training: 4000  :3.4328\n",
      "Training: 4100  :4.3386\n",
      "Training: 4200  :5.7594\n",
      "Training: 4300  :4.9791\n",
      "Training: 4400  :3.4729\n",
      "Training: 4500  :3.8503\n",
      "Training: 4600  :3.4559\n",
      "Training: 4700  :4.1994\n",
      "Training: 4800  :3.8705\n",
      "Training: 4900  :4.1672\n",
      "Training: 5000  :4.4605\n",
      "Training: 5100  :3.5669\n",
      "Training: 5200  :4.0366\n",
      "Training: 5300  :3.9973\n",
      "Training: 5400  :4.4455\n",
      "Training: 5500  :3.4647\n",
      "Training: 5600  :4.2779\n",
      "Training: 5700  :4.2502\n",
      "Training: 5800  :3.1748\n",
      "Training: 5900  :2.6752\n",
      "Training: 6000  :6.0900\n",
      "Training: 6100  :4.6365\n",
      "Training: 6200  :4.7323\n",
      "Training: 6300  :3.5035\n",
      "Training: 6400  :6.1070\n",
      "Training: 6500  :5.0242\n",
      "Training: 6600  :3.0567\n",
      "Training: 6700  :5.1211\n",
      "Training: 6800  :3.3349\n",
      "Training: 6900  :2.3509\n",
      "Training: 7000  :6.2741\n",
      "Validation: 0  :2.6876\n",
      "Validation: 100  :3.0082\n",
      "Validation: 200  :3.3381\n",
      "Epoch: 0. Training Loss = 4.4839, Training Perplexity: 88.5783. Validation Loss: 3.9986, Validation Perplexity: 54.5220. Time: 16.224469\n",
      "Training: 0  :4.2797\n",
      "Training: 100  :4.5169\n",
      "Training: 200  :3.2150\n",
      "Training: 300  :5.3632\n",
      "Training: 400  :5.0808\n",
      "Training: 500  :4.1273\n",
      "Training: 600  :3.1822\n",
      "Training: 700  :3.8023\n",
      "Training: 800  :5.0120\n",
      "Training: 900  :5.5653\n",
      "Training: 1000  :4.2699\n",
      "Training: 1100  :4.9225\n",
      "Training: 1200  :5.8892\n",
      "Training: 1300  :3.6390\n",
      "Training: 1400  :4.2862\n",
      "Training: 1500  :3.9659\n",
      "Training: 1600  :3.2586\n",
      "Training: 1700  :4.0326\n",
      "Training: 1800  :6.1728\n",
      "Training: 1900  :5.2628\n",
      "Training: 2000  :5.2582\n",
      "Training: 2100  :6.1861\n",
      "Training: 2200  :4.5466\n",
      "Training: 2300  :4.1381\n",
      "Training: 2400  :5.4486\n",
      "Training: 2500  :3.4115\n",
      "Training: 2600  :3.1233\n",
      "Training: 2700  :2.9071\n",
      "Training: 2800  :3.6129\n",
      "Training: 2900  :2.6719\n",
      "Training: 3000  :3.5065\n",
      "Training: 3100  :5.6075\n",
      "Training: 3200  :4.5834\n",
      "Training: 3300  :2.7365\n",
      "Training: 3400  :6.6113\n",
      "Training: 3500  :4.8951\n",
      "Training: 3600  :4.0040\n",
      "Training: 3700  :4.4229\n",
      "Training: 3800  :4.8178\n",
      "Training: 3900  :5.0748\n",
      "Training: 4000  :5.6076\n",
      "Training: 4100  :3.2858\n",
      "Training: 4200  :4.1164\n",
      "Training: 4300  :2.4495\n",
      "Training: 4400  :2.3958\n",
      "Training: 4500  :3.5428\n",
      "Training: 4600  :3.4578\n",
      "Training: 4700  :4.0876\n",
      "Training: 4800  :3.4654\n",
      "Training: 4900  :5.6743\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': 10,\n",
    "    'checkpoint_path': CHECKPOINT\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c953de0-bd13-47b9-b7b5-dead76bbcae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a3c33-e253-4f19-8e81-f107c84a682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = './model/image_captioning_model_v8.pth'\n",
    "if Path(CHECKPOINT).exists():\n",
    "    encoder, decoder, training_loss, validation_loss = load_model(encoder, decoder, CHECKPOINT)\n",
    "else:\n",
    "    print(f'{CHECKPOINT} file does not exist, training startging from scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711dec22-ef01-41d2-bfcc-a1383e803bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss, validation_loss = train(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca2a7a-5f09-4d20-a937-477b240c5ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
