{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb673dc",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f57e85",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ba6a74-1033-44ad-ac91-930843f720db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models import Encoder, Decoder\n",
    "\n",
    "from pathlib import Path\n",
    "from DatasetInterface import MSCOCOInterface, get_loader\n",
    "from data_prep_utils import *\n",
    "from utils import train, save_model, load_model, plot_loss\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "###### download the data we need\n",
    "# !cd ~/INM706-image-captioning/Datasets/coco/images/\n",
    "# !wget http://images.cocodataset.org/zips/train2017.zip\n",
    "# !wget http://images.cocodataset.org/zips/val2017.zip\n",
    "# !unzip train2017.zip\n",
    "# !unzip val2017.zip\n",
    "# !rm train2017.zip\n",
    "# !rm val2017.zip\n",
    "\n",
    "##### run code below if nltk hasn't been set up in clound instance yet\n",
    "# !python -m nltk.downloader -d /usr/local/share/nltk_data all\n",
    "\n",
    "###### run code below to save pre-trained weights if needed\n",
    "# cd ~/INM706-image-captioning/model\n",
    "# !wget https://download.pytorch.org/models/resnet152-394f9c45.pth\n",
    "# !mv resnet152-394f9c45.pth resnet152_model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaa079",
   "metadata": {},
   "source": [
    "## Load Dataset Interface and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2472cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 15000 images\n",
      " val dataset has 3000 images\n",
      " test dataset has 938 images\n"
     ]
    }
   ],
   "source": [
    "root = Path('Datasets/coco')\n",
    "imgs_path = root/'images'/'train2017'\n",
    "imgs_path_test = root/'images'/'val2017'\n",
    "\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=['sports'],\n",
    "                    max_train=15000, max_val=3000, max_test=3000)\n",
    "\n",
    "#### build vocab using full original coco train. Uncomment to run\n",
    "# build_vocab(freq_threshold=2, sequence_length=40,\n",
    "#             captions_file='captions_train2017.json')\n",
    "\n",
    "train_captions_path = root/'annotations'/'custom_captions_train.json'\n",
    "val_captions_path = root/'annotations'/'custom_captions_val.json'\n",
    "test_captions_path = root/'annotations'/'custom_captions_test.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab72e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to boost the performence of CUDA use:\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8143132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vocab size is 16232\n",
      "####################\n",
      "\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:07<00:00, 10116.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vocab size is 16232\n",
      "####################\n",
      "\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 9982.21it/s] \n"
     ]
    }
   ],
   "source": [
    "train_interface_params = {\n",
    "    'imgs_path': imgs_path,\n",
    "    'captions_path': train_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    # 'sequence_length': 20,\n",
    "    'caps_per_img': 5,\n",
    "    # 'stage': \"train\",\n",
    "    'vocab_from_file': True\n",
    "}\n",
    "\n",
    "val_interface_params = {\n",
    "    'imgs_path': imgs_path,\n",
    "    'captions_path': val_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    # 'sequence_length': 20,\n",
    "    'caps_per_img': 1,\n",
    "    # 'stage': \"validation\",\n",
    "    'vocab_from_file': True\n",
    "}\n",
    "\n",
    "test_interface_params = {\n",
    "    'imgs_path': imgs_path_test,\n",
    "    'captions_path': test_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    # 'sequence_length': 20,\n",
    "    'caps_per_img': 1,\n",
    "    # 'stage': \"test\",\n",
    "    'vocab_from_file': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8237cb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vocab size is 16232\n",
      "####################\n",
      "\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:07<00:00, 9874.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vocab size is 16232\n",
      "####################\n",
      "\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 10215.84it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# # Training Interface\n",
    "# coco_interface_train = MSCOCOInterface(**train_interface_params)\n",
    "\n",
    "# # Validation Interface\n",
    "# coco_interface_val = MSCOCOInterface(**val_interface_params)\n",
    "\n",
    "train_loader = get_loader(**train_interface_params, batch_size=batch_size)\n",
    "val_loader = get_loader(**val_interface_params, batch_size=batch_size)\n",
    "# train_loader = data.DataLoader(coco_interface_train, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = data.DataLoader(coco_interface_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f38c73a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training captions: 75000\n",
      "Validation captions: 3000\n"
     ]
    }
   ],
   "source": [
    "print(\"training captions: {}\\nValidation captions: {}\"\n",
    "      .format(len(train_loader.dataset), len(val_loader.dataset)))\n",
    "\n",
    "# print(f\"Length of vocabulary: {len(coco_interface_train.idx_to_string)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a85bb9",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e041f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "hidden_size = 512\n",
    "vocab_size = len(train_loader.dataset.idx_to_string)\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52f24c",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc0b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################READY########################################\n"
     ]
    }
   ],
   "source": [
    "# pretrained = False does use a pretrained resnet but loads \n",
    "# from local .pth file \n",
    "encoder = Encoder(embed_size=embed_size, pretrained=False, model_weight_path=\"./model/resnet152_model.pth\")\n",
    "decoder = Decoder(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size, num_layers=num_layers)\n",
    "print(\"########################################READY########################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f6a4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=coco_interface_train.string_to_index[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':5e-4, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436dff8a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14c8e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, encoder, decoder, training_loss, validation_loss, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'training_loss': training_loss,\n",
    "        'validation_loss': validation_loss\n",
    "    }, checkpoint_path)\n",
    "\n",
    "def load_model(encoder, decoder, checkpoint_path):\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    training_loss = checkpoint['training_loss']\n",
    "    validation_loss = checkpoint['validation_loss']\n",
    "\n",
    "    return encoder, decoder, training_loss, validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de69f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/image_captioning_model_v7.pth file does not exist, training startging from scratch\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = './model/image_captioning_model_v7.pth'\n",
    "if Path(CHECKPOINT).exists():\n",
    "    encoder, decoder, training_loss, validation_loss = load_model(encoder, decoder, CHECKPOINT)\n",
    "else:\n",
    "    print(f'{CHECKPOINT} file does not exist, training startging from scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67994682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train_step = math.ceil(len(train_loader.dataset.caption_lengths) / train_loader.batch_sampler.batch_size)\n",
    "val_step = math.ceil(len(val_loader.dataset.caption_lengths) / val_loader.batch_sampler.batch_size)\n",
    "\n",
    "def train(encoder, decoder, criterion, optimizer, train_loader, val_loader, total_epoch, checkpoint_path):\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(total_epoch):\n",
    "        train_epoch_loss = 0\n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        # Training phase\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        for i_step in range(train_step):\n",
    "            # obtain a sample where all captions have same length\n",
    "            indices = train_loader.dataset.get_train_indices()\n",
    "            # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "            train_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "            # Obtain the batch.\n",
    "            idx, images, captions = next(iter(train_loader))\n",
    "\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            \n",
    "            # Zero the gradients.\n",
    "            encoder.zero_grad()\n",
    "            decoder.zero_grad()\n",
    "            \n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_epoch_loss += loss.item()\n",
    "            if i_step % 100 == 0:\n",
    "                print('Training: {}  :{:.4f}'.format(i_step,loss.item()))\n",
    "                \n",
    "        train_epoch_loss /= train_step\n",
    "        training_loss.append(train_epoch_loss)\n",
    "        \n",
    "        # validation phase\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        for i_step in range(val_step):\n",
    "            # obtain a sample where all captions have same length\n",
    "            indices = val_loader.dataset.get_train_indices()\n",
    "            # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "            val_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "            # Obtain the batch.\n",
    "            idx, images, captions = next(iter(val_loader))\n",
    "\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            \n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "            \n",
    "            val_epoch_loss += loss.item()\n",
    "            if i_step % 100 == 0:\n",
    "                print('Validation: {}  :{:.4f}'.format(i_step,loss.item()))\n",
    "            \n",
    "        val_epoch_loss /= val_step\n",
    "        validation_loss.append(val_epoch_loss)\n",
    "    \n",
    "        epoch_time = (time.time() - start_time) /60**1\n",
    "\n",
    "        save_model(epoch, encoder, decoder, training_loss, validation_loss, checkpoint_path)\n",
    "\n",
    "        print(\"Epoch: {:d}. Training Loss = {:.4f}, Training Perplexity: {:.4f}. Validation Loss: {:.4f}, Validation Perplexity: {:.4f}. Time: {:f}\" \\\n",
    "          .format(epoch, train_epoch_loss, np.exp(train_epoch_loss), val_epoch_loss, np.exp(val_epoch_loss), epoch_time))\n",
    "    \n",
    "    return training_loss, validation_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62a0559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0  :4.0140\n",
      "Training: 100  :3.6668\n",
      "Training: 200  :2.4205\n",
      "Training: 300  :2.6587\n",
      "Training: 400  :1.8600\n",
      "Training: 500  :3.0281\n",
      "Training: 600  :1.9365\n",
      "Training: 700  :1.9621\n",
      "Training: 800  :2.4196\n",
      "Training: 900  :4.0941\n",
      "Training: 1000  :1.6715\n",
      "Training: 1100  :3.8746\n",
      "Training: 1200  :3.4008\n",
      "Training: 1300  :2.8754\n",
      "Training: 1400  :2.3959\n",
      "Training: 1500  :2.5596\n",
      "Training: 1600  :3.2610\n",
      "Training: 1700  :4.4235\n",
      "Training: 1800  :2.8325\n",
      "Training: 1900  :2.9630\n",
      "Training: 2000  :2.3668\n",
      "Training: 2100  :5.0385\n",
      "Training: 2200  :2.5006\n",
      "Training: 2300  :1.7134\n",
      "Validation: 0  :3.0233\n",
      "Epoch: 0. Training Loss = 3.1229, Training Perplexity: 22.7113. Validation Loss: 3.0649, Validation Perplexity: 21.4314. Time: 3.008752\n",
      "Training: 0  :3.1670\n",
      "Training: 100  :4.0962\n",
      "Training: 200  :3.0191\n",
      "Training: 300  :3.0917\n",
      "Training: 400  :1.7003\n",
      "Training: 500  :5.6103\n",
      "Training: 600  :6.4488\n",
      "Training: 700  :4.2989\n",
      "Training: 800  :3.7549\n",
      "Training: 900  :3.1569\n",
      "Training: 1000  :1.5605\n",
      "Training: 1100  :1.6264\n",
      "Training: 1200  :4.2063\n",
      "Training: 1300  :3.1147\n",
      "Training: 1400  :2.1119\n",
      "Training: 1500  :2.6292\n",
      "Training: 1600  :1.7005\n",
      "Training: 1700  :3.1418\n",
      "Training: 1800  :2.9320\n",
      "Training: 1900  :4.2087\n",
      "Training: 2000  :3.2053\n",
      "Training: 2100  :1.9264\n",
      "Training: 2200  :2.8248\n",
      "Training: 2300  :3.0290\n",
      "Validation: 0  :2.8844\n",
      "Epoch: 1. Training Loss = 3.0648, Training Perplexity: 21.4305. Validation Loss: 2.6665, Validation Perplexity: 14.3895. Time: 5.998200\n",
      "Training: 0  :3.2014\n",
      "Training: 100  :2.0055\n",
      "Training: 200  :2.9506\n",
      "Training: 300  :6.1074\n",
      "Training: 400  :3.2143\n",
      "Training: 500  :1.4258\n",
      "Training: 600  :2.0793\n",
      "Training: 700  :2.8813\n",
      "Training: 800  :4.5253\n",
      "Training: 900  :1.5170\n",
      "Training: 1000  :2.3819\n",
      "Training: 1100  :3.2852\n",
      "Training: 1200  :3.5294\n",
      "Training: 1300  :4.1721\n",
      "Training: 1400  :3.2823\n",
      "Training: 1500  :1.5852\n",
      "Training: 1600  :3.0339\n",
      "Training: 1700  :3.8537\n",
      "Training: 1800  :2.8913\n",
      "Training: 1900  :1.6207\n",
      "Training: 2000  :2.6374\n",
      "Training: 2100  :4.9216\n",
      "Training: 2200  :1.5087\n",
      "Training: 2300  :3.8637\n",
      "Validation: 0  :4.4734\n",
      "Epoch: 2. Training Loss = 3.0621, Training Perplexity: 21.3731. Validation Loss: 2.8514, Validation Perplexity: 17.3125. Time: 9.020111\n",
      "Training: 0  :1.9416\n",
      "Training: 100  :4.0054\n",
      "Training: 200  :3.9831\n",
      "Training: 300  :3.5608\n",
      "Training: 400  :2.7064\n",
      "Training: 500  :2.5170\n",
      "Training: 600  :3.3893\n",
      "Training: 700  :2.6932\n",
      "Training: 800  :2.7938\n",
      "Training: 900  :3.0696\n",
      "Training: 1000  :1.0887\n",
      "Training: 1100  :4.9691\n",
      "Training: 1200  :3.1757\n",
      "Training: 1300  :1.4882\n",
      "Training: 1400  :2.8276\n",
      "Training: 1500  :2.8028\n",
      "Training: 1600  :1.7261\n",
      "Training: 1700  :2.6291\n",
      "Training: 1800  :3.2597\n",
      "Training: 1900  :3.2611\n",
      "Training: 2000  :2.5561\n",
      "Training: 2100  :3.1827\n",
      "Training: 2200  :2.9128\n",
      "Training: 2300  :3.2837\n",
      "Validation: 0  :2.8700\n",
      "Epoch: 3. Training Loss = 3.0267, Training Perplexity: 20.6293. Validation Loss: 2.8150, Validation Perplexity: 16.6930. Time: 12.056620\n",
      "Training: 0  :2.3605\n",
      "Training: 100  :3.5540\n",
      "Training: 200  :4.8008\n",
      "Training: 300  :4.4607\n",
      "Training: 400  :1.4528\n",
      "Training: 500  :4.1986\n",
      "Training: 600  :2.4082\n",
      "Training: 700  :3.9325\n",
      "Training: 800  :4.1064\n",
      "Training: 900  :3.4369\n",
      "Training: 1000  :1.2950\n",
      "Training: 1100  :3.2610\n",
      "Training: 1200  :4.8446\n",
      "Training: 1300  :3.6956\n",
      "Training: 1400  :2.3363\n",
      "Training: 1500  :2.7113\n",
      "Training: 1600  :3.0634\n",
      "Training: 1700  :3.1636\n",
      "Training: 1800  :2.2328\n",
      "Training: 1900  :2.4179\n",
      "Training: 2000  :4.1026\n",
      "Training: 2100  :3.2333\n",
      "Training: 2200  :1.2377\n",
      "Training: 2300  :3.0047\n",
      "Validation: 0  :3.4393\n",
      "Epoch: 4. Training Loss = 3.0559, Training Perplexity: 21.2398. Validation Loss: 2.5523, Validation Perplexity: 12.8361. Time: 15.053087\n",
      "Training: 0  :5.5411\n",
      "Training: 100  :2.5159\n",
      "Training: 200  :2.4187\n",
      "Training: 300  :5.1827\n",
      "Training: 400  :2.0577\n",
      "Training: 500  :2.4041\n",
      "Training: 600  :2.0696\n",
      "Training: 700  :2.4754\n",
      "Training: 800  :2.8431\n",
      "Training: 900  :1.3235\n",
      "Training: 1000  :2.6649\n",
      "Training: 1100  :3.6226\n",
      "Training: 1200  :2.4843\n",
      "Training: 1300  :6.9541\n",
      "Training: 1400  :1.8021\n",
      "Training: 1500  :4.1898\n",
      "Training: 1600  :3.7310\n",
      "Training: 1700  :4.1922\n",
      "Training: 1800  :2.1238\n",
      "Training: 1900  :1.8759\n",
      "Training: 2000  :2.4554\n",
      "Training: 2100  :2.7044\n",
      "Training: 2200  :3.1081\n",
      "Training: 2300  :2.2915\n",
      "Validation: 0  :2.6246\n",
      "Epoch: 5. Training Loss = 3.0408, Training Perplexity: 20.9223. Validation Loss: 3.0067, Validation Perplexity: 20.2196. Time: 18.087622\n",
      "Training: 0  :3.4031\n",
      "Training: 100  :2.5767\n",
      "Training: 200  :4.1336\n",
      "Training: 300  :3.0085\n",
      "Training: 400  :4.4425\n",
      "Training: 500  :2.8019\n",
      "Training: 600  :3.9042\n",
      "Training: 700  :2.2552\n",
      "Training: 800  :3.0850\n",
      "Training: 900  :2.4806\n",
      "Training: 1000  :2.7459\n",
      "Training: 1100  :2.2604\n",
      "Training: 1200  :1.4521\n",
      "Training: 1300  :1.6929\n",
      "Training: 1400  :1.0901\n",
      "Training: 1500  :4.9642\n",
      "Training: 1600  :4.9713\n",
      "Training: 1700  :2.3184\n",
      "Training: 1800  :1.3579\n",
      "Training: 1900  :2.3108\n",
      "Training: 2000  :4.2074\n",
      "Training: 2100  :3.4678\n",
      "Training: 2200  :2.0022\n",
      "Training: 2300  :6.1601\n",
      "Validation: 0  :2.4605\n",
      "Epoch: 6. Training Loss = 3.0302, Training Perplexity: 20.7023. Validation Loss: 2.9144, Validation Perplexity: 18.4375. Time: 21.108206\n",
      "Training: 0  :2.8191\n",
      "Training: 100  :2.9441\n",
      "Training: 200  :6.1154\n",
      "Training: 300  :1.8473\n",
      "Training: 400  :1.8828\n",
      "Training: 500  :1.7897\n",
      "Training: 600  :5.3250\n",
      "Training: 700  :5.6415\n",
      "Training: 800  :4.2246\n",
      "Training: 900  :3.0307\n",
      "Training: 1000  :3.7879\n",
      "Training: 1100  :2.0003\n",
      "Training: 1200  :3.2691\n",
      "Training: 1300  :1.6689\n",
      "Training: 1400  :5.3752\n",
      "Training: 1500  :1.3031\n",
      "Training: 1600  :3.2318\n",
      "Training: 1700  :5.2126\n",
      "Training: 1800  :2.5501\n",
      "Training: 1900  :2.1779\n",
      "Training: 2000  :3.7689\n",
      "Training: 2100  :3.5291\n",
      "Training: 2200  :3.4882\n",
      "Training: 2300  :4.4550\n",
      "Validation: 0  :3.1029\n",
      "Epoch: 7. Training Loss = 3.0033, Training Perplexity: 20.1521. Validation Loss: 2.9307, Validation Perplexity: 18.7416. Time: 24.119857\n",
      "Training: 0  :1.2485\n",
      "Training: 100  :3.5324\n",
      "Training: 200  :2.1280\n",
      "Training: 300  :2.0043\n",
      "Training: 400  :1.8835\n",
      "Training: 500  :1.7761\n",
      "Training: 600  :3.4634\n",
      "Training: 700  :2.4670\n",
      "Training: 800  :2.7387\n",
      "Training: 900  :3.8715\n",
      "Training: 1000  :2.6527\n",
      "Training: 1100  :2.1328\n",
      "Training: 1200  :1.6680\n",
      "Training: 1300  :2.7208\n",
      "Training: 1400  :1.6911\n",
      "Training: 1500  :5.3636\n",
      "Training: 1600  :2.9524\n",
      "Training: 1700  :3.5802\n",
      "Training: 1800  :4.9002\n",
      "Training: 1900  :2.1492\n",
      "Training: 2000  :2.2941\n",
      "Training: 2100  :2.6865\n",
      "Training: 2200  :4.8682\n",
      "Training: 2300  :4.8835\n",
      "Validation: 0  :2.2416\n",
      "Epoch: 8. Training Loss = 2.9885, Training Perplexity: 19.8566. Validation Loss: 2.7184, Validation Perplexity: 15.1566. Time: 27.169627\n",
      "Training: 0  :1.9430\n",
      "Training: 100  :2.3166\n",
      "Training: 200  :2.4342\n",
      "Training: 300  :2.9877\n",
      "Training: 400  :3.1386\n",
      "Training: 500  :1.5669\n",
      "Training: 600  :2.0383\n",
      "Training: 700  :2.2379\n",
      "Training: 800  :5.6058\n",
      "Training: 900  :4.8970\n",
      "Training: 1000  :1.9252\n",
      "Training: 1100  :2.8246\n",
      "Training: 1200  :2.5355\n",
      "Training: 1300  :3.6996\n",
      "Training: 1400  :3.0623\n",
      "Training: 1500  :4.2729\n",
      "Training: 1600  :1.3229\n",
      "Training: 1700  :4.5114\n",
      "Training: 1800  :3.5425\n",
      "Training: 1900  :2.6206\n",
      "Training: 2000  :3.5321\n",
      "Training: 2100  :3.1941\n",
      "Training: 2200  :5.2259\n",
      "Training: 2300  :3.2859\n",
      "Validation: 0  :4.8580\n",
      "Epoch: 9. Training Loss = 2.9872, Training Perplexity: 19.8296. Validation Loss: 2.7564, Validation Perplexity: 15.7424. Time: 30.164306\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': 10,\n",
    "    'checkpoint_path': CHECKPOINT\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c953de0-bd13-47b9-b7b5-dead76bbcae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711dec22-ef01-41d2-bfcc-a1383e803bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
