{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb673dc",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f57e85",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ba6a74-1033-44ad-ac91-930843f720db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models import Encoder, Decoder\n",
    "\n",
    "from pathlib import Path\n",
    "from DatasetInterface import MSCOCOInterface\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "###### run code below if nltk hasn't been set up in clound instance yet\n",
    "# !python -m nltk.downloader -d /usr/local/share/nltk_data all\n",
    "\n",
    "###### run code below to save pre-trained weights if needed\n",
    "# !wget https://download.pytorch.org/models/resnet152-394f9c45.pth\n",
    "# !mv resnet152-394f9c45.pth resnet152_model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaa079",
   "metadata": {},
   "source": [
    "## Load Dataset Interface and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2472cfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# load vocab\\nwith open('vocabulary/idx_to_string.json') as json_file:\\n    idx_to_string_json = json.load(json_file)\\n        \\nidx_to_string = dict()\\nfor key in idx_to_string_json:\\n    idx_to_string[int(key)] = idx_to_string_json[key]\\n    \\nwith open('vocabulary/string_to_index.json') as json_file:\\n    string_to_index = json.load(json_file)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########\n",
    "# paths for Khalil\n",
    "#########\n",
    "# root = Path('Data')\n",
    "\n",
    "#captions_path = root/'annotations'/'captions_train2017.json'\n",
    "\n",
    "# train_captions_path = root/'annotations_trainval2017'/'annotations'/'sports_captions_train.json'\n",
    "# val_captions_path = root/'annotations_trainval2017'/'annotations'/'sports_captions_val.json'\n",
    "# test_captions_path = root/'annotations_trainval2017'/'annotations'/'sports_captions_test.json'\n",
    "\n",
    "#########\n",
    "# paths for Alex\n",
    "#########\n",
    "\n",
    "root = Path('Datasets/coco')\n",
    "imgs_path = root/'images'/'train2017'\n",
    "imgs_path_test = root/'images'/'val2017'\n",
    "\n",
    "train_captions_path = root/'annotations'/'sports_captions_train.json'\n",
    "val_captions_path = root/'annotations'/'sports_captions_val.json'\n",
    "test_captions_path = root/'annotations'/'sports_captions_test.json'\n",
    "\n",
    "\"\"\"\n",
    "# load vocab\n",
    "with open('vocabulary/idx_to_string.json') as json_file:\n",
    "    idx_to_string_json = json.load(json_file)\n",
    "        \n",
    "idx_to_string = dict()\n",
    "for key in idx_to_string_json:\n",
    "    idx_to_string[int(key)] = idx_to_string_json[key]\n",
    "    \n",
    "with open('vocabulary/string_to_index.json') as json_file:\n",
    "    string_to_index = json.load(json_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab72e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to boost the performence of CUDA use:\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8143132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interface_params = {\n",
    "    'imgs_path': imgs_path,\n",
    "    'captions_path': train_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    'sequence_length': 20,\n",
    "    'caps_per_img': 1,\n",
    "    'stage': \"train\",\n",
    "    'idx_to_string': None,\n",
    "    'string_to_index': None,\n",
    "}\n",
    "\n",
    "val_interface_params = {\n",
    "    'imgs_path': imgs_path,\n",
    "    'captions_path': val_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    'sequence_length': 20,\n",
    "    'caps_per_img': 1,\n",
    "    'stage': \"validation\",\n",
    "    'idx_to_string': None,\n",
    "    'string_to_index': None,\n",
    "}\n",
    "\n",
    "test_interface_params = {\n",
    "    'imgs_path': imgs_path_test,\n",
    "    'captions_path': test_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    'sequence_length': 20,\n",
    "    'caps_per_img': 1,\n",
    "    'stage': \"test\",\n",
    "    'idx_to_string': None,\n",
    "    'string_to_index': None,\n",
    "}\n",
    "\n",
    "\n",
    "# Training Interface\n",
    "coco_interface_train = MSCOCOInterface(**train_interface_params)\n",
    "\n",
    "# Validation Interface\n",
    "coco_interface_val = MSCOCOInterface(**val_interface_params)\n",
    "\n",
    "# Testing Interface\n",
    "coco_interface_test = MSCOCOInterface(**test_interface_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f38c73a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of training image: 500, Lenght of Validation image: 100 Lenght of Testing image: 4939\n",
      "Lenght of vocabulary: 619\n"
     ]
    }
   ],
   "source": [
    "print(\"Lenght of training image: {}, Lenght of Validation image: {} Lenght of Testing image: {}\"\\\n",
    "      .format(len(coco_interface_train), len(coco_interface_val), len(coco_interface_test)))\n",
    "\n",
    "print(f\"Lenght of vocabulary: {len(coco_interface_train.idx_to_string)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8237cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = data.DataLoader(coco_interface_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(coco_interface_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = data.DataLoader(coco_interface_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a85bb9",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46e041f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "hidden_size = 512\n",
    "vocab_size = len(coco_interface_train.idx_to_string)\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52f24c",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fc0b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################READY########################################\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(embed_size=embed_size, pretrained=False, model_weight_path=\"./model/resnet152_model.pth\")\n",
    "decoder = Decoder(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size, num_layers=num_layers)\n",
    "print(\"########################################READY########################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f6a4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=coco_interface_train.string_to_index[\"<PAD>\"])\n",
    "\n",
    "# combine the paramters of decoder and ecnoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-5, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436dff8a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14c8e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, encoder, decoder, training_loss, validation_loss, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'training_loss': training_loss,\n",
    "        'validation_loss': validation_loss\n",
    "    }, checkpoint_path)\n",
    "\n",
    "def load_model(encoder, decoder, checkpoint_path):\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    training_loss = checkpoint['training_loss']\n",
    "    validation_loss = checkpoint['validation_loss']\n",
    "\n",
    "    return encoder, decoder, training_loss, validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de69f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/image_captioning_model_v1.pth file does not exist, training startging from scratch\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = './model/image_captioning_model_v1.pth'\n",
    "if Path(CHECKPOINT).exists():\n",
    "    encoder, decoder, training_loss, validation_loss = load_model(encoder, decoder, CHECKPOINT)\n",
    "else:\n",
    "    print(f'{CHECKPOINT} file does not exist, training startging from scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67994682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, criterion, optimizer, train_loader, val_loader, total_epoch, checkpoint_path):\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(total_epoch):\n",
    "        train_epoch_loss = 0\n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        \n",
    "        # Training phase\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            idx, images, captions = batch\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            \n",
    "            # Zero the gradients.\n",
    "            encoder.zero_grad()\n",
    "            decoder.zero_grad()\n",
    "            \n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_epoch_loss += loss.item()\n",
    "            if i % 100 == 0:\n",
    "                print('Training: ', i, ' ', loss.item())\n",
    "                \n",
    "        train_epoch_loss /= len(train_loader)\n",
    "        training_loss.append(train_epoch_loss)\n",
    "        \n",
    "        # validation phase\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        for id, batch in enumerate(val_loader):\n",
    "            idx, images, captions = batch\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "            val_epoch_loss += loss.item()\n",
    "            if id % 100 == 0:\n",
    "                print('Validation: ', id, ' ', loss.item())\n",
    "            \n",
    "        val_epoch_loss /= len(val_loader)\n",
    "        validation_loss.append(val_epoch_loss)\n",
    "    \n",
    "        epoch_time = (time.time() - start_time) /60**1\n",
    "\n",
    "        save_model(epoch, encoder, decoder, training_loss, validation_loss, checkpoint_path)\n",
    "\n",
    "        print(\"Epoch: {1:}. Training Loss = {1:.4f}, Training Perplexity: {2:.4f}. Validation Loss: {3:.4f}, Validation Perplexity: {4:.4f}. Time: {5:f}\" \\\n",
    "          .format(epoch, train_epoch_loss, np.exp(train_epoch_loss), val_epoch_loss, np.exp(val_epoch_loss), epoch_time))\n",
    "    \n",
    "    return training_loss, validation_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0   3.445712089538574\n",
      "Training:  100   4.557940483093262\n",
      "Training:  200   4.148390293121338\n",
      "Training:  300   4.806350231170654\n",
      "Training:  400   4.394772529602051\n",
      "Validation:  0   3.6761958599090576\n",
      "Epoch: 4.1024859943389895. Training Loss = 4.1025, Training Perplexity: 60.4905. Validation Loss: 4.2559, Validation Perplexity: 70.5187. Time: 0.523141\n",
      "Training:  0   4.025753974914551\n",
      "Training:  100   4.109526634216309\n",
      "Training:  200   4.796710014343262\n",
      "Training:  300   3.686384439468384\n",
      "Training:  400   4.2791852951049805\n",
      "Validation:  0   3.5996787548065186\n",
      "Epoch: 3.924295334815979. Training Loss = 3.9243, Training Perplexity: 50.6174. Validation Loss: 4.2515, Validation Perplexity: 70.2116. Time: 0.960140\n",
      "Training:  0   3.89064621925354\n",
      "Training:  100   3.271575689315796\n",
      "Training:  200   3.811802387237549\n",
      "Training:  300   3.7648942470550537\n",
      "Training:  400   3.4575552940368652\n",
      "Validation:  0   3.5623321533203125\n",
      "Epoch: 3.8280628042221068. Training Loss = 3.8281, Training Perplexity: 45.9734. Validation Loss: 4.2368, Validation Perplexity: 69.1855. Time: 1.413288\n",
      "Training:  0   3.8021240234375\n",
      "Training:  100   4.219902038574219\n",
      "Training:  200   3.2227249145507812\n",
      "Training:  300   2.73561692237854\n",
      "Training:  400   3.405524969100952\n",
      "Validation:  0   3.5400142669677734\n",
      "Epoch: 3.7555224194526673. Training Loss = 3.7555, Training Perplexity: 42.7566. Validation Loss: 4.2472, Validation Perplexity: 69.9103. Time: 1.878962\n",
      "Training:  0   3.068007469177246\n",
      "Training:  100   3.579451084136963\n",
      "Training:  200   3.454888343811035\n",
      "Training:  300   3.819934606552124\n",
      "Training:  400   3.5224344730377197\n",
      "Validation:  0   3.533256769180298\n",
      "Epoch: 3.694494204044342. Training Loss = 3.6945, Training Perplexity: 40.2252. Validation Loss: 4.2551, Validation Perplexity: 70.4637. Time: 2.340777\n",
      "Training:  0   3.925957441329956\n",
      "Training:  100   3.486142635345459\n",
      "Training:  200   2.94818377494812\n",
      "Training:  300   4.713620185852051\n",
      "Training:  400   3.9051566123962402\n",
      "Validation:  0   3.5151422023773193\n",
      "Epoch: 3.6411872577667235. Training Loss = 3.6412, Training Perplexity: 38.1371. Validation Loss: 4.2542, Validation Perplexity: 70.4019. Time: 2.757564\n",
      "Training:  0   3.1318674087524414\n",
      "Training:  100   3.6345088481903076\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': 10,\n",
    "    'checkpoint_path': './model/image_captioning_model_v0.pth'\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c953de0-bd13-47b9-b7b5-dead76bbcae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711dec22-ef01-41d2-bfcc-a1383e803bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
