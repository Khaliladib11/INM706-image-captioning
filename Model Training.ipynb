{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb673dc",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f57e85",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ba6a74-1033-44ad-ac91-930843f720db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models import Encoder, Decoder\n",
    "\n",
    "from pathlib import Path\n",
    "from DatasetInterface import MSCOCOInterface, get_loader\n",
    "from data_prep_utils import *\n",
    "from utils import train, save_model, load_model, plot_loss\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "###### download the data we need\n",
    "# !cd ~/INM706-image-captioning/Datasets/coco/images/\n",
    "# !wget http://images.cocodataset.org/zips/train2017.zip\n",
    "# !wget http://images.cocodataset.org/zips/val2017.zip\n",
    "# !unzip train2017.zip\n",
    "# !unzip val2017.zip\n",
    "# !rm train2017.zip\n",
    "# !rm val2017.zip\n",
    "\n",
    "##### run code below if nltk hasn't been set up in clound instance yet\n",
    "# !python -m nltk.downloader -d /usr/local/share/nltk_data all\n",
    "\n",
    "###### run code below to save pre-trained weights if needed\n",
    "# cd ~/INM706-image-captioning/model\n",
    "# !wget https://download.pytorch.org/models/resnet152-394f9c45.pth\n",
    "# !mv resnet152-394f9c45.pth resnet152_model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaa079",
   "metadata": {},
   "source": [
    "## Load Dataset Interface and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2472cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('Datasets/coco')\n",
    "imgs_path = root/'images'/'train2017'\n",
    "imgs_path_test = root/'images'/'val2017'\n",
    "\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=['sports'],\n",
    "                    max_train=15000, max_val=3000, max_test=3000)\n",
    "\n",
    "#### build vocab using full original coco train. Uncomment to run\n",
    "# build_vocab(freq_threshold=2, sequence_length=40,\n",
    "#             captions_file='captions_train2017.json')\n",
    "\n",
    "train_captions_path = root/'annotations'/'custom_captions_train.json'\n",
    "val_captions_path = root/'annotations'/'custom_captions_val.json'\n",
    "test_captions_path = root/'annotations'/'custom_captions_test.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab72e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to boost the performence of CUDA use:\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8143132d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Datasets/coco/annotations/custom_captions_train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m test_interface_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs_path\u001b[39m\u001b[38;5;124m'\u001b[39m: imgs_path_test,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptions_path\u001b[39m\u001b[38;5;124m'\u001b[39m: test_captions_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab_from_file\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     29\u001b[0m }\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Training Interface\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m coco_interface_train \u001b[38;5;241m=\u001b[39m \u001b[43mMSCOCOInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_interface_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Validation Interface\u001b[39;00m\n\u001b[1;32m     35\u001b[0m coco_interface_val \u001b[38;5;241m=\u001b[39m MSCOCOInterface(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mval_interface_params)\n",
      "File \u001b[0;32m~/INM706-image-captioning/DatasetInterface.py:91\u001b[0m, in \u001b[0;36mMSCOCOInterface.__init__\u001b[0;34m(self, imgs_path, captions_path, freq_threshold, caps_per_img, vocab_from_file)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaps_per_img \u001b[38;5;241m=\u001b[39m caps_per_img\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco \u001b[38;5;241m=\u001b[39m \u001b[43mCOCO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptions_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# create vocab from scratch if it is not already done\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vocab_from_file:\n",
      "File \u001b[0;32m~/INM706-image-captioning/MSCOCO.py:24\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, imgs_path, captions_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptions_path \u001b[38;5;241m=\u001b[39m captions_path\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# load captions json file and put json['captions'] and json['images'] into attributes:\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaption_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_captions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# use images from caption file to define the list of images used in data set\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_imgs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaption_images)\n",
      "File \u001b[0;32m~/INM706-image-captioning/MSCOCO.py:35\u001b[0m, in \u001b[0;36mCOCO._load_captions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_captions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Opening JSON file\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptions_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m captions_json:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# returns JSON object as\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# a dictionary\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(captions_json)\n\u001b[1;32m     39\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Datasets/coco/annotations/custom_captions_train.json'"
     ]
    }
   ],
   "source": [
    "train_interface_params = {\n",
    "    'imgs_path': imgs_path,\n",
    "    'captions_path': train_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    # 'sequence_length': 20,\n",
    "    'caps_per_img': 5,\n",
    "    # 'stage': \"train\",\n",
    "    'vocab_from_file': True\n",
    "}\n",
    "\n",
    "val_interface_params = {\n",
    "    'imgs_path': imgs_path,\n",
    "    'captions_path': val_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    # 'sequence_length': 20,\n",
    "    'caps_per_img': 1,\n",
    "    # 'stage': \"validation\",\n",
    "    'vocab_from_file': True\n",
    "}\n",
    "\n",
    "test_interface_params = {\n",
    "    'imgs_path': imgs_path_test,\n",
    "    'captions_path': test_captions_path,\n",
    "    'freq_threshold': 5,\n",
    "    # 'sequence_length': 20,\n",
    "    'caps_per_img': 1,\n",
    "    # 'stage': \"test\",\n",
    "    'vocab_from_file': True\n",
    "}\n",
    "\n",
    "# Training Interface\n",
    "coco_interface_train = MSCOCOInterface(**train_interface_params)\n",
    "\n",
    "# Validation Interface\n",
    "coco_interface_val = MSCOCOInterface(**val_interface_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training captions: {}\\nValidation captions: {}\"\n",
    "      .format(len(coco_interface_train), len(coco_interface_val)))\n",
    "\n",
    "# print(f\"Length of vocabulary: {len(coco_interface_train.idx_to_string)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = \n",
    "train_loader = data.DataLoader(coco_interface_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(coco_interface_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a85bb9",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e041f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "hidden_size = 512\n",
    "vocab_size = len(coco_interface_train.idx_to_string)\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52f24c",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc0b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################READY########################################\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(embed_size=embed_size, pretrained=False, model_weight_path=\"./model/resnet152_model.pth\")\n",
    "decoder = Decoder(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size, num_layers=num_layers)\n",
    "print(\"########################################READY########################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6a4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=coco_interface_train.string_to_index[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436dff8a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14c8e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, encoder, decoder, training_loss, validation_loss, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'encoder_state_dict': encoder.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'training_loss': training_loss,\n",
    "        'validation_loss': validation_loss\n",
    "    }, checkpoint_path)\n",
    "\n",
    "def load_model(encoder, decoder, checkpoint_path):\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    training_loss = checkpoint['training_loss']\n",
    "    validation_loss = checkpoint['validation_loss']\n",
    "\n",
    "    return encoder, decoder, training_loss, validation_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de69f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/image_captioning_model_v4.pth file does not exist, training startging from scratch\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = './model/image_captioning_model_v4.pth'\n",
    "if Path(CHECKPOINT).exists():\n",
    "    encoder, decoder, training_loss, validation_loss = load_model(encoder, decoder, CHECKPOINT)\n",
    "else:\n",
    "    print(f'{CHECKPOINT} file does not exist, training startging from scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67994682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, criterion, optimizer, train_loader, val_loader, total_epoch, checkpoint_path):\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(total_epoch):\n",
    "        train_epoch_loss = 0\n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        \n",
    "        # Training phase\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            idx, images, captions = batch\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            \n",
    "            # Zero the gradients.\n",
    "            encoder.zero_grad()\n",
    "            decoder.zero_grad()\n",
    "            \n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_epoch_loss += loss.item()\n",
    "            if i % 100 == 0:\n",
    "                print('Training: ', i, ' ', loss.item())\n",
    "                \n",
    "        train_epoch_loss /= len(train_loader)\n",
    "        training_loss.append(train_epoch_loss)\n",
    "        \n",
    "        # validation phase\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        for i, batch in enumerate(val_loader):\n",
    "            idx, images, captions = batch\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "            val_epoch_loss += loss.item()\n",
    "            if i % 100 == 0:\n",
    "                print('Validation: ', i, ' ', loss.item())\n",
    "            \n",
    "        val_epoch_loss /= len(val_loader)\n",
    "        validation_loss.append(val_epoch_loss)\n",
    "    \n",
    "        epoch_time = (time.time() - start_time) /60**1\n",
    "\n",
    "        save_model(epoch, encoder, decoder, training_loss, validation_loss, checkpoint_path)\n",
    "\n",
    "        print(\"Epoch: {:d}. Training Loss = {:.4f}, Training Perplexity: {:.4f}. Validation Loss: {:.4f}, Validation Perplexity: {:.4f}. Time: {:f}\" \\\n",
    "          .format(epoch, train_epoch_loss, np.exp(train_epoch_loss), val_epoch_loss, np.exp(val_epoch_loss), epoch_time))\n",
    "    \n",
    "    return training_loss, validation_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  0   9.515953063964844\n",
      "Training:  100   9.210984230041504\n",
      "Training:  200   8.009066581726074\n",
      "Training:  300   6.2172160148620605\n",
      "Training:  400   5.627791404724121\n",
      "Training:  500   5.186134338378906\n",
      "Training:  600   4.789130210876465\n",
      "Training:  700   4.47951078414917\n",
      "Training:  800   4.59299898147583\n",
      "Training:  900   4.52008581161499\n",
      "Training:  1000   4.329635143280029\n",
      "Training:  1100   4.214908599853516\n",
      "Training:  1200   4.064572334289551\n",
      "Training:  1300   4.279685020446777\n",
      "Training:  1400   4.430614948272705\n",
      "Validation:  0   4.184030055999756\n",
      "Validation:  100   4.056313991546631\n",
      "Validation:  200   4.125290870666504\n",
      "Epoch: 0. Training Loss = 5.4922, Training Perplexity: 242.7841. Validation Loss: 4.2283, Validation Perplexity: 68.6030. Time: 19.891358\n",
      "Training:  0   4.340602874755859\n",
      "Training:  100   4.150339126586914\n",
      "Training:  200   4.1194915771484375\n",
      "Training:  300   4.195537567138672\n",
      "Training:  400   4.215137004852295\n",
      "Training:  500   4.233678817749023\n",
      "Training:  600   3.9670445919036865\n",
      "Training:  700   3.9944844245910645\n",
      "Training:  800   4.057607173919678\n",
      "Training:  900   3.902231454849243\n",
      "Training:  1000   4.199789047241211\n",
      "Training:  1100   3.798816442489624\n",
      "Training:  1200   4.055152893066406\n",
      "Training:  1300   4.0771050453186035\n",
      "Training:  1400   3.738969087600708\n",
      "Validation:  0   3.808748722076416\n",
      "Validation:  100   3.6798224449157715\n",
      "Validation:  200   3.7597479820251465\n",
      "Epoch: 1. Training Loss = 4.0486, Training Perplexity: 57.3164. Validation Loss: 3.8805, Validation Perplexity: 48.4465. Time: 39.861753\n",
      "Training:  0   3.6249170303344727\n",
      "Training:  100   4.083412170410156\n",
      "Training:  200   3.816410779953003\n",
      "Training:  300   3.575230121612549\n",
      "Training:  400   3.6545469760894775\n",
      "Training:  500   3.504828929901123\n",
      "Training:  600   3.782625198364258\n",
      "Training:  700   3.947258710861206\n",
      "Training:  800   3.5093300342559814\n",
      "Training:  900   3.7277274131774902\n",
      "Training:  1000   3.860720634460449\n",
      "Training:  1100   3.7024245262145996\n",
      "Training:  1200   3.8282036781311035\n",
      "Training:  1300   3.7178497314453125\n",
      "Training:  1400   3.694422721862793\n",
      "Validation:  0   3.5606040954589844\n",
      "Validation:  100   3.4138944149017334\n",
      "Validation:  200   3.509812116622925\n",
      "Epoch: 2. Training Loss = 3.7596, Training Perplexity: 42.9315. Validation Loss: 3.6445, Validation Perplexity: 38.2651. Time: 59.812832\n",
      "Training:  0   3.6686666011810303\n",
      "Training:  100   3.728597402572632\n",
      "Training:  200   3.5417871475219727\n",
      "Training:  300   3.4234201908111572\n",
      "Training:  400   3.4313347339630127\n",
      "Training:  500   3.1673684120178223\n",
      "Training:  600   3.4916634559631348\n",
      "Training:  700   3.4469285011291504\n",
      "Training:  800   3.296787738800049\n",
      "Training:  900   3.4082930088043213\n",
      "Training:  1000   3.920438766479492\n",
      "Training:  1100   3.6550467014312744\n",
      "Training:  1200   3.2024340629577637\n",
      "Training:  1300   3.4600753784179688\n",
      "Training:  1400   3.3787529468536377\n",
      "Validation:  0   3.3963825702667236\n",
      "Validation:  100   3.23158860206604\n",
      "Validation:  200   3.3466243743896484\n",
      "Epoch: 3. Training Loss = 3.5616, Training Perplexity: 35.2202. Validation Loss: 3.4802, Validation Perplexity: 32.4668. Time: 79.655049\n",
      "Training:  0   3.562284469604492\n",
      "Training:  100   3.463280439376831\n",
      "Training:  200   3.560683488845825\n",
      "Training:  300   3.1694793701171875\n",
      "Training:  400   3.446065902709961\n",
      "Training:  500   3.67322039604187\n",
      "Training:  600   3.223501205444336\n",
      "Training:  700   3.380782127380371\n",
      "Training:  800   3.323033094406128\n",
      "Training:  900   3.2525110244750977\n",
      "Training:  1000   3.451152801513672\n",
      "Training:  1100   3.542400360107422\n",
      "Training:  1200   3.590986967086792\n",
      "Training:  1300   3.444225311279297\n",
      "Training:  1400   3.2147698402404785\n",
      "Validation:  0   3.2764058113098145\n",
      "Validation:  100   3.1042380332946777\n",
      "Validation:  200   3.2260682582855225\n",
      "Epoch: 4. Training Loss = 3.4185, Training Perplexity: 30.5243. Validation Loss: 3.3595, Validation Perplexity: 28.7752. Time: 99.516010\n",
      "Training:  0   3.5734245777130127\n",
      "Training:  100   2.9618566036224365\n",
      "Training:  200   3.2190635204315186\n",
      "Training:  300   3.211134433746338\n",
      "Training:  400   3.4272024631500244\n",
      "Training:  500   3.387782335281372\n",
      "Training:  600   3.55112624168396\n",
      "Training:  700   3.6092967987060547\n",
      "Training:  800   3.0479583740234375\n",
      "Training:  900   3.3998594284057617\n",
      "Training:  1000   3.217118263244629\n",
      "Training:  1100   3.4625494480133057\n",
      "Training:  1200   3.0982472896575928\n",
      "Training:  1300   3.4707417488098145\n",
      "Training:  1400   3.4284050464630127\n",
      "Validation:  0   3.187938690185547\n",
      "Validation:  100   3.007162570953369\n",
      "Validation:  200   3.137634038925171\n",
      "Epoch: 5. Training Loss = 3.3107, Training Perplexity: 27.4055. Validation Loss: 3.2661, Validation Perplexity: 26.2099. Time: 119.381997\n",
      "Training:  0   2.9860379695892334\n",
      "Training:  100   3.6229820251464844\n",
      "Training:  200   3.1692800521850586\n",
      "Training:  300   3.5129778385162354\n",
      "Training:  400   3.391225814819336\n",
      "Training:  500   2.940432548522949\n",
      "Training:  600   3.3392765522003174\n",
      "Training:  700   2.859853744506836\n",
      "Training:  800   3.337738275527954\n",
      "Training:  900   3.0478641986846924\n",
      "Training:  1000   2.855185031890869\n",
      "Training:  1100   3.0398800373077393\n",
      "Training:  1200   3.4518625736236572\n",
      "Training:  1300   2.9528257846832275\n",
      "Training:  1400   3.1511077880859375\n",
      "Validation:  0   3.1143438816070557\n",
      "Validation:  100   2.9269754886627197\n",
      "Validation:  200   3.067152976989746\n",
      "Epoch: 6. Training Loss = 3.2249, Training Perplexity: 25.1509. Validation Loss: 3.1896, Validation Perplexity: 24.2795. Time: 139.250589\n",
      "Training:  0   3.166330337524414\n",
      "Training:  100   3.3203272819519043\n",
      "Training:  200   3.402881383895874\n",
      "Training:  300   2.976362943649292\n",
      "Training:  400   3.606471061706543\n",
      "Training:  500   3.513500213623047\n",
      "Training:  600   3.123234748840332\n",
      "Training:  700   3.1571664810180664\n",
      "Training:  800   2.7893688678741455\n",
      "Training:  900   3.2186849117279053\n",
      "Training:  1000   3.424802780151367\n",
      "Training:  1100   3.1460351943969727\n",
      "Training:  1200   3.104172706604004\n",
      "Training:  1300   3.05460524559021\n",
      "Training:  1400   3.2455196380615234\n",
      "Validation:  0   3.0547842979431152\n",
      "Validation:  100   2.8662567138671875\n",
      "Validation:  200   3.000877857208252\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': 100,\n",
    "    'checkpoint_path': CHECKPOINT\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c953de0-bd13-47b9-b7b5-dead76bbcae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711dec22-ef01-41d2-bfcc-a1383e803bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
