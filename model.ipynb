{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af65adb",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf3c89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e5788c",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed689a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152_model = models.resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f1db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet152_model.state_dict(), 'resnet152_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eeee9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet152_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "089cc209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, embed_size, pretrained=True, model_weight_path=None):\n",
    "        \"\"\"\n",
    "        Encoder is the first part to our model.\n",
    "        The main porpose of encoder is to extract the usefule feature from an image\n",
    "        We will use Resnet152 architecture pre-trained on ImageNet dataset\n",
    "        Parameters\n",
    "        ----------\n",
    "        :param embed_size (int): the embed_size will be the output of the encoder since embed_size represents the input of the decoder\n",
    "        :param pretrained (bool): if we want to load the pretrained weigth or not\n",
    "        :param model_weight_path (sting): path to the pre trained weight\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # Load pretrained resnet152 on ImageNet\n",
    "        if pretrained:\n",
    "            self.resnet152 = models.resnet152(pretrained=True)\n",
    "        else:\n",
    "            self.resnet152 = models.resnet152(pretrained=False)\n",
    "            self.resnet152.load_state_dict(torch.load(model_weight_path))\n",
    "            \n",
    "        # Freeze the parameters of pre trained model\n",
    "        for param in self.resnet152.paramters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        # change the last fully connected layer output with embed_size\n",
    "        self.resnet50.fc = nn.Linear(self.resnet50.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    \n",
    "    def forward(self, images):\n",
    "        features = self.resnet152(images)\n",
    "        return self.dropout(self.relu(features))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a4cd8",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67569b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        \"\"\"\n",
    "        Decoder is the second part of our model.\n",
    "        Decoder takes as input the outputs of the encoder: the image feature vectors\n",
    "        The input of the decoder and output of the encoder must be the same size\n",
    "        Parameters\n",
    "        ----------\n",
    "        :param embed_size (int) : Dimensionality of image and word embeddings\n",
    "        :param hidden_size (int) : number of features in hidden state of the RNN decoder\n",
    "        :param vocab_size  (int) : The size of vocabulary or output size\n",
    "        :param num_layers (int) : Number of layers\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        elf.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embed_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first = True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "                \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        # Concatenating features to embedding\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embedding), dim=0)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(lstm_out)\n",
    "        return outputs\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        # embeding layer\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)        \n",
    "        #fully connected layers\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.fc1.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abbe10",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd3dd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, pretrained=True, model_weight_path=None):\n",
    "        \"\"\"\n",
    "        The Image Captioning model\n",
    "        it will pass the data to encoder, encoder will be connected to decoder\n",
    "        Parameters\n",
    "        ----------\n",
    "        :param embed_size (int) : Dimensionality of image and word embeddings\n",
    "        :param hidden_size (int) : number of features in hidden state of the RNN decoder\n",
    "        :param vocab_size  (int) : The size of vocabulary or output size\n",
    "        :param num_layers (int) : Number of layers\n",
    "        :param pretrained (bool): if we want to load the pretrained weigth or not\n",
    "        :param model_weight_path (sting): path to the pre trained weight\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(embed_size, pretrained=Trpretrainedue, model_weight_path=model_weight_path)\n",
    "        self.decoder = Decoder(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c88fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a825adc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ailab] *",
   "language": "python",
   "name": "conda-env-ailab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
