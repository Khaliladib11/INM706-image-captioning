{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256e51d2",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff24f55",
   "metadata": {},
   "source": [
    "In this notebook, we will run models, also this notebook can be a template to run other models with different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7767e",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a880f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_loader import get_loader\n",
    "from models import Encoder, Decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from data_prep_utils import *\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd4d49",
   "metadata": {},
   "source": [
    "## Load train and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "523632e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'random'\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 1\n",
    "CHECKPOINT = '../model/model_v1'\n",
    "\n",
    "PRINT_EVERY = 500 # run print_every batches and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c74cc57-cc4a-4326-bb20-a99603dde422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 10000 images\n",
      " val dataset has 2000 images\n",
      " test dataset has 2000 images\n",
      "There are 50026 captions in the data set\n",
      "With FREQ_THRESHOLD = 5, vocab size is 3389\n"
     ]
    }
   ],
   "source": [
    "# create custom data set if we need it. We can choose to work with certain types\n",
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=None,\n",
    "                 max_train=10000, max_val=2000, max_test=2000,\n",
    "                 save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file=f'{CAPTIONS_NAME}_captions_train.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6149d22-1b1e-4225-b3af-eeb326e376fd",
   "metadata": {},
   "source": [
    "These are data loaders built with the prepare_datasets method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "503f98fa-8228-4a0e-b522-866f4052ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae20f05-89b6-45f6-8e9c-b62753fe94f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 50000, Length of testing dataloader: 188\n",
      "Length of vocabulary: 3389\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_dataset)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "attachments": {
    "37a7c4aa-ce83-4525-a46a-1e34582cb9bf.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c6e52833-744c-436d-ae66-a02c3ec75ad2",
   "metadata": {},
   "source": [
    "###  These are data loaders run with original code. Don't rerun but you can see results\n",
    "\n",
    "![image.png](attachment:37a7c4aa-ce83-4525-a46a-1e34582cb9bf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0113db3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of training dataloader: 50000, Lenght of testing dataloader: 188\n",
      "Lenght of vocabulary: 3387\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    'idx2word': None,\n",
    "    'word2idx': None\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': train_dataset.vocab.word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_dataset)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64e62129-f532-4414-a7df-c6de6ca0fa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 187.5, 50000, 1562.5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset.img_deque), 6000/32, len(train_dataset.img_deque), len(train_dataset.img_deque)/32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb037c",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a492102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b4ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673155db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed65aa52-7d2c-434e-b317-e3619bc82a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45158cda",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2ab890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/1] || Step: [0/1563] || Average Training Loss: 8.1363\n",
      "Epoch: [0/1] || Step: [500/1563] || Average Training Loss: 3.7902\n",
      "Epoch: [0/1] || Step: [1000/1563] || Average Training Loss: 3.5424\n",
      "Epoch: [0/1] || Step: [1500/1563] || Average Training Loss: 3.4285\n",
      "Epoch: [0/1] || Step: [0/188] || Average Validation Loss: 3.0506\n",
      "****************************************************************************************************\n",
      "Epoch: [0/1] || Training Loss = 3.42 || Validation Loss: 3.12 || Time: 10.744735\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0a2c2-8bf7-4a91-9ebb-ffe9601c1ef4",
   "metadata": {},
   "source": [
    "## Try with different hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c88d1-e10a-4b3b-a017-a28e227d509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data loader\n",
    "BATCH_SIZE = 128\n",
    "CAPS_PER_IMAGE = 5 # how many captions for each image to include in data set\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 1024 # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3 #hidden layers in LTSM\n",
    "vocab_size = len(train_dataset.vocab.idx2word)\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 10\n",
    "CHECKPOINT = '../model/model_v2'\n",
    "PRINT_EVERY = 500 # run print_every batches and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3017d8de",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../model/model_v2/model_v2_1_param.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../model/model_v2/model_v2_1_param.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: BATCH_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39midx2word)\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m \u001b[43msave_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/INM706-image-captioning/code/utils.py:248\u001b[0m, in \u001b[0;36msave_params\u001b[0;34m(path, batch_size, embed_size, hidden_size, num_layers, vocab_size)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_params\u001b[39m(path, batch_size, embed_size, hidden_size, num_layers, vocab_size):\n\u001b[1;32m    241\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: batch_size,\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed_size\u001b[39m\u001b[38;5;124m'\u001b[39m: embed_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m'\u001b[39m: vocab_size\n\u001b[1;32m    247\u001b[0m     }\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m    249\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(params, outfile)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../model/model_v2/model_v2_1_param.json'"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67378d-6e62-4b7c-9557-ea91b2f06802",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder_ = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e938c-e9cc-47bd-9290-343304a18e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder_.parameters()) + list(encoder_.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':3e-4, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d3cbe-6ae1-4619-98ac-a3f15e3f7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder_,\n",
    "    'decoder': decoder_,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa16cd-fa96-4164-923f-8fedfc4c78d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
