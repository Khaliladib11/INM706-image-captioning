{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09b4c42",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a3db4",
   "metadata": {},
   "source": [
    "In this notebook, we will run models, also this notebook can be a template to run other models with different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606af7ca",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fccfe284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_loader import get_loader\n",
    "from models import Encoder, Decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from data_prep_utils import *\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff406e5e",
   "metadata": {},
   "source": [
    "## Load train and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd484d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "#IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "#CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'random_v2'\n",
    "\n",
    "# for data loader\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a247cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50026 captions in the data set\n",
      "With FREQ_THRESHOLD = 5, vocab size is 3389\n"
     ]
    }
   ],
   "source": [
    "# create custom data set if we need it. We can choose to work with certain types\n",
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "# prepare_datasets(train_percent = 0.87, super_categories=None,\n",
    "#                  max_train=10000, max_val=2000, max_test=2000,\n",
    "#                  save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file=f'{CAPTIONS_NAME}_captions_train.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d977f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4115c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 391, Length of testing dataloader: 47\n",
      "Length of vocabulary: 3389\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_loader)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde1d50",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593a54d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8405f913-3836-4994-b6a8-69f6d5f81ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for encoder and decoder\n",
    "EMBED_SIZE = 1024 # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3 #hidden layers in LTSM\n",
    "vocab_size = len(train_dataset.vocab.idx2word)\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 50\n",
    "CHECKPOINT = '../model/model_v2'\n",
    "PRINT_EVERY = 50 # run print_every batches and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a5f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c755db3-cc25-4452-a6da-a32bc47bc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(CHECKPOINT, 'word2idx.json'), \"w\") as outfile:\n",
    "    json.dump(word2idx, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda6392-2c98-4cb4-81ef-e96c21d4c631",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a086978",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder_ = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e07a2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder_.parameters()) + list(encoder_.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':3e-4, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296dccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/50]          || Step: [0/391]         || Average Training Loss: 8.1361\n",
      "Epoch: [0/50]          || Step: [50/391]        || Average Training Loss: 6.1486\n",
      "Epoch: [0/50]          || Step: [100/391]       || Average Training Loss: 5.4955\n",
      "Epoch: [0/50]          || Step: [150/391]       || Average Training Loss: 5.2212\n",
      "Epoch: [0/50]          || Step: [200/391]       || Average Training Loss: 5.0248\n",
      "Epoch: [0/50]          || Step: [250/391]       || Average Training Loss: 4.8700\n",
      "Epoch: [0/50]          || Step: [300/391]       || Average Training Loss: 4.7425\n",
      "Epoch: [0/50]          || Step: [350/391]       || Average Training Loss: 4.6387\n",
      "Epoch: [0/50]          || Step: [0/47]          || Average Validation Loss: 3.9465\n",
      "****************************************************************************************************\n",
      "Epoch: [0/50] || Training Loss = 4.57 || Validation Loss: 3.86 || Time: 19.594954\n",
      "****************************************************************************************************\n",
      "Epoch: [1/50]          || Step: [0/391]         || Average Training Loss: 3.9838\n",
      "Epoch: [1/50]          || Step: [50/391]        || Average Training Loss: 3.8713\n",
      "Epoch: [1/50]          || Step: [100/391]       || Average Training Loss: 3.8353\n",
      "Epoch: [1/50]          || Step: [150/391]       || Average Training Loss: 3.8127\n",
      "Epoch: [1/50]          || Step: [200/391]       || Average Training Loss: 3.7933\n",
      "Epoch: [1/50]          || Step: [250/391]       || Average Training Loss: 3.7718\n",
      "Epoch: [1/50]          || Step: [300/391]       || Average Training Loss: 3.7522\n",
      "Epoch: [1/50]          || Step: [350/391]       || Average Training Loss: 3.7337\n",
      "Epoch: [1/50]          || Step: [0/47]          || Average Validation Loss: 3.5996\n",
      "****************************************************************************************************\n",
      "Epoch: [1/50] || Training Loss = 3.72 || Validation Loss: 3.55 || Time: 38.791010\n",
      "****************************************************************************************************\n",
      "Epoch: [2/50]          || Step: [0/391]         || Average Training Loss: 3.4455\n",
      "Epoch: [2/50]          || Step: [50/391]        || Average Training Loss: 3.5563\n",
      "Epoch: [2/50]          || Step: [100/391]       || Average Training Loss: 3.5550\n",
      "Epoch: [2/50]          || Step: [150/391]       || Average Training Loss: 3.5412\n",
      "Epoch: [2/50]          || Step: [200/391]       || Average Training Loss: 3.5286\n",
      "Epoch: [2/50]          || Step: [250/391]       || Average Training Loss: 3.5121\n",
      "Epoch: [2/50]          || Step: [300/391]       || Average Training Loss: 3.4954\n",
      "Epoch: [2/50]          || Step: [350/391]       || Average Training Loss: 3.4775\n",
      "Epoch: [2/50]          || Step: [0/47]          || Average Validation Loss: 3.3854\n",
      "****************************************************************************************************\n",
      "Epoch: [2/50] || Training Loss = 3.47 || Validation Loss: 3.30 || Time: 57.472753\n",
      "****************************************************************************************************\n",
      "Epoch: [3/50]          || Step: [0/391]         || Average Training Loss: 3.4080\n",
      "Epoch: [3/50]          || Step: [50/391]        || Average Training Loss: 3.2945\n",
      "Epoch: [3/50]          || Step: [100/391]       || Average Training Loss: 3.3003\n",
      "Epoch: [3/50]          || Step: [150/391]       || Average Training Loss: 3.2944\n",
      "Epoch: [3/50]          || Step: [200/391]       || Average Training Loss: 3.2807\n",
      "Epoch: [3/50]          || Step: [250/391]       || Average Training Loss: 3.2731\n",
      "Epoch: [3/50]          || Step: [300/391]       || Average Training Loss: 3.2666\n",
      "Epoch: [3/50]          || Step: [350/391]       || Average Training Loss: 3.2576\n",
      "Epoch: [3/50]          || Step: [0/47]          || Average Validation Loss: 3.1352\n",
      "****************************************************************************************************\n",
      "Epoch: [3/50] || Training Loss = 3.25 || Validation Loss: 3.18 || Time: 76.109224\n",
      "****************************************************************************************************\n",
      "Epoch: [4/50]          || Step: [0/391]         || Average Training Loss: 3.1855\n",
      "Epoch: [4/50]          || Step: [50/391]        || Average Training Loss: 3.1661\n",
      "Epoch: [4/50]          || Step: [100/391]       || Average Training Loss: 3.1608\n",
      "Epoch: [4/50]          || Step: [150/391]       || Average Training Loss: 3.1516\n",
      "Epoch: [4/50]          || Step: [200/391]       || Average Training Loss: 3.1476\n",
      "Epoch: [4/50]          || Step: [250/391]       || Average Training Loss: 3.1452\n",
      "Epoch: [4/50]          || Step: [300/391]       || Average Training Loss: 3.1419\n",
      "Epoch: [4/50]          || Step: [350/391]       || Average Training Loss: 3.1407\n",
      "Epoch: [4/50]          || Step: [0/47]          || Average Validation Loss: 3.2093\n",
      "****************************************************************************************************\n",
      "Epoch: [4/50] || Training Loss = 3.14 || Validation Loss: 3.10 || Time: 94.623314\n",
      "****************************************************************************************************\n",
      "Epoch: [5/50]          || Step: [0/391]         || Average Training Loss: 3.0766\n",
      "Epoch: [5/50]          || Step: [50/391]        || Average Training Loss: 3.0789\n",
      "Epoch: [5/50]          || Step: [100/391]       || Average Training Loss: 3.0871\n",
      "Epoch: [5/50]          || Step: [150/391]       || Average Training Loss: 3.0787\n",
      "Epoch: [5/50]          || Step: [200/391]       || Average Training Loss: 3.0734\n",
      "Epoch: [5/50]          || Step: [250/391]       || Average Training Loss: 3.0692\n",
      "Epoch: [5/50]          || Step: [300/391]       || Average Training Loss: 3.0671\n",
      "Epoch: [5/50]          || Step: [350/391]       || Average Training Loss: 3.0623\n",
      "Epoch: [5/50]          || Step: [0/47]          || Average Validation Loss: 3.0137\n",
      "****************************************************************************************************\n",
      "Epoch: [5/50] || Training Loss = 3.06 || Validation Loss: 3.02 || Time: 113.784356\n",
      "****************************************************************************************************\n",
      "Epoch: [6/50]          || Step: [0/391]         || Average Training Loss: 2.9547\n",
      "Epoch: [6/50]          || Step: [50/391]        || Average Training Loss: 3.0246\n",
      "Epoch: [6/50]          || Step: [100/391]       || Average Training Loss: 3.0111\n",
      "Epoch: [6/50]          || Step: [150/391]       || Average Training Loss: 3.0089\n",
      "Epoch: [6/50]          || Step: [200/391]       || Average Training Loss: 3.0092\n",
      "Epoch: [6/50]          || Step: [250/391]       || Average Training Loss: 3.0052\n",
      "Epoch: [6/50]          || Step: [300/391]       || Average Training Loss: 3.0056\n",
      "Epoch: [6/50]          || Step: [350/391]       || Average Training Loss: 3.0058\n",
      "Epoch: [6/50]          || Step: [0/47]          || Average Validation Loss: 3.0653\n",
      "****************************************************************************************************\n",
      "Epoch: [6/50] || Training Loss = 3.00 || Validation Loss: 2.97 || Time: 133.274973\n",
      "****************************************************************************************************\n",
      "Epoch: [7/50]          || Step: [0/391]         || Average Training Loss: 2.8974\n",
      "Epoch: [7/50]          || Step: [50/391]        || Average Training Loss: 2.9745\n",
      "Epoch: [7/50]          || Step: [100/391]       || Average Training Loss: 2.9721\n",
      "Epoch: [7/50]          || Step: [150/391]       || Average Training Loss: 2.9681\n",
      "Epoch: [7/50]          || Step: [200/391]       || Average Training Loss: 2.9702\n",
      "Epoch: [7/50]          || Step: [250/391]       || Average Training Loss: 2.9692\n",
      "Epoch: [7/50]          || Step: [300/391]       || Average Training Loss: 2.9686\n",
      "Epoch: [7/50]          || Step: [350/391]       || Average Training Loss: 2.9681\n",
      "Epoch: [7/50]          || Step: [0/47]          || Average Validation Loss: 2.9297\n",
      "****************************************************************************************************\n",
      "Epoch: [7/50] || Training Loss = 2.97 || Validation Loss: 2.95 || Time: 156.947059\n",
      "****************************************************************************************************\n",
      "Epoch: [8/50]          || Step: [0/391]         || Average Training Loss: 2.9240\n",
      "Epoch: [8/50]          || Step: [50/391]        || Average Training Loss: 2.9277\n",
      "Epoch: [8/50]          || Step: [100/391]       || Average Training Loss: 2.9369\n",
      "Epoch: [8/50]          || Step: [150/391]       || Average Training Loss: 2.9394\n",
      "Epoch: [8/50]          || Step: [200/391]       || Average Training Loss: 2.9390\n",
      "Epoch: [8/50]          || Step: [250/391]       || Average Training Loss: 2.9399\n",
      "Epoch: [8/50]          || Step: [300/391]       || Average Training Loss: 2.9393\n",
      "Epoch: [8/50]          || Step: [350/391]       || Average Training Loss: 2.9367\n",
      "Epoch: [8/50]          || Step: [0/47]          || Average Validation Loss: 2.9650\n",
      "Epoch: [9/50]          || Step: [50/391]        || Average Training Loss: 2.9278\n",
      "Epoch: [9/50]          || Step: [100/391]       || Average Training Loss: 2.9215\n",
      "Epoch: [9/50]          || Step: [150/391]       || Average Training Loss: 2.9211\n",
      "Epoch: [9/50]          || Step: [200/391]       || Average Training Loss: 2.9151\n",
      "Epoch: [9/50]          || Step: [250/391]       || Average Training Loss: 2.9152\n",
      "Epoch: [9/50]          || Step: [300/391]       || Average Training Loss: 2.9176\n",
      "Epoch: [9/50]          || Step: [350/391]       || Average Training Loss: 2.9159\n",
      "Epoch: [9/50]          || Step: [0/47]          || Average Validation Loss: 2.8023\n",
      "****************************************************************************************************\n",
      "Epoch: [9/50] || Training Loss = 2.92 || Validation Loss: 2.90 || Time: 196.161387\n",
      "****************************************************************************************************\n",
      "Epoch: [10/50]         || Step: [0/391]         || Average Training Loss: 2.8981\n",
      "Epoch: [10/50]         || Step: [50/391]        || Average Training Loss: 2.8892\n",
      "Epoch: [10/50]         || Step: [100/391]       || Average Training Loss: 2.8947\n",
      "Epoch: [10/50]         || Step: [150/391]       || Average Training Loss: 2.8934\n",
      "Epoch: [10/50]         || Step: [200/391]       || Average Training Loss: 2.8917\n",
      "Epoch: [10/50]         || Step: [250/391]       || Average Training Loss: 2.8929\n",
      "Epoch: [10/50]         || Step: [300/391]       || Average Training Loss: 2.8930\n",
      "Epoch: [10/50]         || Step: [350/391]       || Average Training Loss: 2.8936\n",
      "Epoch: [10/50]         || Step: [0/47]          || Average Validation Loss: 2.9822\n",
      "****************************************************************************************************\n",
      "Epoch: [10/50] || Training Loss = 2.90 || Validation Loss: 2.88 || Time: 215.977560\n",
      "****************************************************************************************************\n",
      "Epoch: [11/50]         || Step: [0/391]         || Average Training Loss: 2.7539\n",
      "Epoch: [11/50]         || Step: [50/391]        || Average Training Loss: 2.8645\n",
      "Epoch: [11/50]         || Step: [100/391]       || Average Training Loss: 2.8741\n",
      "Epoch: [11/50]         || Step: [150/391]       || Average Training Loss: 2.8746\n",
      "Epoch: [11/50]         || Step: [200/391]       || Average Training Loss: 2.8788\n",
      "Epoch: [11/50]         || Step: [250/391]       || Average Training Loss: 2.8818\n",
      "Epoch: [11/50]         || Step: [300/391]       || Average Training Loss: 2.8811\n",
      "Epoch: [11/50]         || Step: [350/391]       || Average Training Loss: 2.8804\n",
      "Epoch: [11/50]         || Step: [0/47]          || Average Validation Loss: 2.8251\n",
      "****************************************************************************************************\n",
      "Epoch: [11/50] || Training Loss = 2.88 || Validation Loss: 2.88 || Time: 235.928063\n",
      "****************************************************************************************************\n",
      "Epoch: [12/50]         || Step: [0/391]         || Average Training Loss: 2.9190\n",
      "Epoch: [12/50]         || Step: [50/391]        || Average Training Loss: 2.8467\n",
      "Epoch: [12/50]         || Step: [100/391]       || Average Training Loss: 2.8551\n",
      "Epoch: [12/50]         || Step: [150/391]       || Average Training Loss: 2.8550\n",
      "Epoch: [12/50]         || Step: [200/391]       || Average Training Loss: 2.8626\n",
      "Epoch: [12/50]         || Step: [250/391]       || Average Training Loss: 2.8656\n",
      "Epoch: [12/50]         || Step: [300/391]       || Average Training Loss: 2.8698\n",
      "Epoch: [12/50]         || Step: [350/391]       || Average Training Loss: 2.8696\n",
      "Epoch: [12/50]         || Step: [0/47]          || Average Validation Loss: 2.9667\n",
      "****************************************************************************************************\n",
      "Epoch: [12/50] || Training Loss = 2.87 || Validation Loss: 2.86 || Time: 255.779274\n",
      "****************************************************************************************************\n",
      "Epoch: [13/50]         || Step: [0/391]         || Average Training Loss: 2.8224\n",
      "Epoch: [13/50]         || Step: [50/391]        || Average Training Loss: 2.8517\n",
      "Epoch: [13/50]         || Step: [100/391]       || Average Training Loss: 2.8525\n",
      "Epoch: [13/50]         || Step: [150/391]       || Average Training Loss: 2.8535\n",
      "Epoch: [13/50]         || Step: [200/391]       || Average Training Loss: 2.8584\n",
      "Epoch: [13/50]         || Step: [250/391]       || Average Training Loss: 2.8632\n",
      "Epoch: [13/50]         || Step: [300/391]       || Average Training Loss: 2.8642\n",
      "Epoch: [13/50]         || Step: [350/391]       || Average Training Loss: 2.8634\n",
      "Epoch: [13/50]         || Step: [0/47]          || Average Validation Loss: 2.8798\n",
      "****************************************************************************************************\n",
      "Epoch: [13/50] || Training Loss = 2.86 || Validation Loss: 2.85 || Time: 275.521717\n",
      "****************************************************************************************************\n",
      "Epoch: [14/50]         || Step: [0/391]         || Average Training Loss: 2.8526\n",
      "Epoch: [14/50]         || Step: [50/391]        || Average Training Loss: 2.8711\n",
      "Epoch: [14/50]         || Step: [100/391]       || Average Training Loss: 2.8578\n",
      "Epoch: [14/50]         || Step: [150/391]       || Average Training Loss: 2.8527\n",
      "Epoch: [14/50]         || Step: [200/391]       || Average Training Loss: 2.8560\n",
      "Epoch: [14/50]         || Step: [250/391]       || Average Training Loss: 2.8595\n",
      "Epoch: [14/50]         || Step: [300/391]       || Average Training Loss: 2.8598\n",
      "Epoch: [14/50]         || Step: [350/391]       || Average Training Loss: 2.8587\n",
      "Epoch: [14/50]         || Step: [0/47]          || Average Validation Loss: 2.7965\n",
      "****************************************************************************************************\n",
      "Epoch: [14/50] || Training Loss = 2.86 || Validation Loss: 2.86 || Time: 295.268861\n",
      "****************************************************************************************************\n",
      "Epoch: [15/50]         || Step: [0/391]         || Average Training Loss: 2.7843\n",
      "Epoch: [15/50]         || Step: [50/391]        || Average Training Loss: 2.8461\n",
      "Epoch: [15/50]         || Step: [100/391]       || Average Training Loss: 2.8457\n",
      "Epoch: [15/50]         || Step: [150/391]       || Average Training Loss: 2.8389\n",
      "Epoch: [15/50]         || Step: [200/391]       || Average Training Loss: 2.8421\n",
      "Epoch: [15/50]         || Step: [250/391]       || Average Training Loss: 2.8427\n",
      "Epoch: [15/50]         || Step: [300/391]       || Average Training Loss: 2.8453\n",
      "Epoch: [15/50]         || Step: [350/391]       || Average Training Loss: 2.8493\n",
      "Epoch: [15/50]         || Step: [0/47]          || Average Validation Loss: 2.8925\n",
      "****************************************************************************************************\n",
      "Epoch: [15/50] || Training Loss = 2.85 || Validation Loss: 2.86 || Time: 315.156587\n",
      "****************************************************************************************************\n",
      "Epoch: [16/50]         || Step: [0/391]         || Average Training Loss: 2.8984\n",
      "Epoch: [16/50]         || Step: [50/391]        || Average Training Loss: 2.8399\n",
      "Epoch: [16/50]         || Step: [100/391]       || Average Training Loss: 2.8402\n",
      "Epoch: [16/50]         || Step: [150/391]       || Average Training Loss: 2.8449\n",
      "Epoch: [16/50]         || Step: [200/391]       || Average Training Loss: 2.8446\n",
      "Epoch: [16/50]         || Step: [250/391]       || Average Training Loss: 2.8467\n",
      "Epoch: [16/50]         || Step: [300/391]       || Average Training Loss: 2.8478\n",
      "Epoch: [16/50]         || Step: [350/391]       || Average Training Loss: 2.8474\n",
      "Epoch: [16/50]         || Step: [0/47]          || Average Validation Loss: 2.7490\n",
      "****************************************************************************************************\n",
      "Epoch: [16/50] || Training Loss = 2.85 || Validation Loss: 2.85 || Time: 334.978920\n",
      "****************************************************************************************************\n",
      "Epoch: [17/50]         || Step: [0/391]         || Average Training Loss: 2.8201\n",
      "Epoch: [17/50]         || Step: [50/391]        || Average Training Loss: 2.8359\n",
      "Epoch: [17/50]         || Step: [100/391]       || Average Training Loss: 2.8321\n",
      "Epoch: [17/50]         || Step: [150/391]       || Average Training Loss: 2.8318\n",
      "Epoch: [17/50]         || Step: [200/391]       || Average Training Loss: 2.8367\n",
      "Epoch: [17/50]         || Step: [250/391]       || Average Training Loss: 2.8372\n",
      "Epoch: [17/50]         || Step: [300/391]       || Average Training Loss: 2.8416\n",
      "Epoch: [17/50]         || Step: [350/391]       || Average Training Loss: 2.8423\n",
      "Epoch: [17/50]         || Step: [0/47]          || Average Validation Loss: 2.8607\n",
      "****************************************************************************************************\n",
      "Epoch: [17/50] || Training Loss = 2.84 || Validation Loss: 2.84 || Time: 354.837484\n",
      "****************************************************************************************************\n",
      "Epoch: [18/50]         || Step: [0/391]         || Average Training Loss: 2.8127\n",
      "Epoch: [18/50]         || Step: [50/391]        || Average Training Loss: 2.8363\n",
      "Epoch: [18/50]         || Step: [100/391]       || Average Training Loss: 2.8341\n",
      "Epoch: [18/50]         || Step: [150/391]       || Average Training Loss: 2.8383\n",
      "Epoch: [18/50]         || Step: [200/391]       || Average Training Loss: 2.8322\n",
      "Epoch: [18/50]         || Step: [250/391]       || Average Training Loss: 2.8316\n",
      "Epoch: [18/50]         || Step: [300/391]       || Average Training Loss: 2.8359\n",
      "Epoch: [18/50]         || Step: [350/391]       || Average Training Loss: 2.8379\n",
      "Epoch: [18/50]         || Step: [0/47]          || Average Validation Loss: 2.7796\n",
      "****************************************************************************************************\n",
      "Epoch: [18/50] || Training Loss = 2.84 || Validation Loss: 2.85 || Time: 374.443970\n",
      "****************************************************************************************************\n",
      "Epoch: [19/50]         || Step: [0/391]         || Average Training Loss: 2.8154\n",
      "Epoch: [19/50]         || Step: [50/391]        || Average Training Loss: 2.8380\n",
      "Epoch: [19/50]         || Step: [100/391]       || Average Training Loss: 2.8328\n",
      "Epoch: [19/50]         || Step: [150/391]       || Average Training Loss: 2.8378\n",
      "Epoch: [19/50]         || Step: [200/391]       || Average Training Loss: 2.8316\n",
      "Epoch: [19/50]         || Step: [250/391]       || Average Training Loss: 2.8300\n",
      "Epoch: [19/50]         || Step: [300/391]       || Average Training Loss: 2.8357\n",
      "Epoch: [19/50]         || Step: [350/391]       || Average Training Loss: 2.8350\n",
      "Epoch: [19/50]         || Step: [0/47]          || Average Validation Loss: 2.8163\n",
      "****************************************************************************************************\n",
      "Epoch: [19/50] || Training Loss = 2.84 || Validation Loss: 2.83 || Time: 393.904815\n",
      "****************************************************************************************************\n",
      "Epoch: [20/50]         || Step: [0/391]         || Average Training Loss: 2.8267\n",
      "Epoch: [20/50]         || Step: [50/391]        || Average Training Loss: 2.8046\n",
      "Epoch: [20/50]         || Step: [100/391]       || Average Training Loss: 2.8151\n",
      "Epoch: [20/50]         || Step: [150/391]       || Average Training Loss: 2.8278\n",
      "Epoch: [20/50]         || Step: [200/391]       || Average Training Loss: 2.8326\n",
      "Epoch: [20/50]         || Step: [250/391]       || Average Training Loss: 2.8333\n",
      "Epoch: [20/50]         || Step: [300/391]       || Average Training Loss: 2.8294\n",
      "Epoch: [20/50]         || Step: [350/391]       || Average Training Loss: 2.8312\n",
      "Epoch: [20/50]         || Step: [0/47]          || Average Validation Loss: 2.8588\n",
      "****************************************************************************************************\n",
      "Epoch: [20/50] || Training Loss = 2.83 || Validation Loss: 2.85 || Time: 412.355657\n",
      "****************************************************************************************************\n",
      "Epoch: [21/50]         || Step: [0/391]         || Average Training Loss: 2.8604\n",
      "Epoch: [21/50]         || Step: [50/391]        || Average Training Loss: 2.8055\n",
      "Epoch: [21/50]         || Step: [100/391]       || Average Training Loss: 2.8078\n",
      "Epoch: [21/50]         || Step: [150/391]       || Average Training Loss: 2.8113\n",
      "Epoch: [21/50]         || Step: [200/391]       || Average Training Loss: 2.8175\n",
      "Epoch: [21/50]         || Step: [250/391]       || Average Training Loss: 2.8234\n",
      "Epoch: [21/50]         || Step: [300/391]       || Average Training Loss: 2.8248\n",
      "Epoch: [21/50]         || Step: [350/391]       || Average Training Loss: 2.8320\n",
      "Epoch: [21/50]         || Step: [0/47]          || Average Validation Loss: 2.7138\n",
      "****************************************************************************************************\n",
      "Epoch: [21/50] || Training Loss = 2.83 || Validation Loss: 2.83 || Time: 430.378337\n",
      "****************************************************************************************************\n",
      "Epoch: [22/50]         || Step: [0/391]         || Average Training Loss: 3.0551\n",
      "Epoch: [22/50]         || Step: [50/391]        || Average Training Loss: 2.8297\n",
      "Epoch: [22/50]         || Step: [100/391]       || Average Training Loss: 2.8207\n",
      "Epoch: [22/50]         || Step: [150/391]       || Average Training Loss: 2.8180\n",
      "Epoch: [22/50]         || Step: [200/391]       || Average Training Loss: 2.8216\n",
      "Epoch: [22/50]         || Step: [250/391]       || Average Training Loss: 2.8251\n",
      "Epoch: [22/50]         || Step: [300/391]       || Average Training Loss: 2.8260\n",
      "Epoch: [22/50]         || Step: [350/391]       || Average Training Loss: 2.8265\n",
      "Epoch: [22/50]         || Step: [0/47]          || Average Validation Loss: 2.8766\n",
      "****************************************************************************************************\n",
      "Epoch: [22/50] || Training Loss = 2.83 || Validation Loss: 2.83 || Time: 448.334231\n",
      "****************************************************************************************************\n",
      "Epoch: [23/50]         || Step: [0/391]         || Average Training Loss: 2.7433\n",
      "Epoch: [23/50]         || Step: [50/391]        || Average Training Loss: 2.8111\n",
      "Epoch: [23/50]         || Step: [100/391]       || Average Training Loss: 2.8010\n",
      "Epoch: [23/50]         || Step: [150/391]       || Average Training Loss: 2.8102\n",
      "Epoch: [23/50]         || Step: [200/391]       || Average Training Loss: 2.8169\n",
      "Epoch: [23/50]         || Step: [250/391]       || Average Training Loss: 2.8227\n",
      "Epoch: [23/50]         || Step: [300/391]       || Average Training Loss: 2.8225\n",
      "Epoch: [23/50]         || Step: [350/391]       || Average Training Loss: 2.8258\n",
      "Epoch: [23/50]         || Step: [0/47]          || Average Validation Loss: 2.8718\n",
      "****************************************************************************************************\n",
      "Epoch: [23/50] || Training Loss = 2.83 || Validation Loss: 2.82 || Time: 467.603068\n",
      "****************************************************************************************************\n",
      "Epoch: [24/50]         || Step: [0/391]         || Average Training Loss: 2.6994\n",
      "Epoch: [24/50]         || Step: [50/391]        || Average Training Loss: 2.8219\n",
      "Epoch: [24/50]         || Step: [100/391]       || Average Training Loss: 2.8119\n",
      "Epoch: [24/50]         || Step: [150/391]       || Average Training Loss: 2.8173\n",
      "Epoch: [24/50]         || Step: [200/391]       || Average Training Loss: 2.8211\n",
      "Epoch: [24/50]         || Step: [250/391]       || Average Training Loss: 2.8224\n",
      "Epoch: [24/50]         || Step: [300/391]       || Average Training Loss: 2.8217\n",
      "Epoch: [24/50]         || Step: [350/391]       || Average Training Loss: 2.8229\n",
      "Epoch: [24/50]         || Step: [0/47]          || Average Validation Loss: 2.8637\n",
      "****************************************************************************************************\n",
      "Epoch: [24/50] || Training Loss = 2.82 || Validation Loss: 2.83 || Time: 486.911529\n",
      "****************************************************************************************************\n",
      "Epoch: [25/50]         || Step: [0/391]         || Average Training Loss: 2.9500\n",
      "Epoch: [25/50]         || Step: [50/391]        || Average Training Loss: 2.7826\n",
      "Epoch: [25/50]         || Step: [100/391]       || Average Training Loss: 2.8120\n",
      "Epoch: [25/50]         || Step: [150/391]       || Average Training Loss: 2.8146\n",
      "Epoch: [25/50]         || Step: [200/391]       || Average Training Loss: 2.8180\n",
      "Epoch: [25/50]         || Step: [250/391]       || Average Training Loss: 2.8189\n",
      "Epoch: [25/50]         || Step: [300/391]       || Average Training Loss: 2.8188\n",
      "Epoch: [25/50]         || Step: [350/391]       || Average Training Loss: 2.8211\n",
      "Epoch: [25/50]         || Step: [0/47]          || Average Validation Loss: 2.8005\n",
      "****************************************************************************************************\n",
      "Epoch: [25/50] || Training Loss = 2.82 || Validation Loss: 2.83 || Time: 506.181128\n",
      "****************************************************************************************************\n",
      "Epoch: [26/50]         || Step: [0/391]         || Average Training Loss: 2.7539\n",
      "Epoch: [26/50]         || Step: [50/391]        || Average Training Loss: 2.7929\n",
      "Epoch: [26/50]         || Step: [100/391]       || Average Training Loss: 2.8094\n",
      "Epoch: [26/50]         || Step: [150/391]       || Average Training Loss: 2.8129\n",
      "Epoch: [26/50]         || Step: [200/391]       || Average Training Loss: 2.8202\n",
      "Epoch: [26/50]         || Step: [250/391]       || Average Training Loss: 2.8272\n",
      "Epoch: [26/50]         || Step: [300/391]       || Average Training Loss: 2.8235\n",
      "Epoch: [26/50]         || Step: [350/391]       || Average Training Loss: 2.8227\n",
      "Epoch: [26/50]         || Step: [0/47]          || Average Validation Loss: 2.7357\n",
      "****************************************************************************************************\n",
      "Epoch: [26/50] || Training Loss = 2.82 || Validation Loss: 2.83 || Time: 526.014157\n",
      "****************************************************************************************************\n",
      "Epoch: [27/50]         || Step: [0/391]         || Average Training Loss: 2.7492\n",
      "Epoch: [27/50]         || Step: [50/391]        || Average Training Loss: 2.8082\n",
      "Epoch: [27/50]         || Step: [100/391]       || Average Training Loss: 2.8103\n",
      "Epoch: [27/50]         || Step: [150/391]       || Average Training Loss: 2.8103\n",
      "Epoch: [27/50]         || Step: [200/391]       || Average Training Loss: 2.8102\n",
      "Epoch: [27/50]         || Step: [250/391]       || Average Training Loss: 2.8162\n",
      "Epoch: [27/50]         || Step: [300/391]       || Average Training Loss: 2.8183\n",
      "Epoch: [27/50]         || Step: [350/391]       || Average Training Loss: 2.8177\n",
      "Epoch: [27/50]         || Step: [0/47]          || Average Validation Loss: 2.7527\n",
      "****************************************************************************************************\n",
      "Epoch: [27/50] || Training Loss = 2.82 || Validation Loss: 2.82 || Time: 545.834349\n",
      "****************************************************************************************************\n",
      "Epoch: [28/50]         || Step: [0/391]         || Average Training Loss: 2.9564\n",
      "Epoch: [28/50]         || Step: [50/391]        || Average Training Loss: 2.7922\n",
      "Epoch: [28/50]         || Step: [100/391]       || Average Training Loss: 2.7996\n",
      "Epoch: [28/50]         || Step: [150/391]       || Average Training Loss: 2.8059\n",
      "Epoch: [28/50]         || Step: [200/391]       || Average Training Loss: 2.8140\n",
      "Epoch: [28/50]         || Step: [250/391]       || Average Training Loss: 2.8148\n",
      "Epoch: [28/50]         || Step: [300/391]       || Average Training Loss: 2.8149\n",
      "Epoch: [28/50]         || Step: [350/391]       || Average Training Loss: 2.8178\n",
      "Epoch: [28/50]         || Step: [0/47]          || Average Validation Loss: 2.8599\n",
      "****************************************************************************************************\n",
      "Epoch: [28/50] || Training Loss = 2.82 || Validation Loss: 2.81 || Time: 565.305786\n",
      "****************************************************************************************************\n",
      "Epoch: [29/50]         || Step: [0/391]         || Average Training Loss: 2.7584\n",
      "Epoch: [29/50]         || Step: [50/391]        || Average Training Loss: 2.8128\n",
      "Epoch: [29/50]         || Step: [100/391]       || Average Training Loss: 2.8107\n",
      "Epoch: [29/50]         || Step: [150/391]       || Average Training Loss: 2.8121\n",
      "Epoch: [29/50]         || Step: [200/391]       || Average Training Loss: 2.8133\n",
      "Epoch: [29/50]         || Step: [250/391]       || Average Training Loss: 2.8140\n",
      "Epoch: [29/50]         || Step: [300/391]       || Average Training Loss: 2.8153\n",
      "Epoch: [29/50]         || Step: [350/391]       || Average Training Loss: 2.8162\n",
      "Epoch: [29/50]         || Step: [0/47]          || Average Validation Loss: 2.8331\n",
      "****************************************************************************************************\n",
      "Epoch: [29/50] || Training Loss = 2.82 || Validation Loss: 2.84 || Time: 581.118859\n",
      "****************************************************************************************************\n",
      "Epoch: [30/50]         || Step: [0/391]         || Average Training Loss: 2.8481\n",
      "Epoch: [30/50]         || Step: [50/391]        || Average Training Loss: 2.8098\n",
      "Epoch: [30/50]         || Step: [100/391]       || Average Training Loss: 2.8025\n",
      "Epoch: [30/50]         || Step: [150/391]       || Average Training Loss: 2.8102\n",
      "Epoch: [30/50]         || Step: [200/391]       || Average Training Loss: 2.8138\n",
      "Epoch: [30/50]         || Step: [250/391]       || Average Training Loss: 2.8135\n",
      "Epoch: [30/50]         || Step: [300/391]       || Average Training Loss: 2.8154\n",
      "Epoch: [30/50]         || Step: [350/391]       || Average Training Loss: 2.8138\n",
      "Epoch: [30/50]         || Step: [0/47]          || Average Validation Loss: 2.8711\n",
      "****************************************************************************************************\n",
      "Epoch: [30/50] || Training Loss = 2.81 || Validation Loss: 2.81 || Time: 598.377700\n",
      "****************************************************************************************************\n",
      "Epoch: [31/50]         || Step: [0/391]         || Average Training Loss: 2.8675\n",
      "Epoch: [31/50]         || Step: [50/391]        || Average Training Loss: 2.7970\n",
      "Epoch: [31/50]         || Step: [100/391]       || Average Training Loss: 2.8036\n",
      "Epoch: [31/50]         || Step: [150/391]       || Average Training Loss: 2.8103\n",
      "Epoch: [31/50]         || Step: [200/391]       || Average Training Loss: 2.8101\n",
      "Epoch: [31/50]         || Step: [250/391]       || Average Training Loss: 2.8102\n",
      "Epoch: [31/50]         || Step: [300/391]       || Average Training Loss: 2.8126\n",
      "Epoch: [31/50]         || Step: [350/391]       || Average Training Loss: 2.8122\n",
      "Epoch: [31/50]         || Step: [0/47]          || Average Validation Loss: 2.7714\n",
      "****************************************************************************************************\n",
      "Epoch: [31/50] || Training Loss = 2.81 || Validation Loss: 2.82 || Time: 618.011294\n",
      "****************************************************************************************************\n",
      "Epoch: [32/50]         || Step: [0/391]         || Average Training Loss: 2.7670\n",
      "Epoch: [32/50]         || Step: [50/391]        || Average Training Loss: 2.7987\n",
      "Epoch: [32/50]         || Step: [100/391]       || Average Training Loss: 2.8027\n",
      "Epoch: [32/50]         || Step: [150/391]       || Average Training Loss: 2.8044\n",
      "Epoch: [32/50]         || Step: [200/391]       || Average Training Loss: 2.8065\n",
      "Epoch: [32/50]         || Step: [250/391]       || Average Training Loss: 2.8052\n",
      "Epoch: [32/50]         || Step: [300/391]       || Average Training Loss: 2.8097\n",
      "Epoch: [32/50]         || Step: [350/391]       || Average Training Loss: 2.8123\n",
      "Epoch: [32/50]         || Step: [0/47]          || Average Validation Loss: 2.9093\n",
      "****************************************************************************************************\n",
      "Epoch: [32/50] || Training Loss = 2.81 || Validation Loss: 2.82 || Time: 637.680203\n",
      "****************************************************************************************************\n",
      "Epoch: [33/50]         || Step: [0/391]         || Average Training Loss: 2.8056\n",
      "Epoch: [33/50]         || Step: [50/391]        || Average Training Loss: 2.8041\n",
      "Epoch: [33/50]         || Step: [100/391]       || Average Training Loss: 2.7987\n",
      "Epoch: [33/50]         || Step: [150/391]       || Average Training Loss: 2.7992\n",
      "Epoch: [33/50]         || Step: [200/391]       || Average Training Loss: 2.8053\n",
      "Epoch: [33/50]         || Step: [250/391]       || Average Training Loss: 2.8067\n",
      "Epoch: [33/50]         || Step: [300/391]       || Average Training Loss: 2.8098\n",
      "Epoch: [33/50]         || Step: [350/391]       || Average Training Loss: 2.8131\n",
      "Epoch: [33/50]         || Step: [0/47]          || Average Validation Loss: 2.8876\n",
      "****************************************************************************************************\n",
      "Epoch: [33/50] || Training Loss = 2.81 || Validation Loss: 2.82 || Time: 657.253474\n",
      "****************************************************************************************************\n",
      "Epoch: [34/50]         || Step: [0/391]         || Average Training Loss: 2.8317\n",
      "Epoch: [34/50]         || Step: [50/391]        || Average Training Loss: 2.8113\n",
      "Epoch: [34/50]         || Step: [100/391]       || Average Training Loss: 2.7996\n",
      "Epoch: [34/50]         || Step: [150/391]       || Average Training Loss: 2.7998\n",
      "Epoch: [34/50]         || Step: [200/391]       || Average Training Loss: 2.8045\n",
      "Epoch: [34/50]         || Step: [250/391]       || Average Training Loss: 2.8065\n",
      "Epoch: [34/50]         || Step: [300/391]       || Average Training Loss: 2.8067\n",
      "Epoch: [34/50]         || Step: [350/391]       || Average Training Loss: 2.8074\n",
      "Epoch: [34/50]         || Step: [0/47]          || Average Validation Loss: 2.6719\n",
      "****************************************************************************************************\n",
      "Epoch: [34/50] || Training Loss = 2.81 || Validation Loss: 2.81 || Time: 677.090601\n",
      "****************************************************************************************************\n",
      "Epoch: [35/50]         || Step: [0/391]         || Average Training Loss: 2.7018\n",
      "Epoch: [35/50]         || Step: [50/391]        || Average Training Loss: 2.7831\n",
      "Epoch: [35/50]         || Step: [100/391]       || Average Training Loss: 2.7942\n",
      "Epoch: [35/50]         || Step: [150/391]       || Average Training Loss: 2.7977\n",
      "Epoch: [35/50]         || Step: [200/391]       || Average Training Loss: 2.7959\n",
      "Epoch: [35/50]         || Step: [250/391]       || Average Training Loss: 2.8010\n",
      "Epoch: [35/50]         || Step: [300/391]       || Average Training Loss: 2.8008\n",
      "Epoch: [35/50]         || Step: [350/391]       || Average Training Loss: 2.8041\n",
      "Epoch: [35/50]         || Step: [0/47]          || Average Validation Loss: 2.7901\n",
      "****************************************************************************************************\n",
      "Epoch: [35/50] || Training Loss = 2.81 || Validation Loss: 2.81 || Time: 696.743643\n",
      "****************************************************************************************************\n",
      "Epoch: [36/50]         || Step: [0/391]         || Average Training Loss: 2.7778\n",
      "Epoch: [36/50]         || Step: [50/391]        || Average Training Loss: 2.7881\n",
      "Epoch: [36/50]         || Step: [100/391]       || Average Training Loss: 2.7977\n",
      "Epoch: [36/50]         || Step: [150/391]       || Average Training Loss: 2.7948\n",
      "Epoch: [36/50]         || Step: [200/391]       || Average Training Loss: 2.8019\n",
      "Epoch: [36/50]         || Step: [250/391]       || Average Training Loss: 2.8025\n",
      "Epoch: [36/50]         || Step: [300/391]       || Average Training Loss: 2.8019\n",
      "Epoch: [36/50]         || Step: [350/391]       || Average Training Loss: 2.8023\n",
      "Epoch: [36/50]         || Step: [0/47]          || Average Validation Loss: 2.7047\n",
      "****************************************************************************************************\n",
      "Epoch: [36/50] || Training Loss = 2.80 || Validation Loss: 2.82 || Time: 715.810678\n",
      "****************************************************************************************************\n",
      "Epoch: [37/50]         || Step: [0/391]         || Average Training Loss: 2.8449\n",
      "Epoch: [37/50]         || Step: [50/391]        || Average Training Loss: 2.7820\n",
      "Epoch: [37/50]         || Step: [100/391]       || Average Training Loss: 2.7817\n",
      "Epoch: [37/50]         || Step: [150/391]       || Average Training Loss: 2.7859\n",
      "Epoch: [37/50]         || Step: [200/391]       || Average Training Loss: 2.7927\n",
      "Epoch: [37/50]         || Step: [250/391]       || Average Training Loss: 2.8006\n",
      "Epoch: [37/50]         || Step: [300/391]       || Average Training Loss: 2.8047\n",
      "Epoch: [37/50]         || Step: [350/391]       || Average Training Loss: 2.8037\n",
      "Epoch: [37/50]         || Step: [0/47]          || Average Validation Loss: 2.7154\n",
      "****************************************************************************************************\n",
      "Epoch: [37/50] || Training Loss = 2.80 || Validation Loss: 2.81 || Time: 733.551590\n",
      "****************************************************************************************************\n",
      "Epoch: [38/50]         || Step: [0/391]         || Average Training Loss: 2.8692\n",
      "Epoch: [38/50]         || Step: [50/391]        || Average Training Loss: 2.7786\n",
      "Epoch: [38/50]         || Step: [100/391]       || Average Training Loss: 2.7900\n",
      "Epoch: [38/50]         || Step: [150/391]       || Average Training Loss: 2.7881\n",
      "Epoch: [38/50]         || Step: [200/391]       || Average Training Loss: 2.7874\n",
      "Epoch: [38/50]         || Step: [250/391]       || Average Training Loss: 2.7912\n",
      "Epoch: [38/50]         || Step: [300/391]       || Average Training Loss: 2.7982\n",
      "Epoch: [38/50]         || Step: [350/391]       || Average Training Loss: 2.7998\n",
      "Epoch: [38/50]         || Step: [0/47]          || Average Validation Loss: 2.7983\n",
      "****************************************************************************************************\n",
      "Epoch: [38/50] || Training Loss = 2.80 || Validation Loss: 2.80 || Time: 752.973297\n",
      "****************************************************************************************************\n",
      "Epoch: [39/50]         || Step: [0/391]         || Average Training Loss: 2.7316\n",
      "Epoch: [39/50]         || Step: [50/391]        || Average Training Loss: 2.7953\n",
      "Epoch: [39/50]         || Step: [100/391]       || Average Training Loss: 2.7767\n",
      "Epoch: [39/50]         || Step: [150/391]       || Average Training Loss: 2.7868\n",
      "Epoch: [39/50]         || Step: [200/391]       || Average Training Loss: 2.7899\n",
      "Epoch: [39/50]         || Step: [250/391]       || Average Training Loss: 2.7935\n",
      "Epoch: [39/50]         || Step: [300/391]       || Average Training Loss: 2.7932\n",
      "Epoch: [39/50]         || Step: [350/391]       || Average Training Loss: 2.7972\n",
      "Epoch: [39/50]         || Step: [0/47]          || Average Validation Loss: 2.8649\n",
      "****************************************************************************************************\n",
      "Epoch: [39/50] || Training Loss = 2.80 || Validation Loss: 2.80 || Time: 775.133922\n",
      "****************************************************************************************************\n",
      "Epoch: [40/50]         || Step: [0/391]         || Average Training Loss: 2.8376\n",
      "Epoch: [40/50]         || Step: [50/391]        || Average Training Loss: 2.7833\n",
      "Epoch: [40/50]         || Step: [100/391]       || Average Training Loss: 2.7929\n",
      "Epoch: [40/50]         || Step: [150/391]       || Average Training Loss: 2.7932\n",
      "Epoch: [40/50]         || Step: [200/391]       || Average Training Loss: 2.7946\n",
      "Epoch: [40/50]         || Step: [250/391]       || Average Training Loss: 2.7968\n",
      "Epoch: [40/50]         || Step: [300/391]       || Average Training Loss: 2.7953\n",
      "Epoch: [40/50]         || Step: [350/391]       || Average Training Loss: 2.7988\n",
      "Epoch: [40/50]         || Step: [0/47]          || Average Validation Loss: 2.8308\n",
      "****************************************************************************************************\n",
      "Epoch: [40/50] || Training Loss = 2.80 || Validation Loss: 2.82 || Time: 795.501853\n",
      "****************************************************************************************************\n",
      "Epoch: [41/50]         || Step: [0/391]         || Average Training Loss: 2.8206\n",
      "Epoch: [41/50]         || Step: [50/391]        || Average Training Loss: 2.7707\n",
      "Epoch: [41/50]         || Step: [100/391]       || Average Training Loss: 2.7751\n",
      "Epoch: [41/50]         || Step: [150/391]       || Average Training Loss: 2.7800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m: encoder_,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m'\u001b[39m: decoder_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m training_loss, validation_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/utils.py:86\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, criterion, optimizer, train_loader, val_loader, total_epoch, device, checkpoint_path, print_every, load_checkpoint)\u001b[0m\n\u001b[1;32m     83\u001b[0m encoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     84\u001b[0m decoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     87\u001b[0m     idx, images, captions \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     88\u001b[0m     images, captions \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), captions\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/get_loader.py:179\u001b[0m, in \u001b[0;36mMSCOCODataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# get X: Image\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# get y: Image Caption\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_deque[idx][\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/get_loader.py:152\u001b[0m, in \u001b[0;36mMSCOCODataset.load_img\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    150\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m img_file_name)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/get_loader.py:138\u001b[0m, in \u001b[0;36mMSCOCODataset.img_transforms\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimg_transforms\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    129\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m    130\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m    131\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m     ])\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py:97\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 97\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py:351\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/functional.py:436\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    434\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    435\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39msize, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, max_size\u001b[38;5;241m=\u001b[39mmax_size, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py:264\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m     )\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py:1980\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1972\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   1973\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1974\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   1975\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   1976\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   1977\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   1978\u001b[0m         )\n\u001b[0;32m-> 1980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder_,\n",
    "    'decoder': decoder_,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f176d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
