{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09b4c42",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a3db4",
   "metadata": {},
   "source": [
    "All our training is done using 10k / 2k / 2k sized data for train / val / test.\n",
    "\n",
    "This notebook narrows images to only those with sports in them. We then randomly sample these to get 10k / 2k / 2k. This gives us a slightly smaller vocab, but same size dataset. \n",
    "\n",
    "We only train on images containing sports categories. \n",
    "\n",
    "This is an alternative version of the v2 sports training. We reduce embed size to 512 from 1024, and increase the size of the training set and vocab. Finally, because the training set is ~10% of full val2017 training set, we reduce the frequency threshold parameter for the vocabulary generation to 4. This increases vocab size from 2576 with threshold = 5 to 2921 with threshold = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606af7ca",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fccfe284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_loader import get_loader\n",
    "from models import Encoder, Decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from data_prep_utils import *\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff406e5e",
   "metadata": {},
   "source": [
    "## Load train and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd484d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 4\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'sports_v2'\n",
    "SUPER_CATEGORIES = ['sports'] # should be list of eligible coco super categories, or None to include all images\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "PRINT_EVERY = 100\n",
    "TOTAL_EPOCH = 50\n",
    "CHECKPOINT = '../model/model_sport_v3' # there is no v1 for sports:\n",
    "# v2 is consistent with previous tests as v2 parameters are shared across data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a247cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 15000 images\n",
      " val dataset has 2000 images\n",
      " test dataset has 938 images\n",
      "There are 75039 captions in the data set\n",
      "With FREQ_THRESHOLD = 4, vocab size is 2921\n"
     ]
    }
   ],
   "source": [
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=['sports'],\n",
    "                 max_train=15000, max_val=2000, max_test=2000,\n",
    "                 save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file=f'{CAPTIONS_NAME}_captions_train.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d977f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4115c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 586, Length of testing dataloader: 47\n",
      "Length of vocabulary: 2921\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_loader)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde1d50",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "593a54d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fd73a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a62141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/50]          || Step: [0/586]         || Average Training Loss: 7.9903\n",
      "Epoch: [0/50]          || Step: [100/586]       || Average Training Loss: 4.6392\n",
      "Epoch: [0/50]          || Step: [200/586]       || Average Training Loss: 4.1221\n",
      "Epoch: [0/50]          || Step: [300/586]       || Average Training Loss: 3.8262\n",
      "Epoch: [0/50]          || Step: [400/586]       || Average Training Loss: 3.6221\n",
      "Epoch: [0/50]          || Step: [500/586]       || Average Training Loss: 3.4742\n",
      "Epoch: [0/50]          || Step: [0/47]          || Average Validation Loss: 2.6269\n",
      "****************************************************************************************************\n",
      "Epoch: [0/50] || Training Loss = 3.37 || Validation Loss: 2.67 || Time: 20.881170\n",
      "****************************************************************************************************\n",
      "Epoch: [1/50]          || Step: [0/586]         || Average Training Loss: 2.9051\n",
      "Epoch: [1/50]          || Step: [100/586]       || Average Training Loss: 2.6678\n",
      "Epoch: [1/50]          || Step: [200/586]       || Average Training Loss: 2.6403\n",
      "Epoch: [1/50]          || Step: [300/586]       || Average Training Loss: 2.6135\n",
      "Epoch: [1/50]          || Step: [400/586]       || Average Training Loss: 2.5931\n",
      "Epoch: [1/50]          || Step: [500/586]       || Average Training Loss: 2.5757\n",
      "Epoch: [1/50]          || Step: [0/47]          || Average Validation Loss: 2.4030\n",
      "****************************************************************************************************\n",
      "Epoch: [1/50] || Training Loss = 2.57 || Validation Loss: 2.43 || Time: 19.635162\n",
      "****************************************************************************************************\n",
      "Epoch: [2/50]          || Step: [0/586]         || Average Training Loss: 2.5094\n",
      "Epoch: [2/50]          || Step: [100/586]       || Average Training Loss: 2.4692\n",
      "Epoch: [2/50]          || Step: [200/586]       || Average Training Loss: 2.4695\n",
      "Epoch: [2/50]          || Step: [300/586]       || Average Training Loss: 2.4674\n",
      "Epoch: [2/50]          || Step: [400/586]       || Average Training Loss: 2.4690\n",
      "Epoch: [2/50]          || Step: [500/586]       || Average Training Loss: 2.4648\n",
      "Epoch: [2/50]          || Step: [0/47]          || Average Validation Loss: 2.3438\n",
      "****************************************************************************************************\n",
      "Epoch: [2/50] || Training Loss = 2.46 || Validation Loss: 2.40 || Time: 21.720943\n",
      "****************************************************************************************************\n",
      "Epoch: [3/50]          || Step: [0/586]         || Average Training Loss: 2.4398\n",
      "Epoch: [3/50]          || Step: [100/586]       || Average Training Loss: 2.4442\n",
      "Epoch: [3/50]          || Step: [200/586]       || Average Training Loss: 2.4323\n",
      "Epoch: [3/50]          || Step: [300/586]       || Average Training Loss: 2.4348\n",
      "Epoch: [3/50]          || Step: [400/586]       || Average Training Loss: 2.4356\n",
      "Epoch: [3/50]          || Step: [500/586]       || Average Training Loss: 2.4342\n",
      "Epoch: [3/50]          || Step: [0/47]          || Average Validation Loss: 2.2241\n",
      "****************************************************************************************************\n",
      "Epoch: [3/50] || Training Loss = 2.43 || Validation Loss: 2.37 || Time: 20.924669\n",
      "****************************************************************************************************\n",
      "Epoch: [4/50]          || Step: [0/586]         || Average Training Loss: 2.4781\n",
      "Epoch: [4/50]          || Step: [100/586]       || Average Training Loss: 2.4068\n",
      "Epoch: [4/50]          || Step: [200/586]       || Average Training Loss: 2.4115\n",
      "Epoch: [4/50]          || Step: [300/586]       || Average Training Loss: 2.4101\n",
      "Epoch: [4/50]          || Step: [400/586]       || Average Training Loss: 2.4124\n",
      "Epoch: [4/50]          || Step: [500/586]       || Average Training Loss: 2.4138\n",
      "Epoch: [4/50]          || Step: [0/47]          || Average Validation Loss: 2.4344\n",
      "****************************************************************************************************\n",
      "Epoch: [4/50] || Training Loss = 2.41 || Validation Loss: 2.37 || Time: 22.216853\n",
      "****************************************************************************************************\n",
      "Epoch: [5/50]          || Step: [0/586]         || Average Training Loss: 2.3693\n",
      "Epoch: [5/50]          || Step: [100/586]       || Average Training Loss: 2.3950\n",
      "Epoch: [5/50]          || Step: [200/586]       || Average Training Loss: 2.4037\n",
      "Epoch: [5/50]          || Step: [300/586]       || Average Training Loss: 2.4037\n",
      "Epoch: [5/50]          || Step: [400/586]       || Average Training Loss: 2.4021\n",
      "Epoch: [5/50]          || Step: [500/586]       || Average Training Loss: 2.4021\n",
      "Epoch: [5/50]          || Step: [0/47]          || Average Validation Loss: 2.3062\n",
      "****************************************************************************************************\n",
      "Epoch: [5/50] || Training Loss = 2.40 || Validation Loss: 2.36 || Time: 19.449629\n",
      "****************************************************************************************************\n",
      "Epoch: [6/50]          || Step: [0/586]         || Average Training Loss: 2.3583\n",
      "Epoch: [6/50]          || Step: [100/586]       || Average Training Loss: 2.3838\n",
      "Epoch: [6/50]          || Step: [200/586]       || Average Training Loss: 2.3903\n",
      "Epoch: [6/50]          || Step: [300/586]       || Average Training Loss: 2.3944\n",
      "Epoch: [6/50]          || Step: [400/586]       || Average Training Loss: 2.3939\n",
      "Epoch: [6/50]          || Step: [500/586]       || Average Training Loss: 2.3942\n",
      "Epoch: [6/50]          || Step: [0/47]          || Average Validation Loss: 2.3443\n",
      "****************************************************************************************************\n",
      "Epoch: [6/50] || Training Loss = 2.40 || Validation Loss: 2.36 || Time: 22.016711\n",
      "****************************************************************************************************\n",
      "Epoch: [7/50]          || Step: [0/586]         || Average Training Loss: 2.3444\n",
      "Epoch: [7/50]          || Step: [100/586]       || Average Training Loss: 2.3791\n",
      "Epoch: [7/50]          || Step: [200/586]       || Average Training Loss: 2.3840\n",
      "Epoch: [7/50]          || Step: [300/586]       || Average Training Loss: 2.3910\n",
      "Epoch: [7/50]          || Step: [400/586]       || Average Training Loss: 2.3906\n",
      "Epoch: [7/50]          || Step: [500/586]       || Average Training Loss: 2.3901\n",
      "Epoch: [7/50]          || Step: [0/47]          || Average Validation Loss: 2.3183\n",
      "****************************************************************************************************\n",
      "Epoch: [7/50] || Training Loss = 2.39 || Validation Loss: 2.33 || Time: 23.715110\n",
      "****************************************************************************************************\n",
      "Epoch: [8/50]          || Step: [0/586]         || Average Training Loss: 2.3472\n",
      "Epoch: [8/50]          || Step: [100/586]       || Average Training Loss: 2.3976\n",
      "Epoch: [8/50]          || Step: [200/586]       || Average Training Loss: 2.3811\n",
      "Epoch: [8/50]          || Step: [300/586]       || Average Training Loss: 2.3847\n",
      "Epoch: [8/50]          || Step: [400/586]       || Average Training Loss: 2.3800\n",
      "Epoch: [8/50]          || Step: [500/586]       || Average Training Loss: 2.3790\n",
      "Epoch: [8/50]          || Step: [0/47]          || Average Validation Loss: 2.4095\n",
      "****************************************************************************************************\n",
      "Epoch: [8/50] || Training Loss = 2.38 || Validation Loss: 2.36 || Time: 22.515672\n",
      "****************************************************************************************************\n",
      "Epoch: [9/50]          || Step: [0/586]         || Average Training Loss: 2.3631\n",
      "Epoch: [9/50]          || Step: [100/586]       || Average Training Loss: 2.3477\n",
      "Epoch: [9/50]          || Step: [200/586]       || Average Training Loss: 2.3674\n",
      "Epoch: [9/50]          || Step: [300/586]       || Average Training Loss: 2.3677\n",
      "Epoch: [9/50]          || Step: [400/586]       || Average Training Loss: 2.3701\n",
      "Epoch: [9/50]          || Step: [500/586]       || Average Training Loss: 2.3717\n",
      "Epoch: [9/50]          || Step: [0/47]          || Average Validation Loss: 2.2803\n",
      "****************************************************************************************************\n",
      "Epoch: [9/50] || Training Loss = 2.37 || Validation Loss: 2.35 || Time: 23.436228\n",
      "****************************************************************************************************\n",
      "Epoch: [10/50]         || Step: [0/586]         || Average Training Loss: 2.3696\n",
      "Epoch: [10/50]         || Step: [100/586]       || Average Training Loss: 2.3650\n",
      "Epoch: [10/50]         || Step: [200/586]       || Average Training Loss: 2.3666\n",
      "Epoch: [10/50]         || Step: [300/586]       || Average Training Loss: 2.3643\n",
      "Epoch: [10/50]         || Step: [400/586]       || Average Training Loss: 2.3658\n",
      "Epoch: [10/50]         || Step: [500/586]       || Average Training Loss: 2.3674\n",
      "Epoch: [10/50]         || Step: [0/47]          || Average Validation Loss: 2.4240\n",
      "****************************************************************************************************\n",
      "Epoch: [10/50] || Training Loss = 2.37 || Validation Loss: 2.33 || Time: 21.564619\n",
      "****************************************************************************************************\n",
      "Epoch: [11/50]         || Step: [0/586]         || Average Training Loss: 2.3104\n",
      "Epoch: [11/50]         || Step: [100/586]       || Average Training Loss: 2.3510\n",
      "Epoch: [11/50]         || Step: [200/586]       || Average Training Loss: 2.3522\n",
      "Epoch: [11/50]         || Step: [300/586]       || Average Training Loss: 2.3561\n",
      "Epoch: [11/50]         || Step: [400/586]       || Average Training Loss: 2.3585\n",
      "Epoch: [11/50]         || Step: [500/586]       || Average Training Loss: 2.3587\n",
      "Epoch: [11/50]         || Step: [0/47]          || Average Validation Loss: 2.3282\n",
      "****************************************************************************************************\n",
      "Epoch: [11/50] || Training Loss = 2.36 || Validation Loss: 2.32 || Time: 24.111844\n",
      "****************************************************************************************************\n",
      "Epoch: [12/50]         || Step: [0/586]         || Average Training Loss: 2.4609\n",
      "Epoch: [12/50]         || Step: [100/586]       || Average Training Loss: 2.3376\n",
      "Epoch: [12/50]         || Step: [200/586]       || Average Training Loss: 2.3495\n",
      "Epoch: [12/50]         || Step: [300/586]       || Average Training Loss: 2.3495\n",
      "Epoch: [12/50]         || Step: [400/586]       || Average Training Loss: 2.3525\n",
      "Epoch: [12/50]         || Step: [500/586]       || Average Training Loss: 2.3571\n",
      "Epoch: [12/50]         || Step: [0/47]          || Average Validation Loss: 2.1895\n",
      "****************************************************************************************************\n",
      "Epoch: [12/50] || Training Loss = 2.36 || Validation Loss: 2.30 || Time: 19.817594\n",
      "****************************************************************************************************\n",
      "Epoch: [13/50]         || Step: [0/586]         || Average Training Loss: 2.3793\n",
      "Epoch: [13/50]         || Step: [100/586]       || Average Training Loss: 2.3428\n",
      "Epoch: [13/50]         || Step: [200/586]       || Average Training Loss: 2.3485\n",
      "Epoch: [13/50]         || Step: [300/586]       || Average Training Loss: 2.3494\n",
      "Epoch: [13/50]         || Step: [400/586]       || Average Training Loss: 2.3501\n",
      "Epoch: [13/50]         || Step: [500/586]       || Average Training Loss: 2.3498\n",
      "Epoch: [13/50]         || Step: [0/47]          || Average Validation Loss: 2.3080\n",
      "****************************************************************************************************\n",
      "Epoch: [13/50] || Training Loss = 2.35 || Validation Loss: 2.31 || Time: 22.438483\n",
      "****************************************************************************************************\n",
      "Epoch: [14/50]         || Step: [0/586]         || Average Training Loss: 2.4213\n",
      "Epoch: [14/50]         || Step: [100/586]       || Average Training Loss: 2.3335\n",
      "Epoch: [14/50]         || Step: [200/586]       || Average Training Loss: 2.3405\n",
      "Epoch: [14/50]         || Step: [300/586]       || Average Training Loss: 2.3467\n",
      "Epoch: [14/50]         || Step: [400/586]       || Average Training Loss: 2.3453\n",
      "Epoch: [14/50]         || Step: [500/586]       || Average Training Loss: 2.3498\n",
      "Epoch: [14/50]         || Step: [0/47]          || Average Validation Loss: 2.3951\n",
      "****************************************************************************************************\n",
      "Epoch: [14/50] || Training Loss = 2.35 || Validation Loss: 2.31 || Time: 20.857909\n",
      "****************************************************************************************************\n",
      "Epoch: [15/50]         || Step: [0/586]         || Average Training Loss: 2.3565\n",
      "Epoch: [15/50]         || Step: [100/586]       || Average Training Loss: 2.3389\n",
      "Epoch: [15/50]         || Step: [200/586]       || Average Training Loss: 2.3344\n",
      "Epoch: [15/50]         || Step: [300/586]       || Average Training Loss: 2.3365\n",
      "Epoch: [15/50]         || Step: [400/586]       || Average Training Loss: 2.3422\n",
      "Epoch: [15/50]         || Step: [500/586]       || Average Training Loss: 2.3447\n",
      "Epoch: [15/50]         || Step: [0/47]          || Average Validation Loss: 2.3889\n",
      "****************************************************************************************************\n",
      "Epoch: [15/50] || Training Loss = 2.35 || Validation Loss: 2.30 || Time: 19.885458\n",
      "****************************************************************************************************\n",
      "Epoch: [16/50]         || Step: [0/586]         || Average Training Loss: 2.2586\n",
      "Epoch: [16/50]         || Step: [100/586]       || Average Training Loss: 2.3359\n",
      "Epoch: [16/50]         || Step: [200/586]       || Average Training Loss: 2.3419\n",
      "Epoch: [16/50]         || Step: [300/586]       || Average Training Loss: 2.3471\n",
      "Epoch: [16/50]         || Step: [400/586]       || Average Training Loss: 2.3422\n",
      "Epoch: [16/50]         || Step: [500/586]       || Average Training Loss: 2.3436\n",
      "Epoch: [16/50]         || Step: [0/47]          || Average Validation Loss: 2.0882\n",
      "****************************************************************************************************\n",
      "Epoch: [16/50] || Training Loss = 2.35 || Validation Loss: 2.31 || Time: 23.550322\n",
      "****************************************************************************************************\n",
      "Epoch: [17/50]         || Step: [0/586]         || Average Training Loss: 2.3431\n",
      "Epoch: [17/50]         || Step: [100/586]       || Average Training Loss: 2.3426\n",
      "Epoch: [17/50]         || Step: [200/586]       || Average Training Loss: 2.3401\n",
      "Epoch: [17/50]         || Step: [300/586]       || Average Training Loss: 2.3406\n",
      "Epoch: [17/50]         || Step: [400/586]       || Average Training Loss: 2.3396\n",
      "Epoch: [17/50]         || Step: [500/586]       || Average Training Loss: 2.3412\n",
      "Epoch: [17/50]         || Step: [0/47]          || Average Validation Loss: 2.3952\n",
      "****************************************************************************************************\n",
      "Epoch: [17/50] || Training Loss = 2.34 || Validation Loss: 2.32 || Time: 23.931148\n",
      "****************************************************************************************************\n",
      "Epoch: [18/50]         || Step: [0/586]         || Average Training Loss: 2.2710\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803aae9-81ae-4390-96ee-d8e5ab8a703b",
   "metadata": {},
   "source": [
    "## Modify the parameters\n",
    "\n",
    "Use entire train2017 data set to build the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63490149-618c-482d-9686-063d12b5c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'sports_v4'\n",
    "SUPER_CATEGORIES = ['sports'] # should be list of eligible coco super categories, or None to include all images\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "PRINT_EVERY = 100\n",
    "TOTAL_EPOCH = 20\n",
    "CHECKPOINT = '../model/model_sport_v4' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708d529b-3ed3-4f83-b65c-815fbf126ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 15000 images\n",
      " val dataset has 2000 images\n",
      " test dataset has 938 images\n",
      "There are 591753 captions in the data set\n",
      "With FREQ_THRESHOLD = 5, vocab size is 10192\n"
     ]
    }
   ],
   "source": [
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=['sports'],\n",
    "                 max_train=15000, max_val=2000, max_test=2000,\n",
    "                 save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file='captions_train2017.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a4df44-9b0e-49e7-8c65-5226a0090b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33408d78-aa23-4ea3-a2e8-2d507bdaacf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 1172, Length of testing dataloader: 94\n",
      "Length of vocabulary: 10192\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_loader)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69841c94-4d7e-4c3a-804c-6ded7f0b37bd",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7113927c-e216-4775-bac8-37543e3f3b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac8a062-06fc-4470-8750-6f1d9554201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e8144e-34e5-4dda-84ec-7b04efb86dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "311c4c64-8751-4d2c-860a-bee388898149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ed405-1249-494d-ad46-22a7f7d72915",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35a05ed4-3a21-4206-a714-52819c83d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]          || Step: [0/1172]        || Average Training Loss: 9.2194\n",
      "Epoch: [0/20]          || Step: [100/1172]      || Average Training Loss: 4.9198\n",
      "Epoch: [0/20]          || Step: [200/1172]      || Average Training Loss: 4.4146\n",
      "Epoch: [0/20]          || Step: [300/1172]      || Average Training Loss: 4.1013\n",
      "Epoch: [0/20]          || Step: [400/1172]      || Average Training Loss: 3.8984\n",
      "Epoch: [0/20]          || Step: [500/1172]      || Average Training Loss: 3.7452\n",
      "Epoch: [0/20]          || Step: [600/1172]      || Average Training Loss: 3.6234\n",
      "Epoch: [0/20]          || Step: [700/1172]      || Average Training Loss: 3.5237\n",
      "Epoch: [0/20]          || Step: [800/1172]      || Average Training Loss: 3.4392\n",
      "Epoch: [0/20]          || Step: [900/1172]      || Average Training Loss: 3.3676\n",
      "Epoch: [0/20]          || Step: [1000/1172]     || Average Training Loss: 3.3101\n",
      "Epoch: [0/20]          || Step: [1100/1172]     || Average Training Loss: 3.2595\n",
      "Epoch: [0/20]          || Step: [0/94]          || Average Validation Loss: 2.7762\n",
      "****************************************************************************************************\n",
      "Epoch: [0/20] || Training Loss = 3.23 || Validation Loss: 2.67 || Time: 20.854592\n",
      "****************************************************************************************************\n",
      "Epoch: [1/20]          || Step: [0/1172]        || Average Training Loss: 2.8567\n",
      "Epoch: [1/20]          || Step: [100/1172]      || Average Training Loss: 2.7165\n",
      "Epoch: [1/20]          || Step: [200/1172]      || Average Training Loss: 2.6961\n",
      "Epoch: [1/20]          || Step: [300/1172]      || Average Training Loss: 2.6785\n",
      "Epoch: [1/20]          || Step: [400/1172]      || Average Training Loss: 2.6674\n",
      "Epoch: [1/20]          || Step: [500/1172]      || Average Training Loss: 2.6603\n",
      "Epoch: [1/20]          || Step: [600/1172]      || Average Training Loss: 2.6557\n",
      "Epoch: [1/20]          || Step: [700/1172]      || Average Training Loss: 2.6559\n",
      "Epoch: [1/20]          || Step: [800/1172]      || Average Training Loss: 2.6507\n",
      "Epoch: [1/20]          || Step: [900/1172]      || Average Training Loss: 2.6456\n",
      "Epoch: [1/20]          || Step: [1000/1172]     || Average Training Loss: 2.6430\n",
      "Epoch: [1/20]          || Step: [1100/1172]     || Average Training Loss: 2.6375\n",
      "Epoch: [1/20]          || Step: [0/94]          || Average Validation Loss: 2.3993\n",
      "****************************************************************************************************\n",
      "Epoch: [1/20] || Training Loss = 2.64 || Validation Loss: 2.56 || Time: 22.516500\n",
      "****************************************************************************************************\n",
      "Epoch: [2/20]          || Step: [0/1172]        || Average Training Loss: 2.5677\n",
      "Epoch: [2/20]          || Step: [100/1172]      || Average Training Loss: 2.5831\n",
      "Epoch: [2/20]          || Step: [200/1172]      || Average Training Loss: 2.5755\n",
      "Epoch: [2/20]          || Step: [300/1172]      || Average Training Loss: 2.5776\n",
      "Epoch: [2/20]          || Step: [400/1172]      || Average Training Loss: 2.5753\n",
      "Epoch: [2/20]          || Step: [500/1172]      || Average Training Loss: 2.5710\n",
      "Epoch: [2/20]          || Step: [600/1172]      || Average Training Loss: 2.5707\n",
      "Epoch: [2/20]          || Step: [700/1172]      || Average Training Loss: 2.5672\n",
      "Epoch: [2/20]          || Step: [800/1172]      || Average Training Loss: 2.5668\n",
      "Epoch: [2/20]          || Step: [900/1172]      || Average Training Loss: 2.5667\n",
      "Epoch: [2/20]          || Step: [1000/1172]     || Average Training Loss: 2.5678\n",
      "Epoch: [2/20]          || Step: [1100/1172]     || Average Training Loss: 2.5687\n",
      "Epoch: [2/20]          || Step: [0/94]          || Average Validation Loss: 2.7686\n",
      "****************************************************************************************************\n",
      "Epoch: [2/20] || Training Loss = 2.57 || Validation Loss: 2.51 || Time: 23.222543\n",
      "****************************************************************************************************\n",
      "Epoch: [3/20]          || Step: [0/1172]        || Average Training Loss: 2.4673\n",
      "Epoch: [3/20]          || Step: [100/1172]      || Average Training Loss: 2.5348\n",
      "Epoch: [3/20]          || Step: [200/1172]      || Average Training Loss: 2.5363\n",
      "Epoch: [3/20]          || Step: [300/1172]      || Average Training Loss: 2.5356\n",
      "Epoch: [3/20]          || Step: [400/1172]      || Average Training Loss: 2.5374\n",
      "Epoch: [3/20]          || Step: [500/1172]      || Average Training Loss: 2.5386\n",
      "Epoch: [3/20]          || Step: [600/1172]      || Average Training Loss: 2.5436\n",
      "Epoch: [3/20]          || Step: [700/1172]      || Average Training Loss: 2.5384\n",
      "Epoch: [3/20]          || Step: [800/1172]      || Average Training Loss: 2.5371\n",
      "Epoch: [3/20]          || Step: [900/1172]      || Average Training Loss: 2.5357\n",
      "Epoch: [3/20]          || Step: [1000/1172]     || Average Training Loss: 2.5371\n",
      "Epoch: [3/20]          || Step: [1100/1172]     || Average Training Loss: 2.5372\n",
      "Epoch: [3/20]          || Step: [0/94]          || Average Validation Loss: 2.5948\n",
      "****************************************************************************************************\n",
      "Epoch: [3/20] || Training Loss = 2.54 || Validation Loss: 2.47 || Time: 23.170069\n",
      "****************************************************************************************************\n",
      "Epoch: [4/20]          || Step: [0/1172]        || Average Training Loss: 2.5414\n",
      "Epoch: [4/20]          || Step: [100/1172]      || Average Training Loss: 2.4871\n",
      "Epoch: [4/20]          || Step: [200/1172]      || Average Training Loss: 2.4979\n",
      "Epoch: [4/20]          || Step: [300/1172]      || Average Training Loss: 2.5119\n",
      "Epoch: [4/20]          || Step: [400/1172]      || Average Training Loss: 2.5138\n",
      "Epoch: [4/20]          || Step: [500/1172]      || Average Training Loss: 2.5093\n",
      "Epoch: [4/20]          || Step: [600/1172]      || Average Training Loss: 2.5074\n",
      "Epoch: [4/20]          || Step: [700/1172]      || Average Training Loss: 2.5133\n",
      "Epoch: [4/20]          || Step: [800/1172]      || Average Training Loss: 2.5135\n",
      "Epoch: [4/20]          || Step: [900/1172]      || Average Training Loss: 2.5127\n",
      "Epoch: [4/20]          || Step: [1000/1172]     || Average Training Loss: 2.5155\n",
      "Epoch: [4/20]          || Step: [1100/1172]     || Average Training Loss: 2.5182\n",
      "Epoch: [4/20]          || Step: [0/94]          || Average Validation Loss: 2.5913\n",
      "****************************************************************************************************\n",
      "Epoch: [4/20] || Training Loss = 2.52 || Validation Loss: 2.49 || Time: 23.322726\n",
      "****************************************************************************************************\n",
      "Epoch: [5/20]          || Step: [0/1172]        || Average Training Loss: 2.8880\n",
      "Epoch: [5/20]          || Step: [100/1172]      || Average Training Loss: 2.5031\n",
      "Epoch: [5/20]          || Step: [200/1172]      || Average Training Loss: 2.5013\n",
      "Epoch: [5/20]          || Step: [300/1172]      || Average Training Loss: 2.5018\n",
      "Epoch: [5/20]          || Step: [400/1172]      || Average Training Loss: 2.5045\n",
      "Epoch: [5/20]          || Step: [500/1172]      || Average Training Loss: 2.5006\n",
      "Epoch: [5/20]          || Step: [600/1172]      || Average Training Loss: 2.4988\n",
      "Epoch: [5/20]          || Step: [700/1172]      || Average Training Loss: 2.5003\n",
      "Epoch: [5/20]          || Step: [800/1172]      || Average Training Loss: 2.5010\n",
      "Epoch: [5/20]          || Step: [900/1172]      || Average Training Loss: 2.5002\n",
      "Epoch: [5/20]          || Step: [1000/1172]     || Average Training Loss: 2.5025\n",
      "Epoch: [5/20]          || Step: [1100/1172]     || Average Training Loss: 2.5035\n",
      "Epoch: [5/20]          || Step: [0/94]          || Average Validation Loss: 2.3051\n",
      "****************************************************************************************************\n",
      "Epoch: [5/20] || Training Loss = 2.50 || Validation Loss: 2.45 || Time: 23.165601\n",
      "****************************************************************************************************\n",
      "Epoch: [6/20]          || Step: [0/1172]        || Average Training Loss: 2.4498\n",
      "Epoch: [6/20]          || Step: [100/1172]      || Average Training Loss: 2.4634\n",
      "Epoch: [6/20]          || Step: [200/1172]      || Average Training Loss: 2.4785\n",
      "Epoch: [6/20]          || Step: [300/1172]      || Average Training Loss: 2.4927\n",
      "Epoch: [6/20]          || Step: [400/1172]      || Average Training Loss: 2.4949\n",
      "Epoch: [6/20]          || Step: [500/1172]      || Average Training Loss: 2.4933\n",
      "Epoch: [6/20]          || Step: [600/1172]      || Average Training Loss: 2.4938\n",
      "Epoch: [6/20]          || Step: [700/1172]      || Average Training Loss: 2.4923\n",
      "Epoch: [6/20]          || Step: [800/1172]      || Average Training Loss: 2.4898\n",
      "Epoch: [6/20]          || Step: [900/1172]      || Average Training Loss: 2.4894\n",
      "Epoch: [6/20]          || Step: [1000/1172]     || Average Training Loss: 2.4908\n",
      "Epoch: [6/20]          || Step: [1100/1172]     || Average Training Loss: 2.4922\n",
      "Epoch: [6/20]          || Step: [0/94]          || Average Validation Loss: 2.4359\n",
      "****************************************************************************************************\n",
      "Epoch: [6/20] || Training Loss = 2.49 || Validation Loss: 2.45 || Time: 22.835479\n",
      "****************************************************************************************************\n",
      "Epoch: [7/20]          || Step: [0/1172]        || Average Training Loss: 2.4275\n",
      "Epoch: [7/20]          || Step: [100/1172]      || Average Training Loss: 2.4925\n",
      "Epoch: [7/20]          || Step: [200/1172]      || Average Training Loss: 2.4763\n",
      "Epoch: [7/20]          || Step: [300/1172]      || Average Training Loss: 2.4796\n",
      "Epoch: [7/20]          || Step: [400/1172]      || Average Training Loss: 2.4805\n",
      "Epoch: [7/20]          || Step: [500/1172]      || Average Training Loss: 2.4831\n",
      "Epoch: [7/20]          || Step: [600/1172]      || Average Training Loss: 2.4788\n",
      "Epoch: [7/20]          || Step: [700/1172]      || Average Training Loss: 2.4801\n",
      "Epoch: [7/20]          || Step: [800/1172]      || Average Training Loss: 2.4822\n",
      "Epoch: [7/20]          || Step: [900/1172]      || Average Training Loss: 2.4835\n",
      "Epoch: [7/20]          || Step: [1000/1172]     || Average Training Loss: 2.4834\n",
      "Epoch: [7/20]          || Step: [1100/1172]     || Average Training Loss: 2.4831\n",
      "Epoch: [7/20]          || Step: [0/94]          || Average Validation Loss: 2.3861\n",
      "****************************************************************************************************\n",
      "Epoch: [7/20] || Training Loss = 2.48 || Validation Loss: 2.45 || Time: 22.379947\n",
      "****************************************************************************************************\n",
      "Epoch: [8/20]          || Step: [0/1172]        || Average Training Loss: 2.4622\n",
      "Epoch: [8/20]          || Step: [100/1172]      || Average Training Loss: 2.4787\n",
      "Epoch: [8/20]          || Step: [200/1172]      || Average Training Loss: 2.4607\n",
      "Epoch: [8/20]          || Step: [300/1172]      || Average Training Loss: 2.4640\n",
      "Epoch: [8/20]          || Step: [400/1172]      || Average Training Loss: 2.4700\n",
      "Epoch: [8/20]          || Step: [500/1172]      || Average Training Loss: 2.4704\n",
      "Epoch: [8/20]          || Step: [600/1172]      || Average Training Loss: 2.4689\n",
      "Epoch: [8/20]          || Step: [700/1172]      || Average Training Loss: 2.4710\n",
      "Epoch: [8/20]          || Step: [800/1172]      || Average Training Loss: 2.4740\n",
      "Epoch: [8/20]          || Step: [900/1172]      || Average Training Loss: 2.4725\n",
      "Epoch: [8/20]          || Step: [1000/1172]     || Average Training Loss: 2.4747\n",
      "Epoch: [8/20]          || Step: [1100/1172]     || Average Training Loss: 2.4765\n",
      "Epoch: [8/20]          || Step: [0/94]          || Average Validation Loss: 2.3775\n",
      "****************************************************************************************************\n",
      "Epoch: [8/20] || Training Loss = 2.48 || Validation Loss: 2.44 || Time: 23.219478\n",
      "****************************************************************************************************\n",
      "Epoch: [9/20]          || Step: [0/1172]        || Average Training Loss: 2.5795\n",
      "Epoch: [9/20]          || Step: [100/1172]      || Average Training Loss: 2.4722\n",
      "Epoch: [9/20]          || Step: [200/1172]      || Average Training Loss: 2.4661\n",
      "Epoch: [9/20]          || Step: [300/1172]      || Average Training Loss: 2.4649\n",
      "Epoch: [9/20]          || Step: [400/1172]      || Average Training Loss: 2.4665\n",
      "Epoch: [9/20]          || Step: [500/1172]      || Average Training Loss: 2.4739\n",
      "Epoch: [9/20]          || Step: [600/1172]      || Average Training Loss: 2.4754\n",
      "Epoch: [9/20]          || Step: [700/1172]      || Average Training Loss: 2.4720\n",
      "Epoch: [9/20]          || Step: [800/1172]      || Average Training Loss: 2.4725\n",
      "Epoch: [9/20]          || Step: [900/1172]      || Average Training Loss: 2.4741\n",
      "Epoch: [9/20]          || Step: [1000/1172]     || Average Training Loss: 2.4740\n",
      "Epoch: [9/20]          || Step: [1100/1172]     || Average Training Loss: 2.4728\n",
      "Epoch: [9/20]          || Step: [0/94]          || Average Validation Loss: 2.3796\n",
      "****************************************************************************************************\n",
      "Epoch: [9/20] || Training Loss = 2.47 || Validation Loss: 2.44 || Time: 23.110099\n",
      "****************************************************************************************************\n",
      "Epoch: [10/20]         || Step: [0/1172]        || Average Training Loss: 2.6726\n",
      "Epoch: [10/20]         || Step: [100/1172]      || Average Training Loss: 2.4745\n",
      "Epoch: [10/20]         || Step: [200/1172]      || Average Training Loss: 2.4692\n",
      "Epoch: [10/20]         || Step: [300/1172]      || Average Training Loss: 2.4758\n",
      "Epoch: [10/20]         || Step: [400/1172]      || Average Training Loss: 2.4716\n",
      "Epoch: [10/20]         || Step: [500/1172]      || Average Training Loss: 2.4698\n",
      "Epoch: [10/20]         || Step: [600/1172]      || Average Training Loss: 2.4700\n",
      "Epoch: [10/20]         || Step: [700/1172]      || Average Training Loss: 2.4679\n",
      "Epoch: [10/20]         || Step: [800/1172]      || Average Training Loss: 2.4667\n",
      "Epoch: [10/20]         || Step: [900/1172]      || Average Training Loss: 2.4674\n",
      "Epoch: [10/20]         || Step: [1000/1172]     || Average Training Loss: 2.4674\n",
      "Epoch: [10/20]         || Step: [1100/1172]     || Average Training Loss: 2.4671\n",
      "Epoch: [10/20]         || Step: [0/94]          || Average Validation Loss: 2.5596\n",
      "****************************************************************************************************\n",
      "Epoch: [10/20] || Training Loss = 2.47 || Validation Loss: 2.43 || Time: 23.393522\n",
      "****************************************************************************************************\n",
      "Epoch: [11/20]         || Step: [0/1172]        || Average Training Loss: 2.5304\n",
      "Epoch: [11/20]         || Step: [100/1172]      || Average Training Loss: 2.4592\n",
      "Epoch: [11/20]         || Step: [200/1172]      || Average Training Loss: 2.4636\n",
      "Epoch: [11/20]         || Step: [300/1172]      || Average Training Loss: 2.4620\n",
      "Epoch: [11/20]         || Step: [400/1172]      || Average Training Loss: 2.4629\n",
      "Epoch: [11/20]         || Step: [500/1172]      || Average Training Loss: 2.4648\n",
      "Epoch: [11/20]         || Step: [600/1172]      || Average Training Loss: 2.4611\n",
      "Epoch: [11/20]         || Step: [700/1172]      || Average Training Loss: 2.4641\n",
      "Epoch: [11/20]         || Step: [800/1172]      || Average Training Loss: 2.4624\n",
      "Epoch: [11/20]         || Step: [900/1172]      || Average Training Loss: 2.4630\n",
      "Epoch: [11/20]         || Step: [1000/1172]     || Average Training Loss: 2.4658\n",
      "Epoch: [11/20]         || Step: [1100/1172]     || Average Training Loss: 2.4663\n",
      "Epoch: [11/20]         || Step: [0/94]          || Average Validation Loss: 2.2494\n",
      "****************************************************************************************************\n",
      "Epoch: [11/20] || Training Loss = 2.47 || Validation Loss: 2.42 || Time: 23.221473\n",
      "****************************************************************************************************\n",
      "Epoch: [12/20]         || Step: [0/1172]        || Average Training Loss: 2.3920\n",
      "Epoch: [12/20]         || Step: [100/1172]      || Average Training Loss: 2.4312\n",
      "Epoch: [12/20]         || Step: [200/1172]      || Average Training Loss: 2.4505\n",
      "Epoch: [12/20]         || Step: [300/1172]      || Average Training Loss: 2.4606\n",
      "Epoch: [12/20]         || Step: [400/1172]      || Average Training Loss: 2.4602\n",
      "Epoch: [12/20]         || Step: [500/1172]      || Average Training Loss: 2.4551\n",
      "Epoch: [12/20]         || Step: [600/1172]      || Average Training Loss: 2.4551\n",
      "Epoch: [12/20]         || Step: [700/1172]      || Average Training Loss: 2.4549\n",
      "Epoch: [12/20]         || Step: [800/1172]      || Average Training Loss: 2.4577\n",
      "Epoch: [12/20]         || Step: [900/1172]      || Average Training Loss: 2.4601\n",
      "Epoch: [12/20]         || Step: [1000/1172]     || Average Training Loss: 2.4597\n",
      "Epoch: [12/20]         || Step: [1100/1172]     || Average Training Loss: 2.4627\n",
      "Epoch: [12/20]         || Step: [0/94]          || Average Validation Loss: 2.7505\n",
      "****************************************************************************************************\n",
      "Epoch: [12/20] || Training Loss = 2.46 || Validation Loss: 2.46 || Time: 23.205209\n",
      "****************************************************************************************************\n",
      "Epoch: [13/20]         || Step: [0/1172]        || Average Training Loss: 2.5739\n",
      "Epoch: [13/20]         || Step: [100/1172]      || Average Training Loss: 2.4687\n",
      "Epoch: [13/20]         || Step: [200/1172]      || Average Training Loss: 2.4706\n",
      "Epoch: [13/20]         || Step: [300/1172]      || Average Training Loss: 2.4625\n",
      "Epoch: [13/20]         || Step: [400/1172]      || Average Training Loss: 2.4609\n",
      "Epoch: [13/20]         || Step: [500/1172]      || Average Training Loss: 2.4600\n",
      "Epoch: [13/20]         || Step: [600/1172]      || Average Training Loss: 2.4589\n",
      "Epoch: [13/20]         || Step: [700/1172]      || Average Training Loss: 2.4602\n",
      "Epoch: [13/20]         || Step: [800/1172]      || Average Training Loss: 2.4591\n",
      "Epoch: [13/20]         || Step: [900/1172]      || Average Training Loss: 2.4593\n",
      "Epoch: [13/20]         || Step: [1000/1172]     || Average Training Loss: 2.4586\n",
      "Epoch: [13/20]         || Step: [1100/1172]     || Average Training Loss: 2.4595\n",
      "Epoch: [13/20]         || Step: [0/94]          || Average Validation Loss: 2.4290\n",
      "****************************************************************************************************\n",
      "Epoch: [13/20] || Training Loss = 2.46 || Validation Loss: 2.45 || Time: 23.070593\n",
      "****************************************************************************************************\n",
      "Epoch: [14/20]         || Step: [0/1172]        || Average Training Loss: 2.4774\n",
      "Epoch: [14/20]         || Step: [100/1172]      || Average Training Loss: 2.4684\n",
      "Epoch: [14/20]         || Step: [200/1172]      || Average Training Loss: 2.4652\n",
      "Epoch: [14/20]         || Step: [300/1172]      || Average Training Loss: 2.4563\n",
      "Epoch: [14/20]         || Step: [400/1172]      || Average Training Loss: 2.4597\n",
      "Epoch: [14/20]         || Step: [500/1172]      || Average Training Loss: 2.4601\n",
      "Epoch: [14/20]         || Step: [600/1172]      || Average Training Loss: 2.4604\n",
      "Epoch: [14/20]         || Step: [700/1172]      || Average Training Loss: 2.4592\n",
      "Epoch: [14/20]         || Step: [800/1172]      || Average Training Loss: 2.4570\n",
      "Epoch: [14/20]         || Step: [900/1172]      || Average Training Loss: 2.4598\n",
      "Epoch: [14/20]         || Step: [1000/1172]     || Average Training Loss: 2.4567\n",
      "Epoch: [14/20]         || Step: [1100/1172]     || Average Training Loss: 2.4577\n",
      "Epoch: [14/20]         || Step: [0/94]          || Average Validation Loss: 2.3111\n",
      "****************************************************************************************************\n",
      "Epoch: [14/20] || Training Loss = 2.46 || Validation Loss: 2.43 || Time: 23.480174\n",
      "****************************************************************************************************\n",
      "Epoch: [15/20]         || Step: [0/1172]        || Average Training Loss: 2.2121\n",
      "Epoch: [15/20]         || Step: [100/1172]      || Average Training Loss: 2.5278\n",
      "Epoch: [15/20]         || Step: [200/1172]      || Average Training Loss: 2.4980\n",
      "Epoch: [15/20]         || Step: [300/1172]      || Average Training Loss: 2.4805\n",
      "Epoch: [15/20]         || Step: [400/1172]      || Average Training Loss: 2.4761\n",
      "Epoch: [15/20]         || Step: [500/1172]      || Average Training Loss: 2.4735\n",
      "Epoch: [15/20]         || Step: [600/1172]      || Average Training Loss: 2.4732\n",
      "Epoch: [15/20]         || Step: [700/1172]      || Average Training Loss: 2.4730\n",
      "Epoch: [15/20]         || Step: [800/1172]      || Average Training Loss: 2.4709\n",
      "Epoch: [15/20]         || Step: [900/1172]      || Average Training Loss: 2.4740\n",
      "Epoch: [15/20]         || Step: [1000/1172]     || Average Training Loss: 2.4723\n",
      "Epoch: [15/20]         || Step: [1100/1172]     || Average Training Loss: 2.4707\n",
      "Epoch: [15/20]         || Step: [0/94]          || Average Validation Loss: 2.3083\n",
      "****************************************************************************************************\n",
      "Epoch: [15/20] || Training Loss = 2.47 || Validation Loss: 2.43 || Time: 23.200717\n",
      "****************************************************************************************************\n",
      "Epoch: [16/20]         || Step: [0/1172]        || Average Training Loss: 2.4225\n",
      "Epoch: [16/20]         || Step: [100/1172]      || Average Training Loss: 2.4391\n",
      "Epoch: [16/20]         || Step: [200/1172]      || Average Training Loss: 2.4503\n",
      "Epoch: [16/20]         || Step: [300/1172]      || Average Training Loss: 2.4489\n",
      "Epoch: [16/20]         || Step: [400/1172]      || Average Training Loss: 2.4476\n",
      "Epoch: [16/20]         || Step: [500/1172]      || Average Training Loss: 2.4537\n",
      "Epoch: [16/20]         || Step: [600/1172]      || Average Training Loss: 2.4563\n",
      "Epoch: [16/20]         || Step: [700/1172]      || Average Training Loss: 2.4592\n",
      "Epoch: [16/20]         || Step: [800/1172]      || Average Training Loss: 2.4571\n",
      "Epoch: [16/20]         || Step: [900/1172]      || Average Training Loss: 2.4562\n",
      "Epoch: [16/20]         || Step: [1000/1172]     || Average Training Loss: 2.4561\n",
      "Epoch: [16/20]         || Step: [1100/1172]     || Average Training Loss: 2.4575\n",
      "Epoch: [16/20]         || Step: [0/94]          || Average Validation Loss: 2.6180\n",
      "****************************************************************************************************\n",
      "Epoch: [16/20] || Training Loss = 2.46 || Validation Loss: 2.42 || Time: 23.103212\n",
      "****************************************************************************************************\n",
      "Epoch: [17/20]         || Step: [0/1172]        || Average Training Loss: 2.3992\n",
      "Epoch: [17/20]         || Step: [100/1172]      || Average Training Loss: 2.4698\n",
      "Epoch: [17/20]         || Step: [200/1172]      || Average Training Loss: 2.4619\n",
      "Epoch: [17/20]         || Step: [300/1172]      || Average Training Loss: 2.4633\n",
      "Epoch: [17/20]         || Step: [400/1172]      || Average Training Loss: 2.4562\n",
      "Epoch: [17/20]         || Step: [500/1172]      || Average Training Loss: 2.4557\n",
      "Epoch: [17/20]         || Step: [600/1172]      || Average Training Loss: 2.4568\n",
      "Epoch: [17/20]         || Step: [700/1172]      || Average Training Loss: 2.4549\n",
      "Epoch: [17/20]         || Step: [800/1172]      || Average Training Loss: 2.4540\n",
      "Epoch: [17/20]         || Step: [900/1172]      || Average Training Loss: 2.4553\n",
      "Epoch: [17/20]         || Step: [1000/1172]     || Average Training Loss: 2.4540\n",
      "Epoch: [17/20]         || Step: [1100/1172]     || Average Training Loss: 2.4551\n",
      "Epoch: [17/20]         || Step: [0/94]          || Average Validation Loss: 2.3671\n",
      "****************************************************************************************************\n",
      "Epoch: [17/20] || Training Loss = 2.46 || Validation Loss: 2.42 || Time: 23.173589\n",
      "****************************************************************************************************\n",
      "Epoch: [18/20]         || Step: [0/1172]        || Average Training Loss: 2.3651\n",
      "Epoch: [18/20]         || Step: [100/1172]      || Average Training Loss: 2.4599\n",
      "Epoch: [18/20]         || Step: [200/1172]      || Average Training Loss: 2.4533\n",
      "Epoch: [18/20]         || Step: [300/1172]      || Average Training Loss: 2.4500\n",
      "Epoch: [18/20]         || Step: [400/1172]      || Average Training Loss: 2.4511\n",
      "Epoch: [18/20]         || Step: [500/1172]      || Average Training Loss: 2.4535\n",
      "Epoch: [18/20]         || Step: [600/1172]      || Average Training Loss: 2.4528\n",
      "Epoch: [18/20]         || Step: [700/1172]      || Average Training Loss: 2.4507\n",
      "Epoch: [18/20]         || Step: [800/1172]      || Average Training Loss: 2.4492\n",
      "Epoch: [18/20]         || Step: [900/1172]      || Average Training Loss: 2.4515\n",
      "Epoch: [18/20]         || Step: [1000/1172]     || Average Training Loss: 2.4529\n",
      "Epoch: [18/20]         || Step: [1100/1172]     || Average Training Loss: 2.4533\n",
      "Epoch: [18/20]         || Step: [0/94]          || Average Validation Loss: 2.3801\n",
      "****************************************************************************************************\n",
      "Epoch: [18/20] || Training Loss = 2.45 || Validation Loss: 2.44 || Time: 23.373509\n",
      "****************************************************************************************************\n",
      "Epoch: [19/20]         || Step: [0/1172]        || Average Training Loss: 2.5565\n",
      "Epoch: [19/20]         || Step: [100/1172]      || Average Training Loss: 2.4281\n",
      "Epoch: [19/20]         || Step: [200/1172]      || Average Training Loss: 2.4489\n",
      "Epoch: [19/20]         || Step: [300/1172]      || Average Training Loss: 2.4494\n",
      "Epoch: [19/20]         || Step: [400/1172]      || Average Training Loss: 2.4363\n",
      "Epoch: [19/20]         || Step: [500/1172]      || Average Training Loss: 2.4398\n",
      "Epoch: [19/20]         || Step: [600/1172]      || Average Training Loss: 2.4446\n",
      "Epoch: [19/20]         || Step: [700/1172]      || Average Training Loss: 2.4443\n",
      "Epoch: [19/20]         || Step: [800/1172]      || Average Training Loss: 2.4477\n",
      "Epoch: [19/20]         || Step: [900/1172]      || Average Training Loss: 2.4476\n",
      "Epoch: [19/20]         || Step: [1000/1172]     || Average Training Loss: 2.4493\n",
      "Epoch: [19/20]         || Step: [1100/1172]     || Average Training Loss: 2.4489\n",
      "Epoch: [19/20]         || Step: [0/94]          || Average Validation Loss: 2.3247\n",
      "****************************************************************************************************\n",
      "Epoch: [19/20] || Training Loss = 2.45 || Validation Loss: 2.44 || Time: 23.445234\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f06f3-1a22-4b53-a87a-9e797a8e861d",
   "metadata": {},
   "source": [
    "## Modify the parameters\n",
    "\n",
    "Use entire train2017 data set to build the vocab\n",
    "\n",
    "And use entire dataset to train. Why not? What the hell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6d730d-0882-424f-b520-e330d1baeb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'everything'\n",
    "SUPER_CATEGORIES = None # should be list of eligible coco super categories, or None to include all images\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "PRINT_EVERY = 100\n",
    "TOTAL_EPOCH = 20\n",
    "CHECKPOINT = '../model/model_everything' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf61d9c2-bcbd-4c14-9090-57b8fe5daa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 102021 images\n",
      " val dataset has 15245 images\n",
      " test dataset has 4952 images\n",
      "There are 591753 captions in the data set\n",
      "With FREQ_THRESHOLD = 5, vocab size is 10192\n"
     ]
    }
   ],
   "source": [
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=SUPER_CATEGORIES,\n",
    "                 max_train=150000, max_val=50000, max_test=50000,\n",
    "                 save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file='captions_train2017.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc75f567-28d3-46fb-94df-02828d3dcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b81f5de-ea54-4f49-85a2-a282ba1c8bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 7971, Length of testing dataloader: 715\n",
      "Length of vocabulary: 10192\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_loader)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b43c55-c36a-4666-8a00-e9af6f9e2afe",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4951dffc-3082-4c18-8b71-b61f2f0774c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f58731e-1029-4a29-a023-5f4fe0845aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd6b7c6f-648b-4495-a457-022d011a0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "111f1d95-658a-41e9-aff3-fa5476cd6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a593bc2a-7354-4bc7-9e90-777451001ffd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bd6ee-a4c4-426f-894b-64ac184cf004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]          || Step: [0/7971]        || Average Training Loss: 9.2285\n",
      "Epoch: [0/20]          || Step: [100/7971]      || Average Training Loss: 5.4096\n",
      "Epoch: [0/20]          || Step: [200/7971]      || Average Training Loss: 4.9976\n",
      "Epoch: [0/20]          || Step: [300/7971]      || Average Training Loss: 4.7313\n",
      "Epoch: [0/20]          || Step: [400/7971]      || Average Training Loss: 4.5499\n",
      "Epoch: [0/20]          || Step: [500/7971]      || Average Training Loss: 4.4200\n",
      "Epoch: [0/20]          || Step: [600/7971]      || Average Training Loss: 4.3241\n",
      "Epoch: [0/20]          || Step: [700/7971]      || Average Training Loss: 4.2454\n",
      "Epoch: [0/20]          || Step: [800/7971]      || Average Training Loss: 4.1795\n",
      "Epoch: [0/20]          || Step: [900/7971]      || Average Training Loss: 4.1217\n",
      "Epoch: [0/20]          || Step: [1000/7971]     || Average Training Loss: 4.0718\n",
      "Epoch: [0/20]          || Step: [1100/7971]     || Average Training Loss: 4.0272\n",
      "Epoch: [0/20]          || Step: [1200/7971]     || Average Training Loss: 3.9841\n",
      "Epoch: [0/20]          || Step: [1300/7971]     || Average Training Loss: 3.9469\n",
      "Epoch: [0/20]          || Step: [1400/7971]     || Average Training Loss: 3.9145\n",
      "Epoch: [0/20]          || Step: [1500/7971]     || Average Training Loss: 3.8836\n",
      "Epoch: [0/20]          || Step: [1600/7971]     || Average Training Loss: 3.8549\n",
      "Epoch: [0/20]          || Step: [1700/7971]     || Average Training Loss: 3.8303\n",
      "Epoch: [0/20]          || Step: [1800/7971]     || Average Training Loss: 3.8059\n",
      "Epoch: [0/20]          || Step: [1900/7971]     || Average Training Loss: 3.7840\n",
      "Epoch: [0/20]          || Step: [2000/7971]     || Average Training Loss: 3.7629\n",
      "Epoch: [0/20]          || Step: [2100/7971]     || Average Training Loss: 3.7451\n",
      "Epoch: [0/20]          || Step: [2200/7971]     || Average Training Loss: 3.7266\n",
      "Epoch: [0/20]          || Step: [2300/7971]     || Average Training Loss: 3.7108\n",
      "Epoch: [0/20]          || Step: [2400/7971]     || Average Training Loss: 3.6951\n",
      "Epoch: [0/20]          || Step: [2500/7971]     || Average Training Loss: 3.6800\n",
      "Epoch: [0/20]          || Step: [2600/7971]     || Average Training Loss: 3.6658\n",
      "Epoch: [0/20]          || Step: [2700/7971]     || Average Training Loss: 3.6534\n",
      "Epoch: [0/20]          || Step: [2800/7971]     || Average Training Loss: 3.6411\n",
      "Epoch: [0/20]          || Step: [2900/7971]     || Average Training Loss: 3.6295\n",
      "Epoch: [0/20]          || Step: [3000/7971]     || Average Training Loss: 3.6183\n",
      "Epoch: [0/20]          || Step: [3100/7971]     || Average Training Loss: 3.6073\n",
      "Epoch: [0/20]          || Step: [3200/7971]     || Average Training Loss: 3.5972\n",
      "Epoch: [0/20]          || Step: [3300/7971]     || Average Training Loss: 3.5878\n",
      "Epoch: [0/20]          || Step: [3400/7971]     || Average Training Loss: 3.5792\n",
      "Epoch: [0/20]          || Step: [3500/7971]     || Average Training Loss: 3.5708\n",
      "Epoch: [0/20]          || Step: [3600/7971]     || Average Training Loss: 3.5617\n",
      "Epoch: [0/20]          || Step: [3700/7971]     || Average Training Loss: 3.5528\n",
      "Epoch: [0/20]          || Step: [3800/7971]     || Average Training Loss: 3.5452\n",
      "Epoch: [0/20]          || Step: [3900/7971]     || Average Training Loss: 3.5379\n",
      "Epoch: [0/20]          || Step: [4000/7971]     || Average Training Loss: 3.5305\n",
      "Epoch: [0/20]          || Step: [4100/7971]     || Average Training Loss: 3.5232\n",
      "Epoch: [0/20]          || Step: [4200/7971]     || Average Training Loss: 3.5170\n",
      "Epoch: [0/20]          || Step: [4300/7971]     || Average Training Loss: 3.5103\n",
      "Epoch: [0/20]          || Step: [4400/7971]     || Average Training Loss: 3.5042\n",
      "Epoch: [0/20]          || Step: [4500/7971]     || Average Training Loss: 3.4978\n",
      "Epoch: [0/20]          || Step: [4600/7971]     || Average Training Loss: 3.4921\n",
      "Epoch: [0/20]          || Step: [4700/7971]     || Average Training Loss: 3.4862\n",
      "Epoch: [0/20]          || Step: [4800/7971]     || Average Training Loss: 3.4811\n",
      "Epoch: [0/20]          || Step: [4900/7971]     || Average Training Loss: 3.4759\n",
      "Epoch: [0/20]          || Step: [5000/7971]     || Average Training Loss: 3.4709\n",
      "Epoch: [0/20]          || Step: [5100/7971]     || Average Training Loss: 3.4661\n",
      "Epoch: [0/20]          || Step: [5200/7971]     || Average Training Loss: 3.4611\n",
      "Epoch: [0/20]          || Step: [5300/7971]     || Average Training Loss: 3.4562\n",
      "Epoch: [0/20]          || Step: [5400/7971]     || Average Training Loss: 3.4515\n",
      "Epoch: [0/20]          || Step: [5500/7971]     || Average Training Loss: 3.4470\n",
      "Epoch: [0/20]          || Step: [5600/7971]     || Average Training Loss: 3.4429\n",
      "Epoch: [0/20]          || Step: [5700/7971]     || Average Training Loss: 3.4388\n",
      "Epoch: [0/20]          || Step: [5800/7971]     || Average Training Loss: 3.4345\n",
      "Epoch: [0/20]          || Step: [5900/7971]     || Average Training Loss: 3.4305\n",
      "Epoch: [0/20]          || Step: [6000/7971]     || Average Training Loss: 3.4266\n",
      "Epoch: [0/20]          || Step: [6100/7971]     || Average Training Loss: 3.4228\n",
      "Epoch: [0/20]          || Step: [6200/7971]     || Average Training Loss: 3.4192\n",
      "Epoch: [0/20]          || Step: [6300/7971]     || Average Training Loss: 3.4157\n",
      "Epoch: [0/20]          || Step: [6400/7971]     || Average Training Loss: 3.4119\n",
      "Epoch: [0/20]          || Step: [6500/7971]     || Average Training Loss: 3.4084\n",
      "Epoch: [0/20]          || Step: [6600/7971]     || Average Training Loss: 3.4054\n",
      "Epoch: [0/20]          || Step: [6700/7971]     || Average Training Loss: 3.4018\n",
      "Epoch: [0/20]          || Step: [6800/7971]     || Average Training Loss: 3.3985\n",
      "Epoch: [0/20]          || Step: [6900/7971]     || Average Training Loss: 3.3955\n",
      "Epoch: [0/20]          || Step: [7000/7971]     || Average Training Loss: 3.3921\n",
      "Epoch: [0/20]          || Step: [7100/7971]     || Average Training Loss: 3.3891\n",
      "Epoch: [0/20]          || Step: [7200/7971]     || Average Training Loss: 3.3862\n",
      "Epoch: [0/20]          || Step: [7300/7971]     || Average Training Loss: 3.3837\n",
      "Epoch: [0/20]          || Step: [7400/7971]     || Average Training Loss: 3.3814\n",
      "Epoch: [0/20]          || Step: [7500/7971]     || Average Training Loss: 3.3785\n",
      "Epoch: [0/20]          || Step: [7600/7971]     || Average Training Loss: 3.3758\n",
      "Epoch: [0/20]          || Step: [7700/7971]     || Average Training Loss: 3.3730\n",
      "Epoch: [0/20]          || Step: [7800/7971]     || Average Training Loss: 3.3701\n",
      "Epoch: [0/20]          || Step: [7900/7971]     || Average Training Loss: 3.3675\n",
      "Epoch: [0/20]          || Step: [0/715]         || Average Validation Loss: 3.2936\n",
      "Epoch: [0/20]          || Step: [100/715]       || Average Validation Loss: 3.1465\n",
      "Epoch: [0/20]          || Step: [200/715]       || Average Validation Loss: 3.1545\n",
      "Epoch: [0/20]          || Step: [300/715]       || Average Validation Loss: 3.1545\n",
      "Epoch: [0/20]          || Step: [400/715]       || Average Validation Loss: 3.1561\n",
      "Epoch: [0/20]          || Step: [500/715]       || Average Validation Loss: 3.1538\n",
      "Epoch: [0/20]          || Step: [600/715]       || Average Validation Loss: 3.1549\n",
      "Epoch: [0/20]          || Step: [700/715]       || Average Validation Loss: 3.1573\n",
      "****************************************************************************************************\n",
      "Epoch: [0/20] || Training Loss = 3.37 || Validation Loss: 3.16 || Time: 154.330686\n",
      "****************************************************************************************************\n",
      "Epoch: [1/20]          || Step: [0/7971]        || Average Training Loss: 3.1757\n",
      "Epoch: [1/20]          || Step: [100/7971]      || Average Training Loss: 3.1674\n",
      "Epoch: [1/20]          || Step: [200/7971]      || Average Training Loss: 3.1715\n",
      "Epoch: [1/20]          || Step: [300/7971]      || Average Training Loss: 3.1686\n",
      "Epoch: [1/20]          || Step: [400/7971]      || Average Training Loss: 3.1663\n",
      "Epoch: [1/20]          || Step: [500/7971]      || Average Training Loss: 3.1688\n",
      "Epoch: [1/20]          || Step: [600/7971]      || Average Training Loss: 3.1689\n",
      "Epoch: [1/20]          || Step: [700/7971]      || Average Training Loss: 3.1671\n",
      "Epoch: [1/20]          || Step: [800/7971]      || Average Training Loss: 3.1673\n",
      "Epoch: [1/20]          || Step: [900/7971]      || Average Training Loss: 3.1666\n",
      "Epoch: [1/20]          || Step: [1000/7971]     || Average Training Loss: 3.1663\n",
      "Epoch: [1/20]          || Step: [1100/7971]     || Average Training Loss: 3.1644\n",
      "Epoch: [1/20]          || Step: [1200/7971]     || Average Training Loss: 3.1655\n",
      "Epoch: [1/20]          || Step: [1300/7971]     || Average Training Loss: 3.1643\n",
      "Epoch: [1/20]          || Step: [1400/7971]     || Average Training Loss: 3.1630\n",
      "Epoch: [1/20]          || Step: [1500/7971]     || Average Training Loss: 3.1630\n",
      "Epoch: [1/20]          || Step: [1600/7971]     || Average Training Loss: 3.1623\n",
      "Epoch: [1/20]          || Step: [1700/7971]     || Average Training Loss: 3.1620\n",
      "Epoch: [1/20]          || Step: [1800/7971]     || Average Training Loss: 3.1611\n",
      "Epoch: [1/20]          || Step: [1900/7971]     || Average Training Loss: 3.1597\n",
      "Epoch: [1/20]          || Step: [2000/7971]     || Average Training Loss: 3.1595\n",
      "Epoch: [1/20]          || Step: [2100/7971]     || Average Training Loss: 3.1593\n",
      "Epoch: [1/20]          || Step: [2200/7971]     || Average Training Loss: 3.1575\n",
      "Epoch: [1/20]          || Step: [2300/7971]     || Average Training Loss: 3.1567\n",
      "Epoch: [1/20]          || Step: [2400/7971]     || Average Training Loss: 3.1570\n",
      "Epoch: [1/20]          || Step: [2500/7971]     || Average Training Loss: 3.1562\n",
      "Epoch: [1/20]          || Step: [2600/7971]     || Average Training Loss: 3.1564\n",
      "Epoch: [1/20]          || Step: [2700/7971]     || Average Training Loss: 3.1561\n",
      "Epoch: [1/20]          || Step: [2800/7971]     || Average Training Loss: 3.1553\n",
      "Epoch: [1/20]          || Step: [2900/7971]     || Average Training Loss: 3.1546\n",
      "Epoch: [1/20]          || Step: [3000/7971]     || Average Training Loss: 3.1552\n",
      "Epoch: [1/20]          || Step: [3100/7971]     || Average Training Loss: 3.1553\n",
      "Epoch: [1/20]          || Step: [3200/7971]     || Average Training Loss: 3.1553\n",
      "Epoch: [1/20]          || Step: [3300/7971]     || Average Training Loss: 3.1542\n",
      "Epoch: [1/20]          || Step: [3400/7971]     || Average Training Loss: 3.1538\n",
      "Epoch: [1/20]          || Step: [3500/7971]     || Average Training Loss: 3.1529\n",
      "Epoch: [1/20]          || Step: [3600/7971]     || Average Training Loss: 3.1519\n",
      "Epoch: [1/20]          || Step: [3700/7971]     || Average Training Loss: 3.1514\n",
      "Epoch: [1/20]          || Step: [3800/7971]     || Average Training Loss: 3.1512\n",
      "Epoch: [1/20]          || Step: [3900/7971]     || Average Training Loss: 3.1508\n",
      "Epoch: [1/20]          || Step: [4000/7971]     || Average Training Loss: 3.1501\n",
      "Epoch: [1/20]          || Step: [4100/7971]     || Average Training Loss: 3.1497\n",
      "Epoch: [1/20]          || Step: [4200/7971]     || Average Training Loss: 3.1488\n",
      "Epoch: [1/20]          || Step: [4300/7971]     || Average Training Loss: 3.1484\n",
      "Epoch: [1/20]          || Step: [4400/7971]     || Average Training Loss: 3.1484\n",
      "Epoch: [1/20]          || Step: [4500/7971]     || Average Training Loss: 3.1483\n",
      "Epoch: [1/20]          || Step: [4600/7971]     || Average Training Loss: 3.1480\n",
      "Epoch: [1/20]          || Step: [4700/7971]     || Average Training Loss: 3.1479\n",
      "Epoch: [1/20]          || Step: [4800/7971]     || Average Training Loss: 3.1476\n",
      "Epoch: [1/20]          || Step: [4900/7971]     || Average Training Loss: 3.1473\n",
      "Epoch: [1/20]          || Step: [5000/7971]     || Average Training Loss: 3.1471\n",
      "Epoch: [1/20]          || Step: [5100/7971]     || Average Training Loss: 3.1470\n",
      "Epoch: [1/20]          || Step: [5200/7971]     || Average Training Loss: 3.1463\n",
      "Epoch: [1/20]          || Step: [5300/7971]     || Average Training Loss: 3.1465\n",
      "Epoch: [1/20]          || Step: [5400/7971]     || Average Training Loss: 3.1462\n",
      "Epoch: [1/20]          || Step: [5500/7971]     || Average Training Loss: 3.1454\n",
      "Epoch: [1/20]          || Step: [5600/7971]     || Average Training Loss: 3.1447\n",
      "Epoch: [1/20]          || Step: [5700/7971]     || Average Training Loss: 3.1442\n",
      "Epoch: [1/20]          || Step: [5800/7971]     || Average Training Loss: 3.1440\n",
      "Epoch: [1/20]          || Step: [5900/7971]     || Average Training Loss: 3.1438\n",
      "Epoch: [1/20]          || Step: [6000/7971]     || Average Training Loss: 3.1434\n",
      "Epoch: [1/20]          || Step: [6100/7971]     || Average Training Loss: 3.1433\n",
      "Epoch: [1/20]          || Step: [6200/7971]     || Average Training Loss: 3.1428\n",
      "Epoch: [1/20]          || Step: [6300/7971]     || Average Training Loss: 3.1428\n",
      "Epoch: [1/20]          || Step: [6400/7971]     || Average Training Loss: 3.1423\n",
      "Epoch: [1/20]          || Step: [6500/7971]     || Average Training Loss: 3.1422\n",
      "Epoch: [1/20]          || Step: [6600/7971]     || Average Training Loss: 3.1422\n",
      "Epoch: [1/20]          || Step: [6700/7971]     || Average Training Loss: 3.1419\n",
      "Epoch: [1/20]          || Step: [6800/7971]     || Average Training Loss: 3.1416\n",
      "Epoch: [1/20]          || Step: [6900/7971]     || Average Training Loss: 3.1413\n",
      "Epoch: [1/20]          || Step: [7000/7971]     || Average Training Loss: 3.1409\n",
      "Epoch: [1/20]          || Step: [7100/7971]     || Average Training Loss: 3.1409\n",
      "Epoch: [1/20]          || Step: [7200/7971]     || Average Training Loss: 3.1407\n",
      "Epoch: [1/20]          || Step: [7300/7971]     || Average Training Loss: 3.1403\n",
      "Epoch: [1/20]          || Step: [7400/7971]     || Average Training Loss: 3.1402\n",
      "Epoch: [1/20]          || Step: [7500/7971]     || Average Training Loss: 3.1397\n",
      "Epoch: [1/20]          || Step: [7600/7971]     || Average Training Loss: 3.1397\n",
      "Epoch: [1/20]          || Step: [7700/7971]     || Average Training Loss: 3.1392\n",
      "Epoch: [1/20]          || Step: [7800/7971]     || Average Training Loss: 3.1390\n",
      "Epoch: [1/20]          || Step: [7900/7971]     || Average Training Loss: 3.1384\n",
      "Epoch: [1/20]          || Step: [0/715]         || Average Validation Loss: 3.1811\n",
      "Epoch: [1/20]          || Step: [100/715]       || Average Validation Loss: 3.0966\n",
      "Epoch: [1/20]          || Step: [200/715]       || Average Validation Loss: 3.0930\n",
      "Epoch: [1/20]          || Step: [300/715]       || Average Validation Loss: 3.0961\n",
      "Epoch: [1/20]          || Step: [400/715]       || Average Validation Loss: 3.0960\n",
      "Epoch: [1/20]          || Step: [500/715]       || Average Validation Loss: 3.0964\n",
      "Epoch: [1/20]          || Step: [600/715]       || Average Validation Loss: 3.0995\n",
      "Epoch: [1/20]          || Step: [700/715]       || Average Validation Loss: 3.1007\n",
      "****************************************************************************************************\n",
      "Epoch: [1/20] || Training Loss = 3.14 || Validation Loss: 3.10 || Time: 113.911075\n",
      "****************************************************************************************************\n",
      "Epoch: [2/20]          || Step: [0/7971]        || Average Training Loss: 3.0873\n",
      "Epoch: [2/20]          || Step: [100/7971]      || Average Training Loss: 3.1192\n",
      "Epoch: [2/20]          || Step: [200/7971]      || Average Training Loss: 3.1106\n",
      "Epoch: [2/20]          || Step: [300/7971]      || Average Training Loss: 3.1057\n",
      "Epoch: [2/20]          || Step: [400/7971]      || Average Training Loss: 3.1152\n",
      "Epoch: [2/20]          || Step: [500/7971]      || Average Training Loss: 3.1131\n",
      "Epoch: [2/20]          || Step: [600/7971]      || Average Training Loss: 3.1120\n",
      "Epoch: [2/20]          || Step: [700/7971]      || Average Training Loss: 3.1102\n",
      "Epoch: [2/20]          || Step: [800/7971]      || Average Training Loss: 3.1128\n",
      "Epoch: [2/20]          || Step: [900/7971]      || Average Training Loss: 3.1109\n",
      "Epoch: [2/20]          || Step: [1000/7971]     || Average Training Loss: 3.1106\n",
      "Epoch: [2/20]          || Step: [1100/7971]     || Average Training Loss: 3.1113\n",
      "Epoch: [2/20]          || Step: [1200/7971]     || Average Training Loss: 3.1108\n",
      "Epoch: [2/20]          || Step: [1300/7971]     || Average Training Loss: 3.1115\n",
      "Epoch: [2/20]          || Step: [1400/7971]     || Average Training Loss: 3.1123\n",
      "Epoch: [2/20]          || Step: [1500/7971]     || Average Training Loss: 3.1113\n",
      "Epoch: [2/20]          || Step: [1600/7971]     || Average Training Loss: 3.1122\n",
      "Epoch: [2/20]          || Step: [1700/7971]     || Average Training Loss: 3.1113\n",
      "Epoch: [2/20]          || Step: [1800/7971]     || Average Training Loss: 3.1124\n",
      "Epoch: [2/20]          || Step: [1900/7971]     || Average Training Loss: 3.1125\n",
      "Epoch: [2/20]          || Step: [2000/7971]     || Average Training Loss: 3.1134\n",
      "Epoch: [2/20]          || Step: [2100/7971]     || Average Training Loss: 3.1139\n",
      "Epoch: [2/20]          || Step: [2200/7971]     || Average Training Loss: 3.1138\n",
      "Epoch: [2/20]          || Step: [2300/7971]     || Average Training Loss: 3.1130\n",
      "Epoch: [2/20]          || Step: [2400/7971]     || Average Training Loss: 3.1130\n",
      "Epoch: [2/20]          || Step: [2500/7971]     || Average Training Loss: 3.1134\n",
      "Epoch: [2/20]          || Step: [2600/7971]     || Average Training Loss: 3.1135\n",
      "Epoch: [2/20]          || Step: [2700/7971]     || Average Training Loss: 3.1139\n",
      "Epoch: [2/20]          || Step: [2800/7971]     || Average Training Loss: 3.1141\n",
      "Epoch: [2/20]          || Step: [2900/7971]     || Average Training Loss: 3.1135\n",
      "Epoch: [2/20]          || Step: [3000/7971]     || Average Training Loss: 3.1136\n",
      "Epoch: [2/20]          || Step: [3100/7971]     || Average Training Loss: 3.1139\n",
      "Epoch: [2/20]          || Step: [3200/7971]     || Average Training Loss: 3.1139\n",
      "Epoch: [2/20]          || Step: [3300/7971]     || Average Training Loss: 3.1137\n",
      "Epoch: [2/20]          || Step: [3400/7971]     || Average Training Loss: 3.1136\n",
      "Epoch: [2/20]          || Step: [3500/7971]     || Average Training Loss: 3.1137\n",
      "Epoch: [2/20]          || Step: [3600/7971]     || Average Training Loss: 3.1131\n",
      "Epoch: [2/20]          || Step: [3700/7971]     || Average Training Loss: 3.1129\n",
      "Epoch: [2/20]          || Step: [3800/7971]     || Average Training Loss: 3.1126\n",
      "Epoch: [2/20]          || Step: [3900/7971]     || Average Training Loss: 3.1125\n",
      "Epoch: [2/20]          || Step: [4000/7971]     || Average Training Loss: 3.1125\n",
      "Epoch: [2/20]          || Step: [4100/7971]     || Average Training Loss: 3.1121\n",
      "Epoch: [2/20]          || Step: [4200/7971]     || Average Training Loss: 3.1122\n",
      "Epoch: [2/20]          || Step: [4300/7971]     || Average Training Loss: 3.1120\n",
      "Epoch: [2/20]          || Step: [4400/7971]     || Average Training Loss: 3.1117\n",
      "Epoch: [2/20]          || Step: [4500/7971]     || Average Training Loss: 3.1112\n",
      "Epoch: [2/20]          || Step: [4600/7971]     || Average Training Loss: 3.1109\n",
      "Epoch: [2/20]          || Step: [4700/7971]     || Average Training Loss: 3.1107\n",
      "Epoch: [2/20]          || Step: [4800/7971]     || Average Training Loss: 3.1103\n",
      "Epoch: [2/20]          || Step: [4900/7971]     || Average Training Loss: 3.1096\n",
      "Epoch: [2/20]          || Step: [5000/7971]     || Average Training Loss: 3.1092\n",
      "Epoch: [2/20]          || Step: [5100/7971]     || Average Training Loss: 3.1092\n",
      "Epoch: [2/20]          || Step: [5200/7971]     || Average Training Loss: 3.1091\n",
      "Epoch: [2/20]          || Step: [5300/7971]     || Average Training Loss: 3.1091\n",
      "Epoch: [2/20]          || Step: [5400/7971]     || Average Training Loss: 3.1087\n",
      "Epoch: [2/20]          || Step: [5500/7971]     || Average Training Loss: 3.1084\n",
      "Epoch: [2/20]          || Step: [5600/7971]     || Average Training Loss: 3.1081\n",
      "Epoch: [2/20]          || Step: [5700/7971]     || Average Training Loss: 3.1082\n",
      "Epoch: [2/20]          || Step: [5800/7971]     || Average Training Loss: 3.1082\n",
      "Epoch: [2/20]          || Step: [5900/7971]     || Average Training Loss: 3.1079\n",
      "Epoch: [2/20]          || Step: [6000/7971]     || Average Training Loss: 3.1081\n",
      "Epoch: [2/20]          || Step: [6100/7971]     || Average Training Loss: 3.1086\n",
      "Epoch: [2/20]          || Step: [6200/7971]     || Average Training Loss: 3.1083\n",
      "Epoch: [2/20]          || Step: [6300/7971]     || Average Training Loss: 3.1078\n",
      "Epoch: [2/20]          || Step: [6400/7971]     || Average Training Loss: 3.1079\n",
      "Epoch: [2/20]          || Step: [6500/7971]     || Average Training Loss: 3.1079\n",
      "Epoch: [2/20]          || Step: [6600/7971]     || Average Training Loss: 3.1077\n",
      "Epoch: [2/20]          || Step: [6700/7971]     || Average Training Loss: 3.1076\n",
      "Epoch: [2/20]          || Step: [6800/7971]     || Average Training Loss: 3.1077\n",
      "Epoch: [2/20]          || Step: [6900/7971]     || Average Training Loss: 3.1079\n",
      "Epoch: [2/20]          || Step: [7000/7971]     || Average Training Loss: 3.1083\n",
      "Epoch: [2/20]          || Step: [7100/7971]     || Average Training Loss: 3.1084\n",
      "Epoch: [2/20]          || Step: [7200/7971]     || Average Training Loss: 3.1081\n",
      "Epoch: [2/20]          || Step: [7300/7971]     || Average Training Loss: 3.1083\n",
      "Epoch: [2/20]          || Step: [7400/7971]     || Average Training Loss: 3.1080\n",
      "Epoch: [2/20]          || Step: [7500/7971]     || Average Training Loss: 3.1082\n",
      "Epoch: [2/20]          || Step: [7600/7971]     || Average Training Loss: 3.1080\n",
      "Epoch: [2/20]          || Step: [7700/7971]     || Average Training Loss: 3.1081\n",
      "Epoch: [2/20]          || Step: [7800/7971]     || Average Training Loss: 3.1080\n",
      "Epoch: [2/20]          || Step: [7900/7971]     || Average Training Loss: 3.1079\n",
      "Epoch: [2/20]          || Step: [0/715]         || Average Validation Loss: 2.9021\n",
      "Epoch: [2/20]          || Step: [100/715]       || Average Validation Loss: 3.0786\n",
      "Epoch: [2/20]          || Step: [200/715]       || Average Validation Loss: 3.0819\n",
      "Epoch: [2/20]          || Step: [300/715]       || Average Validation Loss: 3.0743\n",
      "Epoch: [2/20]          || Step: [400/715]       || Average Validation Loss: 3.0675\n",
      "Epoch: [2/20]          || Step: [500/715]       || Average Validation Loss: 3.0677\n",
      "Epoch: [2/20]          || Step: [600/715]       || Average Validation Loss: 3.0671\n",
      "Epoch: [2/20]          || Step: [700/715]       || Average Validation Loss: 3.0686\n",
      "****************************************************************************************************\n",
      "Epoch: [2/20] || Training Loss = 3.11 || Validation Loss: 3.07 || Time: 117.050364\n",
      "****************************************************************************************************\n",
      "Epoch: [3/20]          || Step: [0/7971]        || Average Training Loss: 3.0901\n",
      "Epoch: [3/20]          || Step: [100/7971]      || Average Training Loss: 3.1129\n",
      "Epoch: [3/20]          || Step: [200/7971]      || Average Training Loss: 3.1070\n",
      "Epoch: [3/20]          || Step: [300/7971]      || Average Training Loss: 3.1075\n",
      "Epoch: [3/20]          || Step: [400/7971]      || Average Training Loss: 3.1046\n",
      "Epoch: [3/20]          || Step: [500/7971]      || Average Training Loss: 3.0964\n",
      "Epoch: [3/20]          || Step: [600/7971]      || Average Training Loss: 3.0985\n",
      "Epoch: [3/20]          || Step: [700/7971]      || Average Training Loss: 3.0959\n",
      "Epoch: [3/20]          || Step: [800/7971]      || Average Training Loss: 3.0959\n",
      "Epoch: [3/20]          || Step: [900/7971]      || Average Training Loss: 3.0956\n",
      "Epoch: [3/20]          || Step: [1000/7971]     || Average Training Loss: 3.0964\n",
      "Epoch: [3/20]          || Step: [1100/7971]     || Average Training Loss: 3.0967\n",
      "Epoch: [3/20]          || Step: [1200/7971]     || Average Training Loss: 3.0960\n",
      "Epoch: [3/20]          || Step: [1300/7971]     || Average Training Loss: 3.0948\n",
      "Epoch: [3/20]          || Step: [1400/7971]     || Average Training Loss: 3.0952\n",
      "Epoch: [3/20]          || Step: [1500/7971]     || Average Training Loss: 3.0941\n",
      "Epoch: [3/20]          || Step: [1600/7971]     || Average Training Loss: 3.0941\n",
      "Epoch: [3/20]          || Step: [1700/7971]     || Average Training Loss: 3.0940\n",
      "Epoch: [3/20]          || Step: [1800/7971]     || Average Training Loss: 3.0934\n",
      "Epoch: [3/20]          || Step: [1900/7971]     || Average Training Loss: 3.0932\n",
      "Epoch: [3/20]          || Step: [2000/7971]     || Average Training Loss: 3.0935\n",
      "Epoch: [3/20]          || Step: [2100/7971]     || Average Training Loss: 3.0933\n",
      "Epoch: [3/20]          || Step: [2200/7971]     || Average Training Loss: 3.0947\n",
      "Epoch: [3/20]          || Step: [2300/7971]     || Average Training Loss: 3.0955\n",
      "Epoch: [3/20]          || Step: [2400/7971]     || Average Training Loss: 3.0963\n",
      "Epoch: [3/20]          || Step: [2500/7971]     || Average Training Loss: 3.0962\n",
      "Epoch: [3/20]          || Step: [2600/7971]     || Average Training Loss: 3.0958\n",
      "Epoch: [3/20]          || Step: [2700/7971]     || Average Training Loss: 3.0953\n",
      "Epoch: [3/20]          || Step: [2800/7971]     || Average Training Loss: 3.0955\n",
      "Epoch: [3/20]          || Step: [2900/7971]     || Average Training Loss: 3.0958\n",
      "Epoch: [3/20]          || Step: [3000/7971]     || Average Training Loss: 3.0951\n",
      "Epoch: [3/20]          || Step: [3100/7971]     || Average Training Loss: 3.0955\n",
      "Epoch: [3/20]          || Step: [3200/7971]     || Average Training Loss: 3.0944\n",
      "Epoch: [3/20]          || Step: [3300/7971]     || Average Training Loss: 3.0948\n",
      "Epoch: [3/20]          || Step: [3400/7971]     || Average Training Loss: 3.0942\n",
      "Epoch: [3/20]          || Step: [3500/7971]     || Average Training Loss: 3.0942\n",
      "Epoch: [3/20]          || Step: [3600/7971]     || Average Training Loss: 3.0948\n",
      "Epoch: [3/20]          || Step: [3700/7971]     || Average Training Loss: 3.0944\n",
      "Epoch: [3/20]          || Step: [3800/7971]     || Average Training Loss: 3.0944\n",
      "Epoch: [3/20]          || Step: [3900/7971]     || Average Training Loss: 3.0947\n",
      "Epoch: [3/20]          || Step: [4000/7971]     || Average Training Loss: 3.0944\n",
      "Epoch: [3/20]          || Step: [4100/7971]     || Average Training Loss: 3.0944\n",
      "Epoch: [3/20]          || Step: [4200/7971]     || Average Training Loss: 3.0947\n",
      "Epoch: [3/20]          || Step: [4300/7971]     || Average Training Loss: 3.0948\n",
      "Epoch: [3/20]          || Step: [4400/7971]     || Average Training Loss: 3.0948\n",
      "Epoch: [3/20]          || Step: [4500/7971]     || Average Training Loss: 3.0946\n",
      "Epoch: [3/20]          || Step: [4600/7971]     || Average Training Loss: 3.0942\n",
      "Epoch: [3/20]          || Step: [4700/7971]     || Average Training Loss: 3.0941\n",
      "Epoch: [3/20]          || Step: [4800/7971]     || Average Training Loss: 3.0938\n",
      "Epoch: [3/20]          || Step: [4900/7971]     || Average Training Loss: 3.0936\n",
      "Epoch: [3/20]          || Step: [5000/7971]     || Average Training Loss: 3.0938\n",
      "Epoch: [3/20]          || Step: [5100/7971]     || Average Training Loss: 3.0941\n",
      "Epoch: [3/20]          || Step: [5200/7971]     || Average Training Loss: 3.0941\n",
      "Epoch: [3/20]          || Step: [5300/7971]     || Average Training Loss: 3.0936\n",
      "Epoch: [3/20]          || Step: [5400/7971]     || Average Training Loss: 3.0930\n",
      "Epoch: [3/20]          || Step: [5500/7971]     || Average Training Loss: 3.0932\n",
      "Epoch: [3/20]          || Step: [5600/7971]     || Average Training Loss: 3.0933\n",
      "Epoch: [3/20]          || Step: [5700/7971]     || Average Training Loss: 3.0935\n",
      "Epoch: [3/20]          || Step: [5800/7971]     || Average Training Loss: 3.0931\n",
      "Epoch: [3/20]          || Step: [5900/7971]     || Average Training Loss: 3.0928\n",
      "Epoch: [3/20]          || Step: [6000/7971]     || Average Training Loss: 3.0930\n",
      "Epoch: [3/20]          || Step: [6100/7971]     || Average Training Loss: 3.0931\n",
      "Epoch: [3/20]          || Step: [6200/7971]     || Average Training Loss: 3.0931\n",
      "Epoch: [3/20]          || Step: [6300/7971]     || Average Training Loss: 3.0930\n",
      "Epoch: [3/20]          || Step: [6400/7971]     || Average Training Loss: 3.0930\n",
      "Epoch: [3/20]          || Step: [6500/7971]     || Average Training Loss: 3.0931\n",
      "Epoch: [3/20]          || Step: [6600/7971]     || Average Training Loss: 3.0930\n",
      "Epoch: [3/20]          || Step: [6700/7971]     || Average Training Loss: 3.0931\n",
      "Epoch: [3/20]          || Step: [6800/7971]     || Average Training Loss: 3.0934\n",
      "Epoch: [3/20]          || Step: [6900/7971]     || Average Training Loss: 3.0935\n",
      "Epoch: [3/20]          || Step: [7000/7971]     || Average Training Loss: 3.0936\n",
      "Epoch: [3/20]          || Step: [7100/7971]     || Average Training Loss: 3.0937\n",
      "Epoch: [3/20]          || Step: [7200/7971]     || Average Training Loss: 3.0939\n",
      "Epoch: [3/20]          || Step: [7300/7971]     || Average Training Loss: 3.0937\n",
      "Epoch: [3/20]          || Step: [7400/7971]     || Average Training Loss: 3.0937\n",
      "Epoch: [3/20]          || Step: [7500/7971]     || Average Training Loss: 3.0937\n",
      "Epoch: [3/20]          || Step: [7600/7971]     || Average Training Loss: 3.0939\n",
      "Epoch: [3/20]          || Step: [7700/7971]     || Average Training Loss: 3.0938\n",
      "Epoch: [3/20]          || Step: [7800/7971]     || Average Training Loss: 3.0937\n",
      "Epoch: [3/20]          || Step: [7900/7971]     || Average Training Loss: 3.0935\n",
      "Epoch: [3/20]          || Step: [0/715]         || Average Validation Loss: 2.9335\n",
      "Epoch: [3/20]          || Step: [100/715]       || Average Validation Loss: 3.0396\n",
      "Epoch: [3/20]          || Step: [200/715]       || Average Validation Loss: 3.0494\n",
      "Epoch: [3/20]          || Step: [300/715]       || Average Validation Loss: 3.0479\n",
      "Epoch: [3/20]          || Step: [400/715]       || Average Validation Loss: 3.0472\n",
      "Epoch: [3/20]          || Step: [500/715]       || Average Validation Loss: 3.0467\n",
      "Epoch: [3/20]          || Step: [600/715]       || Average Validation Loss: 3.0464\n",
      "Epoch: [3/20]          || Step: [700/715]       || Average Validation Loss: 3.0481\n",
      "****************************************************************************************************\n",
      "Epoch: [3/20] || Training Loss = 3.09 || Validation Loss: 3.05 || Time: 99.071148\n",
      "****************************************************************************************************\n",
      "Epoch: [4/20]          || Step: [0/7971]        || Average Training Loss: 3.0914\n",
      "Epoch: [4/20]          || Step: [100/7971]      || Average Training Loss: 3.0828\n",
      "Epoch: [4/20]          || Step: [200/7971]      || Average Training Loss: 3.0935\n",
      "Epoch: [4/20]          || Step: [300/7971]      || Average Training Loss: 3.0947\n",
      "Epoch: [4/20]          || Step: [400/7971]      || Average Training Loss: 3.0976\n",
      "Epoch: [4/20]          || Step: [500/7971]      || Average Training Loss: 3.0975\n",
      "Epoch: [4/20]          || Step: [600/7971]      || Average Training Loss: 3.0940\n",
      "Epoch: [4/20]          || Step: [700/7971]      || Average Training Loss: 3.0956\n",
      "Epoch: [4/20]          || Step: [800/7971]      || Average Training Loss: 3.0926\n",
      "Epoch: [4/20]          || Step: [900/7971]      || Average Training Loss: 3.0935\n",
      "Epoch: [4/20]          || Step: [1000/7971]     || Average Training Loss: 3.0920\n",
      "Epoch: [4/20]          || Step: [1100/7971]     || Average Training Loss: 3.0912\n",
      "Epoch: [4/20]          || Step: [1200/7971]     || Average Training Loss: 3.0905\n",
      "Epoch: [4/20]          || Step: [1300/7971]     || Average Training Loss: 3.0912\n",
      "Epoch: [4/20]          || Step: [1400/7971]     || Average Training Loss: 3.0909\n",
      "Epoch: [4/20]          || Step: [1500/7971]     || Average Training Loss: 3.0905\n",
      "Epoch: [4/20]          || Step: [1600/7971]     || Average Training Loss: 3.0908\n",
      "Epoch: [4/20]          || Step: [1700/7971]     || Average Training Loss: 3.0902\n",
      "Epoch: [4/20]          || Step: [1800/7971]     || Average Training Loss: 3.0895\n",
      "Epoch: [4/20]          || Step: [1900/7971]     || Average Training Loss: 3.0887\n",
      "Epoch: [4/20]          || Step: [2000/7971]     || Average Training Loss: 3.0878\n",
      "Epoch: [4/20]          || Step: [2100/7971]     || Average Training Loss: 3.0885\n",
      "Epoch: [4/20]          || Step: [2200/7971]     || Average Training Loss: 3.0877\n",
      "Epoch: [4/20]          || Step: [2300/7971]     || Average Training Loss: 3.0879\n",
      "Epoch: [4/20]          || Step: [2400/7971]     || Average Training Loss: 3.0882\n",
      "Epoch: [4/20]          || Step: [2500/7971]     || Average Training Loss: 3.0890\n",
      "Epoch: [4/20]          || Step: [2600/7971]     || Average Training Loss: 3.0880\n",
      "Epoch: [4/20]          || Step: [2700/7971]     || Average Training Loss: 3.0874\n",
      "Epoch: [4/20]          || Step: [2800/7971]     || Average Training Loss: 3.0882\n",
      "Epoch: [4/20]          || Step: [2900/7971]     || Average Training Loss: 3.0890\n",
      "Epoch: [4/20]          || Step: [3000/7971]     || Average Training Loss: 3.0893\n",
      "Epoch: [4/20]          || Step: [3100/7971]     || Average Training Loss: 3.0895\n",
      "Epoch: [4/20]          || Step: [3200/7971]     || Average Training Loss: 3.0907\n",
      "Epoch: [4/20]          || Step: [3300/7971]     || Average Training Loss: 3.0914\n",
      "Epoch: [4/20]          || Step: [3400/7971]     || Average Training Loss: 3.0913\n",
      "Epoch: [4/20]          || Step: [3500/7971]     || Average Training Loss: 3.0910\n",
      "Epoch: [4/20]          || Step: [3600/7971]     || Average Training Loss: 3.0910\n",
      "Epoch: [4/20]          || Step: [3700/7971]     || Average Training Loss: 3.0912\n",
      "Epoch: [4/20]          || Step: [3800/7971]     || Average Training Loss: 3.0910\n",
      "Epoch: [4/20]          || Step: [3900/7971]     || Average Training Loss: 3.0912\n",
      "Epoch: [4/20]          || Step: [4000/7971]     || Average Training Loss: 3.0908\n",
      "Epoch: [4/20]          || Step: [4100/7971]     || Average Training Loss: 3.0905\n",
      "Epoch: [4/20]          || Step: [4200/7971]     || Average Training Loss: 3.0900\n",
      "Epoch: [4/20]          || Step: [4300/7971]     || Average Training Loss: 3.0891\n",
      "Epoch: [4/20]          || Step: [4400/7971]     || Average Training Loss: 3.0886\n",
      "Epoch: [4/20]          || Step: [4500/7971]     || Average Training Loss: 3.0885\n",
      "Epoch: [4/20]          || Step: [4600/7971]     || Average Training Loss: 3.0884\n",
      "Epoch: [4/20]          || Step: [4700/7971]     || Average Training Loss: 3.0882\n",
      "Epoch: [4/20]          || Step: [4800/7971]     || Average Training Loss: 3.0884\n",
      "Epoch: [4/20]          || Step: [4900/7971]     || Average Training Loss: 3.0881\n",
      "Epoch: [4/20]          || Step: [5000/7971]     || Average Training Loss: 3.0878\n",
      "Epoch: [4/20]          || Step: [5100/7971]     || Average Training Loss: 3.0876\n",
      "Epoch: [4/20]          || Step: [5200/7971]     || Average Training Loss: 3.0875\n",
      "Epoch: [4/20]          || Step: [5300/7971]     || Average Training Loss: 3.0873\n",
      "Epoch: [4/20]          || Step: [5400/7971]     || Average Training Loss: 3.0875\n",
      "Epoch: [4/20]          || Step: [5500/7971]     || Average Training Loss: 3.0869\n",
      "Epoch: [4/20]          || Step: [5600/7971]     || Average Training Loss: 3.0871\n",
      "Epoch: [4/20]          || Step: [5700/7971]     || Average Training Loss: 3.0874\n",
      "Epoch: [4/20]          || Step: [5800/7971]     || Average Training Loss: 3.0876\n",
      "Epoch: [4/20]          || Step: [5900/7971]     || Average Training Loss: 3.0871\n",
      "Epoch: [4/20]          || Step: [6000/7971]     || Average Training Loss: 3.0871\n",
      "Epoch: [4/20]          || Step: [6100/7971]     || Average Training Loss: 3.0870\n",
      "Epoch: [4/20]          || Step: [6200/7971]     || Average Training Loss: 3.0871\n",
      "Epoch: [4/20]          || Step: [6300/7971]     || Average Training Loss: 3.0869\n",
      "Epoch: [4/20]          || Step: [6400/7971]     || Average Training Loss: 3.0867\n",
      "Epoch: [4/20]          || Step: [6500/7971]     || Average Training Loss: 3.0867\n",
      "Epoch: [4/20]          || Step: [6600/7971]     || Average Training Loss: 3.0868\n",
      "Epoch: [4/20]          || Step: [6700/7971]     || Average Training Loss: 3.0865\n",
      "Epoch: [4/20]          || Step: [6800/7971]     || Average Training Loss: 3.0865\n",
      "Epoch: [4/20]          || Step: [6900/7971]     || Average Training Loss: 3.0862\n",
      "Epoch: [4/20]          || Step: [7000/7971]     || Average Training Loss: 3.0865\n",
      "Epoch: [4/20]          || Step: [7100/7971]     || Average Training Loss: 3.0868\n",
      "Epoch: [4/20]          || Step: [7200/7971]     || Average Training Loss: 3.0869\n",
      "Epoch: [4/20]          || Step: [7300/7971]     || Average Training Loss: 3.0869\n",
      "Epoch: [4/20]          || Step: [7400/7971]     || Average Training Loss: 3.0866\n",
      "Epoch: [4/20]          || Step: [7500/7971]     || Average Training Loss: 3.0866\n",
      "Epoch: [4/20]          || Step: [7600/7971]     || Average Training Loss: 3.0862\n",
      "Epoch: [4/20]          || Step: [7700/7971]     || Average Training Loss: 3.0865\n",
      "Epoch: [4/20]          || Step: [7800/7971]     || Average Training Loss: 3.0864\n",
      "Epoch: [4/20]          || Step: [7900/7971]     || Average Training Loss: 3.0862\n",
      "Epoch: [4/20]          || Step: [0/715]         || Average Validation Loss: 3.1593\n",
      "Epoch: [4/20]          || Step: [100/715]       || Average Validation Loss: 3.0426\n",
      "Epoch: [4/20]          || Step: [200/715]       || Average Validation Loss: 3.0517\n",
      "Epoch: [4/20]          || Step: [300/715]       || Average Validation Loss: 3.0521\n",
      "Epoch: [4/20]          || Step: [400/715]       || Average Validation Loss: 3.0494\n",
      "Epoch: [4/20]          || Step: [500/715]       || Average Validation Loss: 3.0512\n",
      "Epoch: [4/20]          || Step: [600/715]       || Average Validation Loss: 3.0486\n",
      "Epoch: [4/20]          || Step: [700/715]       || Average Validation Loss: 3.0512\n",
      "****************************************************************************************************\n",
      "Epoch: [4/20] || Training Loss = 3.09 || Validation Loss: 3.05 || Time: 90.970179\n",
      "****************************************************************************************************\n",
      "Epoch: [5/20]          || Step: [0/7971]        || Average Training Loss: 3.2760\n",
      "Epoch: [5/20]          || Step: [100/7971]      || Average Training Loss: 3.0696\n",
      "Epoch: [5/20]          || Step: [200/7971]      || Average Training Loss: 3.0759\n",
      "Epoch: [5/20]          || Step: [300/7971]      || Average Training Loss: 3.0797\n",
      "Epoch: [5/20]          || Step: [400/7971]      || Average Training Loss: 3.0840\n",
      "Epoch: [5/20]          || Step: [500/7971]      || Average Training Loss: 3.0798\n",
      "Epoch: [5/20]          || Step: [600/7971]      || Average Training Loss: 3.0797\n",
      "Epoch: [5/20]          || Step: [700/7971]      || Average Training Loss: 3.0800\n",
      "Epoch: [5/20]          || Step: [800/7971]      || Average Training Loss: 3.0785\n",
      "Epoch: [5/20]          || Step: [900/7971]      || Average Training Loss: 3.0779\n",
      "Epoch: [5/20]          || Step: [1000/7971]     || Average Training Loss: 3.0805\n",
      "Epoch: [5/20]          || Step: [1100/7971]     || Average Training Loss: 3.0825\n",
      "Epoch: [5/20]          || Step: [1200/7971]     || Average Training Loss: 3.0842\n",
      "Epoch: [5/20]          || Step: [1300/7971]     || Average Training Loss: 3.0844\n",
      "Epoch: [5/20]          || Step: [1400/7971]     || Average Training Loss: 3.0852\n",
      "Epoch: [5/20]          || Step: [1500/7971]     || Average Training Loss: 3.0848\n",
      "Epoch: [5/20]          || Step: [1600/7971]     || Average Training Loss: 3.0852\n",
      "Epoch: [5/20]          || Step: [1700/7971]     || Average Training Loss: 3.0862\n",
      "Epoch: [5/20]          || Step: [1800/7971]     || Average Training Loss: 3.0862\n",
      "Epoch: [5/20]          || Step: [1900/7971]     || Average Training Loss: 3.0854\n",
      "Epoch: [5/20]          || Step: [2000/7971]     || Average Training Loss: 3.0845\n",
      "Epoch: [5/20]          || Step: [2100/7971]     || Average Training Loss: 3.0845\n",
      "Epoch: [5/20]          || Step: [2200/7971]     || Average Training Loss: 3.0849\n",
      "Epoch: [5/20]          || Step: [2300/7971]     || Average Training Loss: 3.0833\n",
      "Epoch: [5/20]          || Step: [2400/7971]     || Average Training Loss: 3.0840\n",
      "Epoch: [5/20]          || Step: [2500/7971]     || Average Training Loss: 3.0842\n",
      "Epoch: [5/20]          || Step: [2600/7971]     || Average Training Loss: 3.0838\n",
      "Epoch: [5/20]          || Step: [2700/7971]     || Average Training Loss: 3.0850\n",
      "Epoch: [5/20]          || Step: [2800/7971]     || Average Training Loss: 3.0845\n",
      "Epoch: [5/20]          || Step: [2900/7971]     || Average Training Loss: 3.0845\n",
      "Epoch: [5/20]          || Step: [3000/7971]     || Average Training Loss: 3.0837\n",
      "Epoch: [5/20]          || Step: [3100/7971]     || Average Training Loss: 3.0836\n",
      "Epoch: [5/20]          || Step: [3200/7971]     || Average Training Loss: 3.0832\n",
      "Epoch: [5/20]          || Step: [3300/7971]     || Average Training Loss: 3.0838\n",
      "Epoch: [5/20]          || Step: [3400/7971]     || Average Training Loss: 3.0832\n",
      "Epoch: [5/20]          || Step: [3500/7971]     || Average Training Loss: 3.0828\n",
      "Epoch: [5/20]          || Step: [3600/7971]     || Average Training Loss: 3.0828\n",
      "Epoch: [5/20]          || Step: [3700/7971]     || Average Training Loss: 3.0828\n",
      "Epoch: [5/20]          || Step: [3800/7971]     || Average Training Loss: 3.0834\n",
      "Epoch: [5/20]          || Step: [3900/7971]     || Average Training Loss: 3.0834\n",
      "Epoch: [5/20]          || Step: [4000/7971]     || Average Training Loss: 3.0831\n",
      "Epoch: [5/20]          || Step: [4100/7971]     || Average Training Loss: 3.0827\n",
      "Epoch: [5/20]          || Step: [4200/7971]     || Average Training Loss: 3.0826\n",
      "Epoch: [5/20]          || Step: [4300/7971]     || Average Training Loss: 3.0824\n",
      "Epoch: [5/20]          || Step: [4400/7971]     || Average Training Loss: 3.0823\n",
      "Epoch: [5/20]          || Step: [4500/7971]     || Average Training Loss: 3.0822\n",
      "Epoch: [5/20]          || Step: [4600/7971]     || Average Training Loss: 3.0821\n",
      "Epoch: [5/20]          || Step: [4700/7971]     || Average Training Loss: 3.0817\n",
      "Epoch: [5/20]          || Step: [4800/7971]     || Average Training Loss: 3.0817\n",
      "Epoch: [5/20]          || Step: [4900/7971]     || Average Training Loss: 3.0816\n",
      "Epoch: [5/20]          || Step: [5000/7971]     || Average Training Loss: 3.0819\n",
      "Epoch: [5/20]          || Step: [5100/7971]     || Average Training Loss: 3.0819\n",
      "Epoch: [5/20]          || Step: [5200/7971]     || Average Training Loss: 3.0818\n",
      "Epoch: [5/20]          || Step: [5300/7971]     || Average Training Loss: 3.0819\n",
      "Epoch: [5/20]          || Step: [5400/7971]     || Average Training Loss: 3.0819\n",
      "Epoch: [5/20]          || Step: [5500/7971]     || Average Training Loss: 3.0822\n",
      "Epoch: [5/20]          || Step: [5600/7971]     || Average Training Loss: 3.0825\n",
      "Epoch: [5/20]          || Step: [5700/7971]     || Average Training Loss: 3.0824\n",
      "Epoch: [5/20]          || Step: [5800/7971]     || Average Training Loss: 3.0821\n",
      "Epoch: [5/20]          || Step: [5900/7971]     || Average Training Loss: 3.0820\n",
      "Epoch: [5/20]          || Step: [6000/7971]     || Average Training Loss: 3.0820\n",
      "Epoch: [5/20]          || Step: [6100/7971]     || Average Training Loss: 3.0816\n",
      "Epoch: [5/20]          || Step: [6200/7971]     || Average Training Loss: 3.0815\n",
      "Epoch: [5/20]          || Step: [6300/7971]     || Average Training Loss: 3.0814\n",
      "Epoch: [5/20]          || Step: [6400/7971]     || Average Training Loss: 3.0812\n",
      "Epoch: [5/20]          || Step: [6500/7971]     || Average Training Loss: 3.0810\n",
      "Epoch: [5/20]          || Step: [6600/7971]     || Average Training Loss: 3.0809\n",
      "Epoch: [5/20]          || Step: [6700/7971]     || Average Training Loss: 3.0806\n",
      "Epoch: [5/20]          || Step: [6800/7971]     || Average Training Loss: 3.0807\n",
      "Epoch: [5/20]          || Step: [6900/7971]     || Average Training Loss: 3.0810\n",
      "Epoch: [5/20]          || Step: [7000/7971]     || Average Training Loss: 3.0808\n",
      "Epoch: [5/20]          || Step: [7100/7971]     || Average Training Loss: 3.0806\n",
      "Epoch: [5/20]          || Step: [7200/7971]     || Average Training Loss: 3.0806\n",
      "Epoch: [5/20]          || Step: [7300/7971]     || Average Training Loss: 3.0804\n",
      "Epoch: [5/20]          || Step: [7400/7971]     || Average Training Loss: 3.0802\n",
      "Epoch: [5/20]          || Step: [7500/7971]     || Average Training Loss: 3.0802\n",
      "Epoch: [5/20]          || Step: [7600/7971]     || Average Training Loss: 3.0804\n",
      "Epoch: [5/20]          || Step: [7700/7971]     || Average Training Loss: 3.0806\n",
      "Epoch: [5/20]          || Step: [7800/7971]     || Average Training Loss: 3.0806\n",
      "Epoch: [5/20]          || Step: [7900/7971]     || Average Training Loss: 3.0808\n",
      "Epoch: [5/20]          || Step: [0/715]         || Average Validation Loss: 3.0314\n",
      "Epoch: [5/20]          || Step: [100/715]       || Average Validation Loss: 3.0628\n",
      "Epoch: [5/20]          || Step: [200/715]       || Average Validation Loss: 3.0538\n",
      "Epoch: [5/20]          || Step: [300/715]       || Average Validation Loss: 3.0493\n",
      "Epoch: [5/20]          || Step: [400/715]       || Average Validation Loss: 3.0455\n",
      "Epoch: [5/20]          || Step: [500/715]       || Average Validation Loss: 3.0446\n",
      "Epoch: [5/20]          || Step: [600/715]       || Average Validation Loss: 3.0465\n",
      "Epoch: [5/20]          || Step: [700/715]       || Average Validation Loss: 3.0489\n",
      "****************************************************************************************************\n",
      "Epoch: [5/20] || Training Loss = 3.08 || Validation Loss: 3.05 || Time: 91.392191\n",
      "****************************************************************************************************\n",
      "Epoch: [6/20]          || Step: [0/7971]        || Average Training Loss: 2.9787\n",
      "Epoch: [6/20]          || Step: [100/7971]      || Average Training Loss: 3.0654\n",
      "Epoch: [6/20]          || Step: [200/7971]      || Average Training Loss: 3.0722\n",
      "Epoch: [6/20]          || Step: [300/7971]      || Average Training Loss: 3.0727\n",
      "Epoch: [6/20]          || Step: [400/7971]      || Average Training Loss: 3.0697\n",
      "Epoch: [6/20]          || Step: [500/7971]      || Average Training Loss: 3.0728\n",
      "Epoch: [6/20]          || Step: [600/7971]      || Average Training Loss: 3.0732\n",
      "Epoch: [6/20]          || Step: [700/7971]      || Average Training Loss: 3.0709\n",
      "Epoch: [6/20]          || Step: [800/7971]      || Average Training Loss: 3.0708\n",
      "Epoch: [6/20]          || Step: [900/7971]      || Average Training Loss: 3.0727\n",
      "Epoch: [6/20]          || Step: [1000/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [6/20]          || Step: [1100/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [1200/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [6/20]          || Step: [1300/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [6/20]          || Step: [1400/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [6/20]          || Step: [1500/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [6/20]          || Step: [1600/7971]     || Average Training Loss: 3.0754\n",
      "Epoch: [6/20]          || Step: [1700/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [1800/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [1900/7971]     || Average Training Loss: 3.0742\n",
      "Epoch: [6/20]          || Step: [2000/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [6/20]          || Step: [2100/7971]     || Average Training Loss: 3.0756\n",
      "Epoch: [6/20]          || Step: [2200/7971]     || Average Training Loss: 3.0755\n",
      "Epoch: [6/20]          || Step: [2300/7971]     || Average Training Loss: 3.0755\n",
      "Epoch: [6/20]          || Step: [2400/7971]     || Average Training Loss: 3.0752\n",
      "Epoch: [6/20]          || Step: [2500/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [6/20]          || Step: [2600/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [2700/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [2800/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [6/20]          || Step: [2900/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [6/20]          || Step: [3000/7971]     || Average Training Loss: 3.0740\n",
      "Epoch: [6/20]          || Step: [3100/7971]     || Average Training Loss: 3.0736\n",
      "Epoch: [6/20]          || Step: [3200/7971]     || Average Training Loss: 3.0736\n",
      "Epoch: [6/20]          || Step: [3300/7971]     || Average Training Loss: 3.0733\n",
      "Epoch: [6/20]          || Step: [3400/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [6/20]          || Step: [3500/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [6/20]          || Step: [3600/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [6/20]          || Step: [3700/7971]     || Average Training Loss: 3.0743\n",
      "Epoch: [6/20]          || Step: [3800/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [3900/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [6/20]          || Step: [4000/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [6/20]          || Step: [4100/7971]     || Average Training Loss: 3.0750\n",
      "Epoch: [6/20]          || Step: [4200/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [6/20]          || Step: [4300/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [6/20]          || Step: [4400/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [6/20]          || Step: [4500/7971]     || Average Training Loss: 3.0750\n",
      "Epoch: [6/20]          || Step: [4600/7971]     || Average Training Loss: 3.0752\n",
      "Epoch: [6/20]          || Step: [4700/7971]     || Average Training Loss: 3.0757\n",
      "Epoch: [6/20]          || Step: [4800/7971]     || Average Training Loss: 3.0758\n",
      "Epoch: [6/20]          || Step: [4900/7971]     || Average Training Loss: 3.0759\n",
      "Epoch: [6/20]          || Step: [5000/7971]     || Average Training Loss: 3.0759\n",
      "Epoch: [6/20]          || Step: [5100/7971]     || Average Training Loss: 3.0758\n",
      "Epoch: [6/20]          || Step: [5200/7971]     || Average Training Loss: 3.0759\n",
      "Epoch: [6/20]          || Step: [5300/7971]     || Average Training Loss: 3.0765\n",
      "Epoch: [6/20]          || Step: [5400/7971]     || Average Training Loss: 3.0767\n",
      "Epoch: [6/20]          || Step: [5500/7971]     || Average Training Loss: 3.0761\n",
      "Epoch: [6/20]          || Step: [5600/7971]     || Average Training Loss: 3.0765\n",
      "Epoch: [6/20]          || Step: [5700/7971]     || Average Training Loss: 3.0767\n",
      "Epoch: [6/20]          || Step: [5800/7971]     || Average Training Loss: 3.0768\n",
      "Epoch: [6/20]          || Step: [5900/7971]     || Average Training Loss: 3.0767\n",
      "Epoch: [6/20]          || Step: [6000/7971]     || Average Training Loss: 3.0768\n",
      "Epoch: [6/20]          || Step: [6100/7971]     || Average Training Loss: 3.0768\n",
      "Epoch: [6/20]          || Step: [6200/7971]     || Average Training Loss: 3.0769\n",
      "Epoch: [6/20]          || Step: [6300/7971]     || Average Training Loss: 3.0770\n",
      "Epoch: [6/20]          || Step: [6400/7971]     || Average Training Loss: 3.0767\n",
      "Epoch: [6/20]          || Step: [6500/7971]     || Average Training Loss: 3.0766\n",
      "Epoch: [6/20]          || Step: [6600/7971]     || Average Training Loss: 3.0770\n",
      "Epoch: [6/20]          || Step: [6700/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [6/20]          || Step: [6800/7971]     || Average Training Loss: 3.0774\n",
      "Epoch: [6/20]          || Step: [6900/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [6/20]          || Step: [7000/7971]     || Average Training Loss: 3.0774\n",
      "Epoch: [6/20]          || Step: [7100/7971]     || Average Training Loss: 3.0775\n",
      "Epoch: [6/20]          || Step: [7200/7971]     || Average Training Loss: 3.0771\n",
      "Epoch: [6/20]          || Step: [7300/7971]     || Average Training Loss: 3.0772\n",
      "Epoch: [6/20]          || Step: [7400/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [6/20]          || Step: [7500/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [6/20]          || Step: [7600/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [6/20]          || Step: [7700/7971]     || Average Training Loss: 3.0774\n",
      "Epoch: [6/20]          || Step: [7800/7971]     || Average Training Loss: 3.0770\n",
      "Epoch: [6/20]          || Step: [7900/7971]     || Average Training Loss: 3.0768\n",
      "Epoch: [6/20]          || Step: [0/715]         || Average Validation Loss: 3.2806\n",
      "Epoch: [6/20]          || Step: [100/715]       || Average Validation Loss: 3.0912\n",
      "Epoch: [6/20]          || Step: [200/715]       || Average Validation Loss: 3.0765\n",
      "Epoch: [6/20]          || Step: [300/715]       || Average Validation Loss: 3.0771\n",
      "Epoch: [6/20]          || Step: [400/715]       || Average Validation Loss: 3.0719\n",
      "Epoch: [6/20]          || Step: [500/715]       || Average Validation Loss: 3.0736\n",
      "Epoch: [6/20]          || Step: [600/715]       || Average Validation Loss: 3.0723\n",
      "Epoch: [6/20]          || Step: [700/715]       || Average Validation Loss: 3.0755\n",
      "****************************************************************************************************\n",
      "Epoch: [6/20] || Training Loss = 3.08 || Validation Loss: 3.08 || Time: 95.277626\n",
      "****************************************************************************************************\n",
      "Epoch: [7/20]          || Step: [0/7971]        || Average Training Loss: 2.9136\n",
      "Epoch: [7/20]          || Step: [100/7971]      || Average Training Loss: 3.0826\n",
      "Epoch: [7/20]          || Step: [200/7971]      || Average Training Loss: 3.0939\n",
      "Epoch: [7/20]          || Step: [300/7971]      || Average Training Loss: 3.0834\n",
      "Epoch: [7/20]          || Step: [400/7971]      || Average Training Loss: 3.0812\n",
      "Epoch: [7/20]          || Step: [500/7971]      || Average Training Loss: 3.0806\n",
      "Epoch: [7/20]          || Step: [600/7971]      || Average Training Loss: 3.0758\n",
      "Epoch: [7/20]          || Step: [700/7971]      || Average Training Loss: 3.0750\n",
      "Epoch: [7/20]          || Step: [800/7971]      || Average Training Loss: 3.0719\n",
      "Epoch: [7/20]          || Step: [900/7971]      || Average Training Loss: 3.0729\n",
      "Epoch: [7/20]          || Step: [1000/7971]     || Average Training Loss: 3.0742\n",
      "Epoch: [7/20]          || Step: [1100/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [7/20]          || Step: [1200/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [1300/7971]     || Average Training Loss: 3.0761\n",
      "Epoch: [7/20]          || Step: [1400/7971]     || Average Training Loss: 3.0760\n",
      "Epoch: [7/20]          || Step: [1500/7971]     || Average Training Loss: 3.0764\n",
      "Epoch: [7/20]          || Step: [1600/7971]     || Average Training Loss: 3.0763\n",
      "Epoch: [7/20]          || Step: [1700/7971]     || Average Training Loss: 3.0757\n",
      "Epoch: [7/20]          || Step: [1800/7971]     || Average Training Loss: 3.0756\n",
      "Epoch: [7/20]          || Step: [1900/7971]     || Average Training Loss: 3.0760\n",
      "Epoch: [7/20]          || Step: [2000/7971]     || Average Training Loss: 3.0757\n",
      "Epoch: [7/20]          || Step: [2100/7971]     || Average Training Loss: 3.0768\n",
      "Epoch: [7/20]          || Step: [2200/7971]     || Average Training Loss: 3.0769\n",
      "Epoch: [7/20]          || Step: [2300/7971]     || Average Training Loss: 3.0765\n",
      "Epoch: [7/20]          || Step: [2400/7971]     || Average Training Loss: 3.0775\n",
      "Epoch: [7/20]          || Step: [2500/7971]     || Average Training Loss: 3.0770\n",
      "Epoch: [7/20]          || Step: [2600/7971]     || Average Training Loss: 3.0766\n",
      "Epoch: [7/20]          || Step: [2700/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [7/20]          || Step: [2800/7971]     || Average Training Loss: 3.0750\n",
      "Epoch: [7/20]          || Step: [2900/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [3000/7971]     || Average Training Loss: 3.0752\n",
      "Epoch: [7/20]          || Step: [3100/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [7/20]          || Step: [3200/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [7/20]          || Step: [3300/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [3400/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [3500/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [3600/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [7/20]          || Step: [3700/7971]     || Average Training Loss: 3.0742\n",
      "Epoch: [7/20]          || Step: [3800/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [7/20]          || Step: [3900/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [7/20]          || Step: [4000/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [7/20]          || Step: [4100/7971]     || Average Training Loss: 3.0758\n",
      "Epoch: [7/20]          || Step: [4200/7971]     || Average Training Loss: 3.0758\n",
      "Epoch: [7/20]          || Step: [4300/7971]     || Average Training Loss: 3.0754\n",
      "Epoch: [7/20]          || Step: [4400/7971]     || Average Training Loss: 3.0754\n",
      "Epoch: [7/20]          || Step: [4500/7971]     || Average Training Loss: 3.0755\n",
      "Epoch: [7/20]          || Step: [4600/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [7/20]          || Step: [4700/7971]     || Average Training Loss: 3.0749\n",
      "Epoch: [7/20]          || Step: [4800/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [7/20]          || Step: [4900/7971]     || Average Training Loss: 3.0743\n",
      "Epoch: [7/20]          || Step: [5000/7971]     || Average Training Loss: 3.0743\n",
      "Epoch: [7/20]          || Step: [5100/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [7/20]          || Step: [5200/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [7/20]          || Step: [5300/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [5400/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [5500/7971]     || Average Training Loss: 3.0742\n",
      "Epoch: [7/20]          || Step: [5600/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [7/20]          || Step: [5700/7971]     || Average Training Loss: 3.0738\n",
      "Epoch: [7/20]          || Step: [5800/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [7/20]          || Step: [5900/7971]     || Average Training Loss: 3.0738\n",
      "Epoch: [7/20]          || Step: [6000/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [7/20]          || Step: [6100/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [6200/7971]     || Average Training Loss: 3.0743\n",
      "Epoch: [7/20]          || Step: [6300/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [6400/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [6500/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [6600/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [6700/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [6800/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [7/20]          || Step: [6900/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [7/20]          || Step: [7000/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [7/20]          || Step: [7100/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [7/20]          || Step: [7200/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [7/20]          || Step: [7300/7971]     || Average Training Loss: 3.0752\n",
      "Epoch: [7/20]          || Step: [7400/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [7/20]          || Step: [7500/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [7/20]          || Step: [7600/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [7/20]          || Step: [7700/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [7/20]          || Step: [7800/7971]     || Average Training Loss: 3.0749\n",
      "Epoch: [7/20]          || Step: [7900/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [0/715]         || Average Validation Loss: 3.0255\n",
      "Epoch: [7/20]          || Step: [100/715]       || Average Validation Loss: 3.0376\n",
      "Epoch: [7/20]          || Step: [200/715]       || Average Validation Loss: 3.0485\n",
      "Epoch: [7/20]          || Step: [300/715]       || Average Validation Loss: 3.0479\n",
      "Epoch: [7/20]          || Step: [400/715]       || Average Validation Loss: 3.0450\n",
      "Epoch: [7/20]          || Step: [500/715]       || Average Validation Loss: 3.0427\n",
      "Epoch: [7/20]          || Step: [600/715]       || Average Validation Loss: 3.0408\n",
      "Epoch: [7/20]          || Step: [700/715]       || Average Validation Loss: 3.0442\n",
      "****************************************************************************************************\n",
      "Epoch: [7/20] || Training Loss = 3.07 || Validation Loss: 3.04 || Time: 108.780173\n",
      "****************************************************************************************************\n",
      "Epoch: [8/20]          || Step: [0/7971]        || Average Training Loss: 2.9852\n",
      "Epoch: [8/20]          || Step: [100/7971]      || Average Training Loss: 3.0566\n",
      "Epoch: [8/20]          || Step: [200/7971]      || Average Training Loss: 3.0710\n",
      "Epoch: [8/20]          || Step: [300/7971]      || Average Training Loss: 3.0735\n",
      "Epoch: [8/20]          || Step: [400/7971]      || Average Training Loss: 3.0773\n",
      "Epoch: [8/20]          || Step: [500/7971]      || Average Training Loss: 3.0796\n",
      "Epoch: [8/20]          || Step: [600/7971]      || Average Training Loss: 3.0808\n",
      "Epoch: [8/20]          || Step: [700/7971]      || Average Training Loss: 3.0787\n",
      "Epoch: [8/20]          || Step: [800/7971]      || Average Training Loss: 3.0776\n",
      "Epoch: [8/20]          || Step: [900/7971]      || Average Training Loss: 3.0797\n",
      "Epoch: [8/20]          || Step: [1000/7971]     || Average Training Loss: 3.0802\n",
      "Epoch: [8/20]          || Step: [1100/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [8/20]          || Step: [1200/7971]     || Average Training Loss: 3.0760\n",
      "Epoch: [8/20]          || Step: [1300/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [8/20]          || Step: [1400/7971]     || Average Training Loss: 3.0759\n",
      "Epoch: [8/20]          || Step: [1500/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [8/20]          || Step: [1600/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [8/20]          || Step: [1700/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [8/20]          || Step: [1800/7971]     || Average Training Loss: 3.0729\n",
      "Epoch: [8/20]          || Step: [1900/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [2000/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [2100/7971]     || Average Training Loss: 3.0723\n",
      "Epoch: [8/20]          || Step: [2200/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [8/20]          || Step: [2300/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [8/20]          || Step: [2400/7971]     || Average Training Loss: 3.0737\n",
      "Epoch: [8/20]          || Step: [2500/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [8/20]          || Step: [2600/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [2700/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [8/20]          || Step: [2800/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [2900/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [8/20]          || Step: [3000/7971]     || Average Training Loss: 3.0738\n",
      "Epoch: [8/20]          || Step: [3100/7971]     || Average Training Loss: 3.0731\n",
      "Epoch: [8/20]          || Step: [3200/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [8/20]          || Step: [3300/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [3400/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [8/20]          || Step: [3500/7971]     || Average Training Loss: 3.0731\n",
      "Epoch: [8/20]          || Step: [3600/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [8/20]          || Step: [3700/7971]     || Average Training Loss: 3.0729\n",
      "Epoch: [8/20]          || Step: [3800/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [8/20]          || Step: [3900/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [8/20]          || Step: [4000/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [4100/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [4200/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [8/20]          || Step: [4300/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [8/20]          || Step: [4400/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [8/20]          || Step: [4500/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [8/20]          || Step: [4600/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [8/20]          || Step: [4700/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [8/20]          || Step: [4800/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [8/20]          || Step: [4900/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [8/20]          || Step: [5000/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [8/20]          || Step: [5100/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [5200/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [5300/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [5400/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [5500/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [5600/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [5700/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [5800/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [8/20]          || Step: [5900/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [8/20]          || Step: [6000/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [6100/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [6200/7971]     || Average Training Loss: 3.0731\n",
      "Epoch: [8/20]          || Step: [6300/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [6400/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [6500/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [6600/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [6700/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [6800/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [6900/7971]     || Average Training Loss: 3.0729\n",
      "Epoch: [8/20]          || Step: [7000/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [7100/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [7200/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [7300/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [7400/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [8/20]          || Step: [7500/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [7600/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [8/20]          || Step: [7700/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [7800/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [8/20]          || Step: [7900/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [8/20]          || Step: [0/715]         || Average Validation Loss: 3.2538\n",
      "Epoch: [8/20]          || Step: [100/715]       || Average Validation Loss: 3.0620\n",
      "Epoch: [8/20]          || Step: [200/715]       || Average Validation Loss: 3.0501\n",
      "Epoch: [8/20]          || Step: [300/715]       || Average Validation Loss: 3.0486\n",
      "Epoch: [8/20]          || Step: [400/715]       || Average Validation Loss: 3.0534\n",
      "Epoch: [8/20]          || Step: [500/715]       || Average Validation Loss: 3.0514\n",
      "Epoch: [8/20]          || Step: [600/715]       || Average Validation Loss: 3.0504\n",
      "Epoch: [8/20]          || Step: [700/715]       || Average Validation Loss: 3.0502\n",
      "****************************************************************************************************\n",
      "Epoch: [8/20] || Training Loss = 3.07 || Validation Loss: 3.05 || Time: 109.411215\n",
      "****************************************************************************************************\n",
      "Epoch: [9/20]          || Step: [0/7971]        || Average Training Loss: 3.2771\n",
      "Epoch: [9/20]          || Step: [100/7971]      || Average Training Loss: 3.0825\n",
      "Epoch: [9/20]          || Step: [200/7971]      || Average Training Loss: 3.0783\n",
      "Epoch: [9/20]          || Step: [300/7971]      || Average Training Loss: 3.0700\n",
      "Epoch: [9/20]          || Step: [400/7971]      || Average Training Loss: 3.0700\n",
      "Epoch: [9/20]          || Step: [500/7971]      || Average Training Loss: 3.0662\n",
      "Epoch: [9/20]          || Step: [600/7971]      || Average Training Loss: 3.0673\n",
      "Epoch: [9/20]          || Step: [700/7971]      || Average Training Loss: 3.0653\n",
      "Epoch: [9/20]          || Step: [800/7971]      || Average Training Loss: 3.0671\n",
      "Epoch: [9/20]          || Step: [900/7971]      || Average Training Loss: 3.0685\n",
      "Epoch: [9/20]          || Step: [1000/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [9/20]          || Step: [1100/7971]     || Average Training Loss: 3.0708\n",
      "Epoch: [9/20]          || Step: [1200/7971]     || Average Training Loss: 3.0704\n",
      "Epoch: [9/20]          || Step: [1300/7971]     || Average Training Loss: 3.0708\n",
      "Epoch: [9/20]          || Step: [1400/7971]     || Average Training Loss: 3.0704\n",
      "Epoch: [9/20]          || Step: [1500/7971]     || Average Training Loss: 3.0704\n",
      "Epoch: [9/20]          || Step: [1600/7971]     || Average Training Loss: 3.0699\n",
      "Epoch: [9/20]          || Step: [1700/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [1800/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [9/20]          || Step: [1900/7971]     || Average Training Loss: 3.0706\n",
      "Epoch: [9/20]          || Step: [2000/7971]     || Average Training Loss: 3.0699\n",
      "Epoch: [9/20]          || Step: [2100/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [9/20]          || Step: [2200/7971]     || Average Training Loss: 3.0699\n",
      "Epoch: [9/20]          || Step: [2300/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [9/20]          || Step: [2400/7971]     || Average Training Loss: 3.0705\n",
      "Epoch: [9/20]          || Step: [2500/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [9/20]          || Step: [2600/7971]     || Average Training Loss: 3.0717\n",
      "Epoch: [9/20]          || Step: [2700/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [9/20]          || Step: [2800/7971]     || Average Training Loss: 3.0710\n",
      "Epoch: [9/20]          || Step: [2900/7971]     || Average Training Loss: 3.0707\n",
      "Epoch: [9/20]          || Step: [3000/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [9/20]          || Step: [3100/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [3200/7971]     || Average Training Loss: 3.0717\n",
      "Epoch: [9/20]          || Step: [3300/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [9/20]          || Step: [3400/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [9/20]          || Step: [3500/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [3600/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [9/20]          || Step: [3700/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [3800/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [9/20]          || Step: [3900/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [4000/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [9/20]          || Step: [4100/7971]     || Average Training Loss: 3.0717\n",
      "Epoch: [9/20]          || Step: [4200/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [9/20]          || Step: [4300/7971]     || Average Training Loss: 3.0710\n",
      "Epoch: [9/20]          || Step: [4400/7971]     || Average Training Loss: 3.0712\n",
      "Epoch: [9/20]          || Step: [4500/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [9/20]          || Step: [4600/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [4700/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [9/20]          || Step: [4800/7971]     || Average Training Loss: 3.0712\n",
      "Epoch: [9/20]          || Step: [4900/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [9/20]          || Step: [5000/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [5100/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [9/20]          || Step: [5200/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [9/20]          || Step: [5300/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [5400/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [5500/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [9/20]          || Step: [5600/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [9/20]          || Step: [5700/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [9/20]          || Step: [5800/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [9/20]          || Step: [5900/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [9/20]          || Step: [6000/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [9/20]          || Step: [6100/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [9/20]          || Step: [6200/7971]     || Average Training Loss: 3.0723\n",
      "Epoch: [9/20]          || Step: [6300/7971]     || Average Training Loss: 3.0723\n",
      "Epoch: [9/20]          || Step: [6400/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [9/20]          || Step: [6500/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [9/20]          || Step: [6600/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [9/20]          || Step: [6700/7971]     || Average Training Loss: 3.0719\n",
      "Epoch: [9/20]          || Step: [6800/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [9/20]          || Step: [6900/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [9/20]          || Step: [7000/7971]     || Average Training Loss: 3.0723\n",
      "Epoch: [9/20]          || Step: [7100/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [9/20]          || Step: [7200/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [9/20]          || Step: [7300/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [9/20]          || Step: [7400/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [9/20]          || Step: [7500/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [7600/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [9/20]          || Step: [7700/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [7800/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [7900/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [0/715]         || Average Validation Loss: 2.9421\n",
      "Epoch: [9/20]          || Step: [100/715]       || Average Validation Loss: 3.0147\n",
      "Epoch: [9/20]          || Step: [200/715]       || Average Validation Loss: 3.0339\n",
      "Epoch: [9/20]          || Step: [300/715]       || Average Validation Loss: 3.0329\n",
      "Epoch: [9/20]          || Step: [400/715]       || Average Validation Loss: 3.0371\n",
      "Epoch: [9/20]          || Step: [500/715]       || Average Validation Loss: 3.0357\n",
      "Epoch: [9/20]          || Step: [600/715]       || Average Validation Loss: 3.0338\n",
      "Epoch: [9/20]          || Step: [700/715]       || Average Validation Loss: 3.0360\n",
      "****************************************************************************************************\n",
      "Epoch: [9/20] || Training Loss = 3.07 || Validation Loss: 3.04 || Time: 108.309496\n",
      "****************************************************************************************************\n",
      "Epoch: [10/20]         || Step: [0/7971]        || Average Training Loss: 3.1072\n",
      "Epoch: [10/20]         || Step: [100/7971]      || Average Training Loss: 3.0533\n",
      "Epoch: [10/20]         || Step: [200/7971]      || Average Training Loss: 3.0620\n",
      "Epoch: [10/20]         || Step: [300/7971]      || Average Training Loss: 3.0618\n",
      "Epoch: [10/20]         || Step: [400/7971]      || Average Training Loss: 3.0600\n",
      "Epoch: [10/20]         || Step: [500/7971]      || Average Training Loss: 3.0657\n",
      "Epoch: [10/20]         || Step: [600/7971]      || Average Training Loss: 3.0681\n",
      "Epoch: [10/20]         || Step: [700/7971]      || Average Training Loss: 3.0650\n",
      "Epoch: [10/20]         || Step: [800/7971]      || Average Training Loss: 3.0645\n",
      "Epoch: [10/20]         || Step: [900/7971]      || Average Training Loss: 3.0670\n",
      "Epoch: [10/20]         || Step: [1000/7971]     || Average Training Loss: 3.0677\n",
      "Epoch: [10/20]         || Step: [1100/7971]     || Average Training Loss: 3.0675\n",
      "Epoch: [10/20]         || Step: [1200/7971]     || Average Training Loss: 3.0656\n",
      "Epoch: [10/20]         || Step: [1300/7971]     || Average Training Loss: 3.0655\n",
      "Epoch: [10/20]         || Step: [1400/7971]     || Average Training Loss: 3.0674\n",
      "Epoch: [10/20]         || Step: [1500/7971]     || Average Training Loss: 3.0675\n",
      "Epoch: [10/20]         || Step: [1600/7971]     || Average Training Loss: 3.0681\n",
      "Epoch: [10/20]         || Step: [1700/7971]     || Average Training Loss: 3.0691\n",
      "Epoch: [10/20]         || Step: [1800/7971]     || Average Training Loss: 3.0701\n",
      "Epoch: [10/20]         || Step: [1900/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [10/20]         || Step: [2000/7971]     || Average Training Loss: 3.0708\n",
      "Epoch: [10/20]         || Step: [2100/7971]     || Average Training Loss: 3.0702\n",
      "Epoch: [10/20]         || Step: [2200/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [10/20]         || Step: [2300/7971]     || Average Training Loss: 3.0716\n",
      "Epoch: [10/20]         || Step: [2400/7971]     || Average Training Loss: 3.0707\n",
      "Epoch: [10/20]         || Step: [2500/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [10/20]         || Step: [2600/7971]     || Average Training Loss: 3.0710\n",
      "Epoch: [10/20]         || Step: [2700/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [10/20]         || Step: [2800/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [10/20]         || Step: [2900/7971]     || Average Training Loss: 3.0723\n",
      "Epoch: [10/20]         || Step: [3000/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [10/20]         || Step: [3100/7971]     || Average Training Loss: 3.0740\n",
      "Epoch: [10/20]         || Step: [3200/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [10/20]         || Step: [3300/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [10/20]         || Step: [3400/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [10/20]         || Step: [3500/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [10/20]         || Step: [3600/7971]     || Average Training Loss: 3.0736\n",
      "Epoch: [10/20]         || Step: [3700/7971]     || Average Training Loss: 3.0738\n",
      "Epoch: [10/20]         || Step: [3800/7971]     || Average Training Loss: 3.0737\n",
      "Epoch: [10/20]         || Step: [3900/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [10/20]         || Step: [4000/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [10/20]         || Step: [4100/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [10/20]         || Step: [4200/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [10/20]         || Step: [4300/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [10/20]         || Step: [4400/7971]     || Average Training Loss: 3.0729\n",
      "Epoch: [10/20]         || Step: [4500/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [10/20]         || Step: [4600/7971]     || Average Training Loss: 3.0729\n",
      "Epoch: [10/20]         || Step: [4700/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [10/20]         || Step: [4800/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [10/20]         || Step: [4900/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [10/20]         || Step: [5000/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [10/20]         || Step: [5100/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [10/20]         || Step: [5200/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [10/20]         || Step: [5300/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [10/20]         || Step: [5400/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [10/20]         || Step: [5500/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [10/20]         || Step: [5600/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [10/20]         || Step: [5700/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [10/20]         || Step: [5800/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [5900/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [6000/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [6100/7971]     || Average Training Loss: 3.0716\n",
      "Epoch: [10/20]         || Step: [6200/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [6300/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [10/20]         || Step: [6400/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [10/20]         || Step: [6500/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [10/20]         || Step: [6600/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [6700/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [10/20]         || Step: [6800/7971]     || Average Training Loss: 3.0710\n",
      "Epoch: [10/20]         || Step: [6900/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [10/20]         || Step: [7000/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [10/20]         || Step: [7100/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [10/20]         || Step: [7200/7971]     || Average Training Loss: 3.0712\n",
      "Epoch: [10/20]         || Step: [7300/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [10/20]         || Step: [7400/7971]     || Average Training Loss: 3.0712\n",
      "Epoch: [10/20]         || Step: [7500/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [10/20]         || Step: [7600/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [7700/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [10/20]         || Step: [7800/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [7900/7971]     || Average Training Loss: 3.0712\n",
      "Epoch: [10/20]         || Step: [0/715]         || Average Validation Loss: 3.2017\n",
      "Epoch: [10/20]         || Step: [100/715]       || Average Validation Loss: 3.0443\n",
      "Epoch: [10/20]         || Step: [200/715]       || Average Validation Loss: 3.0362\n",
      "Epoch: [10/20]         || Step: [300/715]       || Average Validation Loss: 3.0486\n",
      "Epoch: [10/20]         || Step: [400/715]       || Average Validation Loss: 3.0585\n",
      "Epoch: [10/20]         || Step: [500/715]       || Average Validation Loss: 3.0546\n",
      "Epoch: [10/20]         || Step: [600/715]       || Average Validation Loss: 3.0515\n",
      "Epoch: [10/20]         || Step: [700/715]       || Average Validation Loss: 3.0534\n",
      "****************************************************************************************************\n",
      "Epoch: [10/20] || Training Loss = 3.07 || Validation Loss: 3.05 || Time: 133.247122\n",
      "****************************************************************************************************\n",
      "Epoch: [11/20]         || Step: [0/7971]        || Average Training Loss: 3.0784\n",
      "Epoch: [11/20]         || Step: [100/7971]      || Average Training Loss: 3.0728\n",
      "Epoch: [11/20]         || Step: [200/7971]      || Average Training Loss: 3.0702\n",
      "Epoch: [11/20]         || Step: [300/7971]      || Average Training Loss: 3.0680\n",
      "Epoch: [11/20]         || Step: [400/7971]      || Average Training Loss: 3.0716\n",
      "Epoch: [11/20]         || Step: [500/7971]      || Average Training Loss: 3.0733\n",
      "Epoch: [11/20]         || Step: [600/7971]      || Average Training Loss: 3.0706\n",
      "Epoch: [11/20]         || Step: [700/7971]      || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [800/7971]      || Average Training Loss: 3.0721\n",
      "Epoch: [11/20]         || Step: [900/7971]      || Average Training Loss: 3.0723\n",
      "Epoch: [11/20]         || Step: [1000/7971]     || Average Training Loss: 3.0719\n",
      "Epoch: [11/20]         || Step: [1100/7971]     || Average Training Loss: 3.0731\n",
      "Epoch: [11/20]         || Step: [1200/7971]     || Average Training Loss: 3.0740\n",
      "Epoch: [11/20]         || Step: [1300/7971]     || Average Training Loss: 3.0749\n",
      "Epoch: [11/20]         || Step: [1400/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [11/20]         || Step: [1500/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [11/20]         || Step: [1600/7971]     || Average Training Loss: 3.0742\n",
      "Epoch: [11/20]         || Step: [1700/7971]     || Average Training Loss: 3.0762\n",
      "Epoch: [11/20]         || Step: [1800/7971]     || Average Training Loss: 3.0754\n",
      "Epoch: [11/20]         || Step: [1900/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [11/20]         || Step: [2000/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [11/20]         || Step: [2100/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [11/20]         || Step: [2200/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [11/20]         || Step: [2300/7971]     || Average Training Loss: 3.0738\n",
      "Epoch: [11/20]         || Step: [2400/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [11/20]         || Step: [2500/7971]     || Average Training Loss: 3.0736\n",
      "Epoch: [11/20]         || Step: [2600/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [11/20]         || Step: [2700/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [11/20]         || Step: [2800/7971]     || Average Training Loss: 3.0717\n",
      "Epoch: [11/20]         || Step: [2900/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [11/20]         || Step: [3000/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [11/20]         || Step: [3100/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [11/20]         || Step: [3200/7971]     || Average Training Loss: 3.0719\n",
      "Epoch: [11/20]         || Step: [3300/7971]     || Average Training Loss: 3.0716\n",
      "Epoch: [11/20]         || Step: [3400/7971]     || Average Training Loss: 3.0719\n",
      "Epoch: [11/20]         || Step: [3500/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [11/20]         || Step: [3600/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [11/20]         || Step: [3700/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [11/20]         || Step: [3800/7971]     || Average Training Loss: 3.0719\n",
      "Epoch: [11/20]         || Step: [3900/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [11/20]         || Step: [4000/7971]     || Average Training Loss: 3.0717\n",
      "Epoch: [11/20]         || Step: [4100/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [11/20]         || Step: [4200/7971]     || Average Training Loss: 3.0708\n",
      "Epoch: [11/20]         || Step: [4300/7971]     || Average Training Loss: 3.0707\n",
      "Epoch: [11/20]         || Step: [4400/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [4500/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [11/20]         || Step: [4600/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [4700/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [4800/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [11/20]         || Step: [4900/7971]     || Average Training Loss: 3.0688\n",
      "Epoch: [11/20]         || Step: [5000/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [11/20]         || Step: [5100/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [11/20]         || Step: [5200/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [11/20]         || Step: [5300/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [11/20]         || Step: [5400/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [11/20]         || Step: [5500/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [11/20]         || Step: [5600/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [11/20]         || Step: [5700/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [11/20]         || Step: [5800/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [11/20]         || Step: [5900/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [11/20]         || Step: [6000/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [11/20]         || Step: [6100/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [11/20]         || Step: [6200/7971]     || Average Training Loss: 3.0691\n",
      "Epoch: [11/20]         || Step: [6300/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [11/20]         || Step: [6400/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [11/20]         || Step: [6500/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [11/20]         || Step: [6600/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [11/20]         || Step: [6700/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [11/20]         || Step: [6800/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [11/20]         || Step: [6900/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [11/20]         || Step: [7000/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [11/20]         || Step: [7100/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [11/20]         || Step: [7200/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [7300/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [7400/7971]     || Average Training Loss: 3.0701\n",
      "Epoch: [11/20]         || Step: [7500/7971]     || Average Training Loss: 3.0702\n",
      "Epoch: [11/20]         || Step: [7600/7971]     || Average Training Loss: 3.0702\n",
      "Epoch: [11/20]         || Step: [7700/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [11/20]         || Step: [7800/7971]     || Average Training Loss: 3.0706\n",
      "Epoch: [11/20]         || Step: [7900/7971]     || Average Training Loss: 3.0707\n",
      "Epoch: [11/20]         || Step: [0/715]         || Average Validation Loss: 3.3300\n",
      "Epoch: [11/20]         || Step: [100/715]       || Average Validation Loss: 3.0574\n",
      "Epoch: [11/20]         || Step: [200/715]       || Average Validation Loss: 3.0520\n",
      "Epoch: [11/20]         || Step: [300/715]       || Average Validation Loss: 3.0568\n",
      "Epoch: [11/20]         || Step: [400/715]       || Average Validation Loss: 3.0532\n",
      "Epoch: [11/20]         || Step: [500/715]       || Average Validation Loss: 3.0504\n",
      "Epoch: [11/20]         || Step: [600/715]       || Average Validation Loss: 3.0484\n",
      "Epoch: [11/20]         || Step: [700/715]       || Average Validation Loss: 3.0494\n",
      "****************************************************************************************************\n",
      "Epoch: [11/20] || Training Loss = 3.07 || Validation Loss: 3.05 || Time: 97.178168\n",
      "****************************************************************************************************\n",
      "Epoch: [12/20]         || Step: [0/7971]        || Average Training Loss: 3.1393\n",
      "Epoch: [12/20]         || Step: [100/7971]      || Average Training Loss: 3.0681\n",
      "Epoch: [12/20]         || Step: [200/7971]      || Average Training Loss: 3.0665\n",
      "Epoch: [12/20]         || Step: [300/7971]      || Average Training Loss: 3.0712\n",
      "Epoch: [12/20]         || Step: [400/7971]      || Average Training Loss: 3.0721\n",
      "Epoch: [12/20]         || Step: [500/7971]      || Average Training Loss: 3.0739\n",
      "Epoch: [12/20]         || Step: [600/7971]      || Average Training Loss: 3.0717\n",
      "Epoch: [12/20]         || Step: [700/7971]      || Average Training Loss: 3.0714\n",
      "Epoch: [12/20]         || Step: [800/7971]      || Average Training Loss: 3.0707\n",
      "Epoch: [12/20]         || Step: [900/7971]      || Average Training Loss: 3.0701\n",
      "Epoch: [12/20]         || Step: [1000/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [12/20]         || Step: [1100/7971]     || Average Training Loss: 3.0680\n",
      "Epoch: [12/20]         || Step: [1200/7971]     || Average Training Loss: 3.0685\n",
      "Epoch: [12/20]         || Step: [1300/7971]     || Average Training Loss: 3.0683\n",
      "Epoch: [12/20]         || Step: [1400/7971]     || Average Training Loss: 3.0684\n",
      "Epoch: [12/20]         || Step: [1500/7971]     || Average Training Loss: 3.0683\n",
      "Epoch: [12/20]         || Step: [1600/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [12/20]         || Step: [1700/7971]     || Average Training Loss: 3.0688\n",
      "Epoch: [12/20]         || Step: [1800/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [12/20]         || Step: [1900/7971]     || Average Training Loss: 3.0691\n",
      "Epoch: [12/20]         || Step: [2000/7971]     || Average Training Loss: 3.0680\n",
      "Epoch: [12/20]         || Step: [2100/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [12/20]         || Step: [2200/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [12/20]         || Step: [2300/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [12/20]         || Step: [2400/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [12/20]         || Step: [2500/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [2600/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [12/20]         || Step: [2700/7971]     || Average Training Loss: 3.0701\n",
      "Epoch: [12/20]         || Step: [2800/7971]     || Average Training Loss: 3.0702\n",
      "Epoch: [12/20]         || Step: [2900/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [12/20]         || Step: [3000/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [12/20]         || Step: [3100/7971]     || Average Training Loss: 3.0701\n",
      "Epoch: [12/20]         || Step: [3200/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [12/20]         || Step: [3300/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [12/20]         || Step: [3400/7971]     || Average Training Loss: 3.0686\n",
      "Epoch: [12/20]         || Step: [3500/7971]     || Average Training Loss: 3.0685\n",
      "Epoch: [12/20]         || Step: [3600/7971]     || Average Training Loss: 3.0679\n",
      "Epoch: [12/20]         || Step: [3700/7971]     || Average Training Loss: 3.0678\n",
      "Epoch: [12/20]         || Step: [3800/7971]     || Average Training Loss: 3.0671\n",
      "Epoch: [12/20]         || Step: [3900/7971]     || Average Training Loss: 3.0678\n",
      "Epoch: [12/20]         || Step: [4000/7971]     || Average Training Loss: 3.0676\n",
      "Epoch: [12/20]         || Step: [4100/7971]     || Average Training Loss: 3.0684\n",
      "Epoch: [12/20]         || Step: [4200/7971]     || Average Training Loss: 3.0682\n",
      "Epoch: [12/20]         || Step: [4300/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [12/20]         || Step: [4400/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [12/20]         || Step: [4500/7971]     || Average Training Loss: 3.0688\n",
      "Epoch: [12/20]         || Step: [4600/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [12/20]         || Step: [4700/7971]     || Average Training Loss: 3.0685\n",
      "Epoch: [12/20]         || Step: [4800/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [12/20]         || Step: [4900/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [12/20]         || Step: [5000/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [12/20]         || Step: [5100/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [12/20]         || Step: [5200/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [12/20]         || Step: [5300/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [12/20]         || Step: [5400/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [12/20]         || Step: [5500/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [5600/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [5700/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [5800/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [12/20]         || Step: [5900/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [12/20]         || Step: [6000/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [6100/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [12/20]         || Step: [6200/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [12/20]         || Step: [6300/7971]     || Average Training Loss: 3.0697\n",
      "Epoch: [12/20]         || Step: [6400/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [12/20]         || Step: [6500/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [12/20]         || Step: [6600/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [12/20]         || Step: [6700/7971]     || Average Training Loss: 3.0697\n",
      "Epoch: [12/20]         || Step: [6800/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [6900/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [7000/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [7100/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [12/20]         || Step: [7200/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [7300/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [7400/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [12/20]         || Step: [7500/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [12/20]         || Step: [7600/7971]     || Average Training Loss: 3.0697\n",
      "Epoch: [12/20]         || Step: [7700/7971]     || Average Training Loss: 3.0699\n",
      "Epoch: [12/20]         || Step: [7800/7971]     || Average Training Loss: 3.0699\n",
      "Epoch: [12/20]         || Step: [7900/7971]     || Average Training Loss: 3.0697\n",
      "Epoch: [12/20]         || Step: [0/715]         || Average Validation Loss: 3.3881\n",
      "Epoch: [12/20]         || Step: [100/715]       || Average Validation Loss: 3.0431\n",
      "Epoch: [12/20]         || Step: [200/715]       || Average Validation Loss: 3.0397\n",
      "Epoch: [12/20]         || Step: [300/715]       || Average Validation Loss: 3.0404\n",
      "Epoch: [12/20]         || Step: [400/715]       || Average Validation Loss: 3.0384\n",
      "Epoch: [12/20]         || Step: [500/715]       || Average Validation Loss: 3.0389\n",
      "Epoch: [12/20]         || Step: [600/715]       || Average Validation Loss: 3.0404\n",
      "Epoch: [12/20]         || Step: [700/715]       || Average Validation Loss: 3.0410\n",
      "****************************************************************************************************\n",
      "Epoch: [12/20] || Training Loss = 3.07 || Validation Loss: 3.04 || Time: 93.512446\n",
      "****************************************************************************************************\n",
      "Epoch: [13/20]         || Step: [0/7971]        || Average Training Loss: 3.0867\n",
      "Epoch: [13/20]         || Step: [100/7971]      || Average Training Loss: 3.0506\n",
      "Epoch: [13/20]         || Step: [200/7971]      || Average Training Loss: 3.0685\n",
      "Epoch: [13/20]         || Step: [300/7971]      || Average Training Loss: 3.0651\n",
      "Epoch: [13/20]         || Step: [400/7971]      || Average Training Loss: 3.0721\n",
      "Epoch: [13/20]         || Step: [500/7971]      || Average Training Loss: 3.0712\n",
      "Epoch: [13/20]         || Step: [600/7971]      || Average Training Loss: 3.0688\n",
      "Epoch: [13/20]         || Step: [700/7971]      || Average Training Loss: 3.0688\n",
      "Epoch: [13/20]         || Step: [800/7971]      || Average Training Loss: 3.0696\n",
      "Epoch: [13/20]         || Step: [900/7971]      || Average Training Loss: 3.0694\n",
      "Epoch: [13/20]         || Step: [1000/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [13/20]         || Step: [1100/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [13/20]         || Step: [1200/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [13/20]         || Step: [1300/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [13/20]         || Step: [1400/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [13/20]         || Step: [1500/7971]     || Average Training Loss: 3.0705\n",
      "Epoch: [13/20]         || Step: [1600/7971]     || Average Training Loss: 3.0706\n",
      "Epoch: [13/20]         || Step: [1700/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [13/20]         || Step: [1800/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [13/20]         || Step: [1900/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [13/20]         || Step: [2000/7971]     || Average Training Loss: 3.0704\n",
      "Epoch: [13/20]         || Step: [2100/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [13/20]         || Step: [2200/7971]     || Average Training Loss: 3.0688\n",
      "Epoch: [13/20]         || Step: [2300/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [13/20]         || Step: [2400/7971]     || Average Training Loss: 3.0680\n",
      "Epoch: [13/20]         || Step: [2500/7971]     || Average Training Loss: 3.0684\n",
      "Epoch: [13/20]         || Step: [2600/7971]     || Average Training Loss: 3.0679\n",
      "Epoch: [13/20]         || Step: [2700/7971]     || Average Training Loss: 3.0682\n",
      "Epoch: [13/20]         || Step: [2800/7971]     || Average Training Loss: 3.0686\n",
      "Epoch: [13/20]         || Step: [2900/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [13/20]         || Step: [3000/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [13/20]         || Step: [3100/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [13/20]         || Step: [3200/7971]     || Average Training Loss: 3.0691\n",
      "Epoch: [13/20]         || Step: [3300/7971]     || Average Training Loss: 3.0685\n",
      "Epoch: [13/20]         || Step: [3400/7971]     || Average Training Loss: 3.0676\n",
      "Epoch: [13/20]         || Step: [3500/7971]     || Average Training Loss: 3.0682\n",
      "Epoch: [13/20]         || Step: [3600/7971]     || Average Training Loss: 3.0678\n",
      "Epoch: [13/20]         || Step: [3700/7971]     || Average Training Loss: 3.0673\n",
      "Epoch: [13/20]         || Step: [3800/7971]     || Average Training Loss: 3.0670\n",
      "Epoch: [13/20]         || Step: [3900/7971]     || Average Training Loss: 3.0675\n",
      "Epoch: [13/20]         || Step: [4000/7971]     || Average Training Loss: 3.0675\n",
      "Epoch: [13/20]         || Step: [4100/7971]     || Average Training Loss: 3.0676\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ea319-7ec4-4d50-88d9-7f14d809724b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
