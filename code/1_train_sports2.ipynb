{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09b4c42",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a3db4",
   "metadata": {},
   "source": [
    "This notebook narrows images to only those with sports in them. We then randomly sample these to get 10k / 2k / 2k. This gives us a slightly smaller vocab, but same size dataset. \n",
    "\n",
    "We only train on images containing sports categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606af7ca",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fccfe284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_loader import get_loader\n",
    "from models import Encoder, Decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from data_prep_utils import *\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff406e5e",
   "metadata": {},
   "source": [
    "## Load train and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd484d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 4\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'sports_v2'\n",
    "SUPER_CATEGORIES = ['sports'] # should be list of eligible coco super categories, or None to include all images\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "PRINT_EVERY = 100\n",
    "TOTAL_EPOCH = 50\n",
    "CHECKPOINT = '../model/model_sport_v3' # there is no v1 for sports:\n",
    "# v2 is consistent with previous tests as v2 parameters are shared across data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a247cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 15000 images\n",
      " val dataset has 2000 images\n",
      " test dataset has 938 images\n",
      "There are 75039 captions in the data set\n",
      "With FREQ_THRESHOLD = 4, vocab size is 2921\n"
     ]
    }
   ],
   "source": [
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=['sports'],\n",
    "                 max_train=15000, max_val=2000, max_test=2000,\n",
    "                 save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file=f'{CAPTIONS_NAME}_captions_train.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d977f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4115c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 586, Length of testing dataloader: 47\n",
      "Length of vocabulary: 2921\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_loader)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde1d50",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "593a54d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fd73a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a62141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/50]          || Step: [0/586]         || Average Training Loss: 7.9903\n",
      "Epoch: [0/50]          || Step: [100/586]       || Average Training Loss: 4.6392\n",
      "Epoch: [0/50]          || Step: [200/586]       || Average Training Loss: 4.1221\n",
      "Epoch: [0/50]          || Step: [300/586]       || Average Training Loss: 3.8262\n",
      "Epoch: [0/50]          || Step: [400/586]       || Average Training Loss: 3.6221\n",
      "Epoch: [0/50]          || Step: [500/586]       || Average Training Loss: 3.4742\n",
      "Epoch: [0/50]          || Step: [0/47]          || Average Validation Loss: 2.6269\n",
      "****************************************************************************************************\n",
      "Epoch: [0/50] || Training Loss = 3.37 || Validation Loss: 2.67 || Time: 20.881170\n",
      "****************************************************************************************************\n",
      "Epoch: [1/50]          || Step: [0/586]         || Average Training Loss: 2.9051\n",
      "Epoch: [1/50]          || Step: [100/586]       || Average Training Loss: 2.6678\n",
      "Epoch: [1/50]          || Step: [200/586]       || Average Training Loss: 2.6403\n",
      "Epoch: [1/50]          || Step: [300/586]       || Average Training Loss: 2.6135\n",
      "Epoch: [1/50]          || Step: [400/586]       || Average Training Loss: 2.5931\n",
      "Epoch: [1/50]          || Step: [500/586]       || Average Training Loss: 2.5757\n",
      "Epoch: [1/50]          || Step: [0/47]          || Average Validation Loss: 2.4030\n",
      "****************************************************************************************************\n",
      "Epoch: [1/50] || Training Loss = 2.57 || Validation Loss: 2.43 || Time: 19.635162\n",
      "****************************************************************************************************\n",
      "Epoch: [2/50]          || Step: [0/586]         || Average Training Loss: 2.5094\n",
      "Epoch: [2/50]          || Step: [100/586]       || Average Training Loss: 2.4692\n",
      "Epoch: [2/50]          || Step: [200/586]       || Average Training Loss: 2.4695\n",
      "Epoch: [2/50]          || Step: [300/586]       || Average Training Loss: 2.4674\n",
      "Epoch: [2/50]          || Step: [400/586]       || Average Training Loss: 2.4690\n",
      "Epoch: [2/50]          || Step: [500/586]       || Average Training Loss: 2.4648\n",
      "Epoch: [2/50]          || Step: [0/47]          || Average Validation Loss: 2.3438\n",
      "****************************************************************************************************\n",
      "Epoch: [2/50] || Training Loss = 2.46 || Validation Loss: 2.40 || Time: 21.720943\n",
      "****************************************************************************************************\n",
      "Epoch: [3/50]          || Step: [0/586]         || Average Training Loss: 2.4398\n",
      "Epoch: [3/50]          || Step: [100/586]       || Average Training Loss: 2.4442\n",
      "Epoch: [3/50]          || Step: [200/586]       || Average Training Loss: 2.4323\n",
      "Epoch: [3/50]          || Step: [300/586]       || Average Training Loss: 2.4348\n",
      "Epoch: [3/50]          || Step: [400/586]       || Average Training Loss: 2.4356\n",
      "Epoch: [3/50]          || Step: [500/586]       || Average Training Loss: 2.4342\n",
      "Epoch: [3/50]          || Step: [0/47]          || Average Validation Loss: 2.2241\n",
      "****************************************************************************************************\n",
      "Epoch: [3/50] || Training Loss = 2.43 || Validation Loss: 2.37 || Time: 20.924669\n",
      "****************************************************************************************************\n",
      "Epoch: [4/50]          || Step: [0/586]         || Average Training Loss: 2.4781\n",
      "Epoch: [4/50]          || Step: [100/586]       || Average Training Loss: 2.4068\n",
      "Epoch: [4/50]          || Step: [200/586]       || Average Training Loss: 2.4115\n",
      "Epoch: [4/50]          || Step: [300/586]       || Average Training Loss: 2.4101\n",
      "Epoch: [4/50]          || Step: [400/586]       || Average Training Loss: 2.4124\n",
      "Epoch: [4/50]          || Step: [500/586]       || Average Training Loss: 2.4138\n",
      "Epoch: [4/50]          || Step: [0/47]          || Average Validation Loss: 2.4344\n",
      "****************************************************************************************************\n",
      "Epoch: [4/50] || Training Loss = 2.41 || Validation Loss: 2.37 || Time: 22.216853\n",
      "****************************************************************************************************\n",
      "Epoch: [5/50]          || Step: [0/586]         || Average Training Loss: 2.3693\n",
      "Epoch: [5/50]          || Step: [100/586]       || Average Training Loss: 2.3950\n",
      "Epoch: [5/50]          || Step: [200/586]       || Average Training Loss: 2.4037\n",
      "Epoch: [5/50]          || Step: [300/586]       || Average Training Loss: 2.4037\n",
      "Epoch: [5/50]          || Step: [400/586]       || Average Training Loss: 2.4021\n",
      "Epoch: [5/50]          || Step: [500/586]       || Average Training Loss: 2.4021\n",
      "Epoch: [5/50]          || Step: [0/47]          || Average Validation Loss: 2.3062\n",
      "****************************************************************************************************\n",
      "Epoch: [5/50] || Training Loss = 2.40 || Validation Loss: 2.36 || Time: 19.449629\n",
      "****************************************************************************************************\n",
      "Epoch: [6/50]          || Step: [0/586]         || Average Training Loss: 2.3583\n",
      "Epoch: [6/50]          || Step: [100/586]       || Average Training Loss: 2.3838\n",
      "Epoch: [6/50]          || Step: [200/586]       || Average Training Loss: 2.3903\n",
      "Epoch: [6/50]          || Step: [300/586]       || Average Training Loss: 2.3944\n",
      "Epoch: [6/50]          || Step: [400/586]       || Average Training Loss: 2.3939\n",
      "Epoch: [6/50]          || Step: [500/586]       || Average Training Loss: 2.3942\n",
      "Epoch: [6/50]          || Step: [0/47]          || Average Validation Loss: 2.3443\n",
      "****************************************************************************************************\n",
      "Epoch: [6/50] || Training Loss = 2.40 || Validation Loss: 2.36 || Time: 22.016711\n",
      "****************************************************************************************************\n",
      "Epoch: [7/50]          || Step: [0/586]         || Average Training Loss: 2.3444\n",
      "Epoch: [7/50]          || Step: [100/586]       || Average Training Loss: 2.3791\n",
      "Epoch: [7/50]          || Step: [200/586]       || Average Training Loss: 2.3840\n",
      "Epoch: [7/50]          || Step: [300/586]       || Average Training Loss: 2.3910\n",
      "Epoch: [7/50]          || Step: [400/586]       || Average Training Loss: 2.3906\n",
      "Epoch: [7/50]          || Step: [500/586]       || Average Training Loss: 2.3901\n",
      "Epoch: [7/50]          || Step: [0/47]          || Average Validation Loss: 2.3183\n",
      "****************************************************************************************************\n",
      "Epoch: [7/50] || Training Loss = 2.39 || Validation Loss: 2.33 || Time: 23.715110\n",
      "****************************************************************************************************\n",
      "Epoch: [8/50]          || Step: [0/586]         || Average Training Loss: 2.3472\n",
      "Epoch: [8/50]          || Step: [100/586]       || Average Training Loss: 2.3976\n",
      "Epoch: [8/50]          || Step: [200/586]       || Average Training Loss: 2.3811\n",
      "Epoch: [8/50]          || Step: [300/586]       || Average Training Loss: 2.3847\n",
      "Epoch: [8/50]          || Step: [400/586]       || Average Training Loss: 2.3800\n",
      "Epoch: [8/50]          || Step: [500/586]       || Average Training Loss: 2.3790\n",
      "Epoch: [8/50]          || Step: [0/47]          || Average Validation Loss: 2.4095\n",
      "****************************************************************************************************\n",
      "Epoch: [8/50] || Training Loss = 2.38 || Validation Loss: 2.36 || Time: 22.515672\n",
      "****************************************************************************************************\n",
      "Epoch: [9/50]          || Step: [0/586]         || Average Training Loss: 2.3631\n",
      "Epoch: [9/50]          || Step: [100/586]       || Average Training Loss: 2.3477\n",
      "Epoch: [9/50]          || Step: [200/586]       || Average Training Loss: 2.3674\n",
      "Epoch: [9/50]          || Step: [300/586]       || Average Training Loss: 2.3677\n",
      "Epoch: [9/50]          || Step: [400/586]       || Average Training Loss: 2.3701\n",
      "Epoch: [9/50]          || Step: [500/586]       || Average Training Loss: 2.3717\n",
      "Epoch: [9/50]          || Step: [0/47]          || Average Validation Loss: 2.2803\n",
      "****************************************************************************************************\n",
      "Epoch: [9/50] || Training Loss = 2.37 || Validation Loss: 2.35 || Time: 23.436228\n",
      "****************************************************************************************************\n",
      "Epoch: [10/50]         || Step: [0/586]         || Average Training Loss: 2.3696\n",
      "Epoch: [10/50]         || Step: [100/586]       || Average Training Loss: 2.3650\n",
      "Epoch: [10/50]         || Step: [200/586]       || Average Training Loss: 2.3666\n",
      "Epoch: [10/50]         || Step: [300/586]       || Average Training Loss: 2.3643\n",
      "Epoch: [10/50]         || Step: [400/586]       || Average Training Loss: 2.3658\n",
      "Epoch: [10/50]         || Step: [500/586]       || Average Training Loss: 2.3674\n",
      "Epoch: [10/50]         || Step: [0/47]          || Average Validation Loss: 2.4240\n",
      "****************************************************************************************************\n",
      "Epoch: [10/50] || Training Loss = 2.37 || Validation Loss: 2.33 || Time: 21.564619\n",
      "****************************************************************************************************\n",
      "Epoch: [11/50]         || Step: [0/586]         || Average Training Loss: 2.3104\n",
      "Epoch: [11/50]         || Step: [100/586]       || Average Training Loss: 2.3510\n",
      "Epoch: [11/50]         || Step: [200/586]       || Average Training Loss: 2.3522\n",
      "Epoch: [11/50]         || Step: [300/586]       || Average Training Loss: 2.3561\n",
      "Epoch: [11/50]         || Step: [400/586]       || Average Training Loss: 2.3585\n",
      "Epoch: [11/50]         || Step: [500/586]       || Average Training Loss: 2.3587\n",
      "Epoch: [11/50]         || Step: [0/47]          || Average Validation Loss: 2.3282\n",
      "****************************************************************************************************\n",
      "Epoch: [11/50] || Training Loss = 2.36 || Validation Loss: 2.32 || Time: 24.111844\n",
      "****************************************************************************************************\n",
      "Epoch: [12/50]         || Step: [0/586]         || Average Training Loss: 2.4609\n",
      "Epoch: [12/50]         || Step: [100/586]       || Average Training Loss: 2.3376\n",
      "Epoch: [12/50]         || Step: [200/586]       || Average Training Loss: 2.3495\n",
      "Epoch: [12/50]         || Step: [300/586]       || Average Training Loss: 2.3495\n",
      "Epoch: [12/50]         || Step: [400/586]       || Average Training Loss: 2.3525\n",
      "Epoch: [12/50]         || Step: [500/586]       || Average Training Loss: 2.3571\n",
      "Epoch: [12/50]         || Step: [0/47]          || Average Validation Loss: 2.1895\n",
      "****************************************************************************************************\n",
      "Epoch: [12/50] || Training Loss = 2.36 || Validation Loss: 2.30 || Time: 19.817594\n",
      "****************************************************************************************************\n",
      "Epoch: [13/50]         || Step: [0/586]         || Average Training Loss: 2.3793\n",
      "Epoch: [13/50]         || Step: [100/586]       || Average Training Loss: 2.3428\n",
      "Epoch: [13/50]         || Step: [200/586]       || Average Training Loss: 2.3485\n",
      "Epoch: [13/50]         || Step: [300/586]       || Average Training Loss: 2.3494\n",
      "Epoch: [13/50]         || Step: [400/586]       || Average Training Loss: 2.3501\n",
      "Epoch: [13/50]         || Step: [500/586]       || Average Training Loss: 2.3498\n",
      "Epoch: [13/50]         || Step: [0/47]          || Average Validation Loss: 2.3080\n",
      "****************************************************************************************************\n",
      "Epoch: [13/50] || Training Loss = 2.35 || Validation Loss: 2.31 || Time: 22.438483\n",
      "****************************************************************************************************\n",
      "Epoch: [14/50]         || Step: [0/586]         || Average Training Loss: 2.4213\n",
      "Epoch: [14/50]         || Step: [100/586]       || Average Training Loss: 2.3335\n",
      "Epoch: [14/50]         || Step: [200/586]       || Average Training Loss: 2.3405\n",
      "Epoch: [14/50]         || Step: [300/586]       || Average Training Loss: 2.3467\n",
      "Epoch: [14/50]         || Step: [400/586]       || Average Training Loss: 2.3453\n",
      "Epoch: [14/50]         || Step: [500/586]       || Average Training Loss: 2.3498\n",
      "Epoch: [14/50]         || Step: [0/47]          || Average Validation Loss: 2.3951\n",
      "****************************************************************************************************\n",
      "Epoch: [14/50] || Training Loss = 2.35 || Validation Loss: 2.31 || Time: 20.857909\n",
      "****************************************************************************************************\n",
      "Epoch: [15/50]         || Step: [0/586]         || Average Training Loss: 2.3565\n",
      "Epoch: [15/50]         || Step: [100/586]       || Average Training Loss: 2.3389\n",
      "Epoch: [15/50]         || Step: [200/586]       || Average Training Loss: 2.3344\n",
      "Epoch: [15/50]         || Step: [300/586]       || Average Training Loss: 2.3365\n",
      "Epoch: [15/50]         || Step: [400/586]       || Average Training Loss: 2.3422\n",
      "Epoch: [15/50]         || Step: [500/586]       || Average Training Loss: 2.3447\n",
      "Epoch: [15/50]         || Step: [0/47]          || Average Validation Loss: 2.3889\n",
      "****************************************************************************************************\n",
      "Epoch: [15/50] || Training Loss = 2.35 || Validation Loss: 2.30 || Time: 19.885458\n",
      "****************************************************************************************************\n",
      "Epoch: [16/50]         || Step: [0/586]         || Average Training Loss: 2.2586\n",
      "Epoch: [16/50]         || Step: [100/586]       || Average Training Loss: 2.3359\n",
      "Epoch: [16/50]         || Step: [200/586]       || Average Training Loss: 2.3419\n",
      "Epoch: [16/50]         || Step: [300/586]       || Average Training Loss: 2.3471\n",
      "Epoch: [16/50]         || Step: [400/586]       || Average Training Loss: 2.3422\n",
      "Epoch: [16/50]         || Step: [500/586]       || Average Training Loss: 2.3436\n",
      "Epoch: [16/50]         || Step: [0/47]          || Average Validation Loss: 2.0882\n",
      "****************************************************************************************************\n",
      "Epoch: [16/50] || Training Loss = 2.35 || Validation Loss: 2.31 || Time: 23.550322\n",
      "****************************************************************************************************\n",
      "Epoch: [17/50]         || Step: [0/586]         || Average Training Loss: 2.3431\n",
      "Epoch: [17/50]         || Step: [100/586]       || Average Training Loss: 2.3426\n",
      "Epoch: [17/50]         || Step: [200/586]       || Average Training Loss: 2.3401\n",
      "Epoch: [17/50]         || Step: [300/586]       || Average Training Loss: 2.3406\n",
      "Epoch: [17/50]         || Step: [400/586]       || Average Training Loss: 2.3396\n",
      "Epoch: [17/50]         || Step: [500/586]       || Average Training Loss: 2.3412\n",
      "Epoch: [17/50]         || Step: [0/47]          || Average Validation Loss: 2.3952\n",
      "****************************************************************************************************\n",
      "Epoch: [17/50] || Training Loss = 2.34 || Validation Loss: 2.32 || Time: 23.931148\n",
      "****************************************************************************************************\n",
      "Epoch: [18/50]         || Step: [0/586]         || Average Training Loss: 2.2710\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803aae9-81ae-4390-96ee-d8e5ab8a703b",
   "metadata": {},
   "source": [
    "## Modify the parameters\n",
    "\n",
    "Use entire train2017 data set to build the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63490149-618c-482d-9686-063d12b5c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'sports_v4'\n",
    "SUPER_CATEGORIES = ['sports'] # should be list of eligible coco super categories, or None to include all images\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "PRINT_EVERY = 100\n",
    "TOTAL_EPOCH = 20\n",
    "CHECKPOINT = '../model/model_sport_v4' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708d529b-3ed3-4f83-b65c-815fbf126ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 15000 images\n",
      " val dataset has 2000 images\n",
      " test dataset has 938 images\n",
      "There are 591753 captions in the data set\n",
      "With FREQ_THRESHOLD = 5, vocab size is 10192\n"
     ]
    }
   ],
   "source": [
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=['sports'],\n",
    "                 max_train=15000, max_val=2000, max_test=2000,\n",
    "                 save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file='captions_train2017.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a4df44-9b0e-49e7-8c65-5226a0090b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33408d78-aa23-4ea3-a2e8-2d507bdaacf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 1172, Length of testing dataloader: 94\n",
      "Length of vocabulary: 10192\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_loader)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69841c94-4d7e-4c3a-804c-6ded7f0b37bd",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7113927c-e216-4775-bac8-37543e3f3b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac8a062-06fc-4470-8750-6f1d9554201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e8144e-34e5-4dda-84ec-7b04efb86dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "311c4c64-8751-4d2c-860a-bee388898149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ed405-1249-494d-ad46-22a7f7d72915",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35a05ed4-3a21-4206-a714-52819c83d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]          || Step: [0/1172]        || Average Training Loss: 9.2194\n",
      "Epoch: [0/20]          || Step: [100/1172]      || Average Training Loss: 4.9198\n",
      "Epoch: [0/20]          || Step: [200/1172]      || Average Training Loss: 4.4146\n",
      "Epoch: [0/20]          || Step: [300/1172]      || Average Training Loss: 4.1013\n",
      "Epoch: [0/20]          || Step: [400/1172]      || Average Training Loss: 3.8984\n",
      "Epoch: [0/20]          || Step: [500/1172]      || Average Training Loss: 3.7452\n",
      "Epoch: [0/20]          || Step: [600/1172]      || Average Training Loss: 3.6234\n",
      "Epoch: [0/20]          || Step: [700/1172]      || Average Training Loss: 3.5237\n",
      "Epoch: [0/20]          || Step: [800/1172]      || Average Training Loss: 3.4392\n",
      "Epoch: [0/20]          || Step: [900/1172]      || Average Training Loss: 3.3676\n",
      "Epoch: [0/20]          || Step: [1000/1172]     || Average Training Loss: 3.3101\n",
      "Epoch: [0/20]          || Step: [1100/1172]     || Average Training Loss: 3.2595\n",
      "Epoch: [0/20]          || Step: [0/94]          || Average Validation Loss: 2.7762\n",
      "****************************************************************************************************\n",
      "Epoch: [0/20] || Training Loss = 3.23 || Validation Loss: 2.67 || Time: 20.854592\n",
      "****************************************************************************************************\n",
      "Epoch: [1/20]          || Step: [0/1172]        || Average Training Loss: 2.8567\n",
      "Epoch: [1/20]          || Step: [100/1172]      || Average Training Loss: 2.7165\n",
      "Epoch: [1/20]          || Step: [200/1172]      || Average Training Loss: 2.6961\n",
      "Epoch: [1/20]          || Step: [300/1172]      || Average Training Loss: 2.6785\n",
      "Epoch: [1/20]          || Step: [400/1172]      || Average Training Loss: 2.6674\n",
      "Epoch: [1/20]          || Step: [500/1172]      || Average Training Loss: 2.6603\n",
      "Epoch: [1/20]          || Step: [600/1172]      || Average Training Loss: 2.6557\n",
      "Epoch: [1/20]          || Step: [700/1172]      || Average Training Loss: 2.6559\n",
      "Epoch: [1/20]          || Step: [800/1172]      || Average Training Loss: 2.6507\n",
      "Epoch: [1/20]          || Step: [900/1172]      || Average Training Loss: 2.6456\n",
      "Epoch: [1/20]          || Step: [1000/1172]     || Average Training Loss: 2.6430\n",
      "Epoch: [1/20]          || Step: [1100/1172]     || Average Training Loss: 2.6375\n",
      "Epoch: [1/20]          || Step: [0/94]          || Average Validation Loss: 2.3993\n",
      "****************************************************************************************************\n",
      "Epoch: [1/20] || Training Loss = 2.64 || Validation Loss: 2.56 || Time: 22.516500\n",
      "****************************************************************************************************\n",
      "Epoch: [2/20]          || Step: [0/1172]        || Average Training Loss: 2.5677\n",
      "Epoch: [2/20]          || Step: [100/1172]      || Average Training Loss: 2.5831\n",
      "Epoch: [2/20]          || Step: [200/1172]      || Average Training Loss: 2.5755\n",
      "Epoch: [2/20]          || Step: [300/1172]      || Average Training Loss: 2.5776\n",
      "Epoch: [2/20]          || Step: [400/1172]      || Average Training Loss: 2.5753\n",
      "Epoch: [2/20]          || Step: [500/1172]      || Average Training Loss: 2.5710\n",
      "Epoch: [2/20]          || Step: [600/1172]      || Average Training Loss: 2.5707\n",
      "Epoch: [2/20]          || Step: [700/1172]      || Average Training Loss: 2.5672\n",
      "Epoch: [2/20]          || Step: [800/1172]      || Average Training Loss: 2.5668\n",
      "Epoch: [2/20]          || Step: [900/1172]      || Average Training Loss: 2.5667\n",
      "Epoch: [2/20]          || Step: [1000/1172]     || Average Training Loss: 2.5678\n",
      "Epoch: [2/20]          || Step: [1100/1172]     || Average Training Loss: 2.5687\n",
      "Epoch: [2/20]          || Step: [0/94]          || Average Validation Loss: 2.7686\n",
      "****************************************************************************************************\n",
      "Epoch: [2/20] || Training Loss = 2.57 || Validation Loss: 2.51 || Time: 23.222543\n",
      "****************************************************************************************************\n",
      "Epoch: [3/20]          || Step: [0/1172]        || Average Training Loss: 2.4673\n",
      "Epoch: [3/20]          || Step: [100/1172]      || Average Training Loss: 2.5348\n",
      "Epoch: [3/20]          || Step: [200/1172]      || Average Training Loss: 2.5363\n",
      "Epoch: [3/20]          || Step: [300/1172]      || Average Training Loss: 2.5356\n",
      "Epoch: [3/20]          || Step: [400/1172]      || Average Training Loss: 2.5374\n",
      "Epoch: [3/20]          || Step: [500/1172]      || Average Training Loss: 2.5386\n",
      "Epoch: [3/20]          || Step: [600/1172]      || Average Training Loss: 2.5436\n",
      "Epoch: [3/20]          || Step: [700/1172]      || Average Training Loss: 2.5384\n",
      "Epoch: [3/20]          || Step: [800/1172]      || Average Training Loss: 2.5371\n",
      "Epoch: [3/20]          || Step: [900/1172]      || Average Training Loss: 2.5357\n",
      "Epoch: [3/20]          || Step: [1000/1172]     || Average Training Loss: 2.5371\n",
      "Epoch: [3/20]          || Step: [1100/1172]     || Average Training Loss: 2.5372\n",
      "Epoch: [3/20]          || Step: [0/94]          || Average Validation Loss: 2.5948\n",
      "****************************************************************************************************\n",
      "Epoch: [3/20] || Training Loss = 2.54 || Validation Loss: 2.47 || Time: 23.170069\n",
      "****************************************************************************************************\n",
      "Epoch: [4/20]          || Step: [0/1172]        || Average Training Loss: 2.5414\n",
      "Epoch: [4/20]          || Step: [100/1172]      || Average Training Loss: 2.4871\n",
      "Epoch: [4/20]          || Step: [200/1172]      || Average Training Loss: 2.4979\n",
      "Epoch: [4/20]          || Step: [300/1172]      || Average Training Loss: 2.5119\n",
      "Epoch: [4/20]          || Step: [400/1172]      || Average Training Loss: 2.5138\n",
      "Epoch: [4/20]          || Step: [500/1172]      || Average Training Loss: 2.5093\n",
      "Epoch: [4/20]          || Step: [600/1172]      || Average Training Loss: 2.5074\n",
      "Epoch: [4/20]          || Step: [700/1172]      || Average Training Loss: 2.5133\n",
      "Epoch: [4/20]          || Step: [800/1172]      || Average Training Loss: 2.5135\n",
      "Epoch: [4/20]          || Step: [900/1172]      || Average Training Loss: 2.5127\n",
      "Epoch: [4/20]          || Step: [1000/1172]     || Average Training Loss: 2.5155\n",
      "Epoch: [4/20]          || Step: [1100/1172]     || Average Training Loss: 2.5182\n",
      "Epoch: [4/20]          || Step: [0/94]          || Average Validation Loss: 2.5913\n",
      "****************************************************************************************************\n",
      "Epoch: [4/20] || Training Loss = 2.52 || Validation Loss: 2.49 || Time: 23.322726\n",
      "****************************************************************************************************\n",
      "Epoch: [5/20]          || Step: [0/1172]        || Average Training Loss: 2.8880\n",
      "Epoch: [5/20]          || Step: [100/1172]      || Average Training Loss: 2.5031\n",
      "Epoch: [5/20]          || Step: [200/1172]      || Average Training Loss: 2.5013\n",
      "Epoch: [5/20]          || Step: [300/1172]      || Average Training Loss: 2.5018\n",
      "Epoch: [5/20]          || Step: [400/1172]      || Average Training Loss: 2.5045\n",
      "Epoch: [5/20]          || Step: [500/1172]      || Average Training Loss: 2.5006\n",
      "Epoch: [5/20]          || Step: [600/1172]      || Average Training Loss: 2.4988\n",
      "Epoch: [5/20]          || Step: [700/1172]      || Average Training Loss: 2.5003\n",
      "Epoch: [5/20]          || Step: [800/1172]      || Average Training Loss: 2.5010\n",
      "Epoch: [5/20]          || Step: [900/1172]      || Average Training Loss: 2.5002\n",
      "Epoch: [5/20]          || Step: [1000/1172]     || Average Training Loss: 2.5025\n",
      "Epoch: [5/20]          || Step: [1100/1172]     || Average Training Loss: 2.5035\n",
      "Epoch: [5/20]          || Step: [0/94]          || Average Validation Loss: 2.3051\n",
      "****************************************************************************************************\n",
      "Epoch: [5/20] || Training Loss = 2.50 || Validation Loss: 2.45 || Time: 23.165601\n",
      "****************************************************************************************************\n",
      "Epoch: [6/20]          || Step: [0/1172]        || Average Training Loss: 2.4498\n",
      "Epoch: [6/20]          || Step: [100/1172]      || Average Training Loss: 2.4634\n",
      "Epoch: [6/20]          || Step: [200/1172]      || Average Training Loss: 2.4785\n",
      "Epoch: [6/20]          || Step: [300/1172]      || Average Training Loss: 2.4927\n",
      "Epoch: [6/20]          || Step: [400/1172]      || Average Training Loss: 2.4949\n",
      "Epoch: [6/20]          || Step: [500/1172]      || Average Training Loss: 2.4933\n",
      "Epoch: [6/20]          || Step: [600/1172]      || Average Training Loss: 2.4938\n",
      "Epoch: [6/20]          || Step: [700/1172]      || Average Training Loss: 2.4923\n",
      "Epoch: [6/20]          || Step: [800/1172]      || Average Training Loss: 2.4898\n",
      "Epoch: [6/20]          || Step: [900/1172]      || Average Training Loss: 2.4894\n",
      "Epoch: [6/20]          || Step: [1000/1172]     || Average Training Loss: 2.4908\n",
      "Epoch: [6/20]          || Step: [1100/1172]     || Average Training Loss: 2.4922\n",
      "Epoch: [6/20]          || Step: [0/94]          || Average Validation Loss: 2.4359\n",
      "****************************************************************************************************\n",
      "Epoch: [6/20] || Training Loss = 2.49 || Validation Loss: 2.45 || Time: 22.835479\n",
      "****************************************************************************************************\n",
      "Epoch: [7/20]          || Step: [0/1172]        || Average Training Loss: 2.4275\n",
      "Epoch: [7/20]          || Step: [100/1172]      || Average Training Loss: 2.4925\n",
      "Epoch: [7/20]          || Step: [200/1172]      || Average Training Loss: 2.4763\n",
      "Epoch: [7/20]          || Step: [300/1172]      || Average Training Loss: 2.4796\n",
      "Epoch: [7/20]          || Step: [400/1172]      || Average Training Loss: 2.4805\n",
      "Epoch: [7/20]          || Step: [500/1172]      || Average Training Loss: 2.4831\n",
      "Epoch: [7/20]          || Step: [600/1172]      || Average Training Loss: 2.4788\n",
      "Epoch: [7/20]          || Step: [700/1172]      || Average Training Loss: 2.4801\n",
      "Epoch: [7/20]          || Step: [800/1172]      || Average Training Loss: 2.4822\n",
      "Epoch: [7/20]          || Step: [900/1172]      || Average Training Loss: 2.4835\n",
      "Epoch: [7/20]          || Step: [1000/1172]     || Average Training Loss: 2.4834\n",
      "Epoch: [7/20]          || Step: [1100/1172]     || Average Training Loss: 2.4831\n",
      "Epoch: [7/20]          || Step: [0/94]          || Average Validation Loss: 2.3861\n",
      "****************************************************************************************************\n",
      "Epoch: [7/20] || Training Loss = 2.48 || Validation Loss: 2.45 || Time: 22.379947\n",
      "****************************************************************************************************\n",
      "Epoch: [8/20]          || Step: [0/1172]        || Average Training Loss: 2.4622\n",
      "Epoch: [8/20]          || Step: [100/1172]      || Average Training Loss: 2.4787\n",
      "Epoch: [8/20]          || Step: [200/1172]      || Average Training Loss: 2.4607\n",
      "Epoch: [8/20]          || Step: [300/1172]      || Average Training Loss: 2.4640\n",
      "Epoch: [8/20]          || Step: [400/1172]      || Average Training Loss: 2.4700\n",
      "Epoch: [8/20]          || Step: [500/1172]      || Average Training Loss: 2.4704\n",
      "Epoch: [8/20]          || Step: [600/1172]      || Average Training Loss: 2.4689\n",
      "Epoch: [8/20]          || Step: [700/1172]      || Average Training Loss: 2.4710\n",
      "Epoch: [8/20]          || Step: [800/1172]      || Average Training Loss: 2.4740\n",
      "Epoch: [8/20]          || Step: [900/1172]      || Average Training Loss: 2.4725\n",
      "Epoch: [8/20]          || Step: [1000/1172]     || Average Training Loss: 2.4747\n",
      "Epoch: [8/20]          || Step: [1100/1172]     || Average Training Loss: 2.4765\n",
      "Epoch: [8/20]          || Step: [0/94]          || Average Validation Loss: 2.3775\n",
      "****************************************************************************************************\n",
      "Epoch: [8/20] || Training Loss = 2.48 || Validation Loss: 2.44 || Time: 23.219478\n",
      "****************************************************************************************************\n",
      "Epoch: [9/20]          || Step: [0/1172]        || Average Training Loss: 2.5795\n",
      "Epoch: [9/20]          || Step: [100/1172]      || Average Training Loss: 2.4722\n",
      "Epoch: [9/20]          || Step: [200/1172]      || Average Training Loss: 2.4661\n",
      "Epoch: [9/20]          || Step: [300/1172]      || Average Training Loss: 2.4649\n",
      "Epoch: [9/20]          || Step: [400/1172]      || Average Training Loss: 2.4665\n",
      "Epoch: [9/20]          || Step: [500/1172]      || Average Training Loss: 2.4739\n",
      "Epoch: [9/20]          || Step: [600/1172]      || Average Training Loss: 2.4754\n",
      "Epoch: [9/20]          || Step: [700/1172]      || Average Training Loss: 2.4720\n",
      "Epoch: [9/20]          || Step: [800/1172]      || Average Training Loss: 2.4725\n",
      "Epoch: [9/20]          || Step: [900/1172]      || Average Training Loss: 2.4741\n",
      "Epoch: [9/20]          || Step: [1000/1172]     || Average Training Loss: 2.4740\n",
      "Epoch: [9/20]          || Step: [1100/1172]     || Average Training Loss: 2.4728\n",
      "Epoch: [9/20]          || Step: [0/94]          || Average Validation Loss: 2.3796\n",
      "****************************************************************************************************\n",
      "Epoch: [9/20] || Training Loss = 2.47 || Validation Loss: 2.44 || Time: 23.110099\n",
      "****************************************************************************************************\n",
      "Epoch: [10/20]         || Step: [0/1172]        || Average Training Loss: 2.6726\n",
      "Epoch: [10/20]         || Step: [100/1172]      || Average Training Loss: 2.4745\n",
      "Epoch: [10/20]         || Step: [200/1172]      || Average Training Loss: 2.4692\n",
      "Epoch: [10/20]         || Step: [300/1172]      || Average Training Loss: 2.4758\n",
      "Epoch: [10/20]         || Step: [400/1172]      || Average Training Loss: 2.4716\n",
      "Epoch: [10/20]         || Step: [500/1172]      || Average Training Loss: 2.4698\n",
      "Epoch: [10/20]         || Step: [600/1172]      || Average Training Loss: 2.4700\n",
      "Epoch: [10/20]         || Step: [700/1172]      || Average Training Loss: 2.4679\n",
      "Epoch: [10/20]         || Step: [800/1172]      || Average Training Loss: 2.4667\n",
      "Epoch: [10/20]         || Step: [900/1172]      || Average Training Loss: 2.4674\n",
      "Epoch: [10/20]         || Step: [1000/1172]     || Average Training Loss: 2.4674\n",
      "Epoch: [10/20]         || Step: [1100/1172]     || Average Training Loss: 2.4671\n",
      "Epoch: [10/20]         || Step: [0/94]          || Average Validation Loss: 2.5596\n",
      "****************************************************************************************************\n",
      "Epoch: [10/20] || Training Loss = 2.47 || Validation Loss: 2.43 || Time: 23.393522\n",
      "****************************************************************************************************\n",
      "Epoch: [11/20]         || Step: [0/1172]        || Average Training Loss: 2.5304\n",
      "Epoch: [11/20]         || Step: [100/1172]      || Average Training Loss: 2.4592\n",
      "Epoch: [11/20]         || Step: [200/1172]      || Average Training Loss: 2.4636\n",
      "Epoch: [11/20]         || Step: [300/1172]      || Average Training Loss: 2.4620\n",
      "Epoch: [11/20]         || Step: [400/1172]      || Average Training Loss: 2.4629\n",
      "Epoch: [11/20]         || Step: [500/1172]      || Average Training Loss: 2.4648\n",
      "Epoch: [11/20]         || Step: [600/1172]      || Average Training Loss: 2.4611\n",
      "Epoch: [11/20]         || Step: [700/1172]      || Average Training Loss: 2.4641\n",
      "Epoch: [11/20]         || Step: [800/1172]      || Average Training Loss: 2.4624\n",
      "Epoch: [11/20]         || Step: [900/1172]      || Average Training Loss: 2.4630\n",
      "Epoch: [11/20]         || Step: [1000/1172]     || Average Training Loss: 2.4658\n",
      "Epoch: [11/20]         || Step: [1100/1172]     || Average Training Loss: 2.4663\n",
      "Epoch: [11/20]         || Step: [0/94]          || Average Validation Loss: 2.2494\n",
      "****************************************************************************************************\n",
      "Epoch: [11/20] || Training Loss = 2.47 || Validation Loss: 2.42 || Time: 23.221473\n",
      "****************************************************************************************************\n",
      "Epoch: [12/20]         || Step: [0/1172]        || Average Training Loss: 2.3920\n",
      "Epoch: [12/20]         || Step: [100/1172]      || Average Training Loss: 2.4312\n",
      "Epoch: [12/20]         || Step: [200/1172]      || Average Training Loss: 2.4505\n",
      "Epoch: [12/20]         || Step: [300/1172]      || Average Training Loss: 2.4606\n",
      "Epoch: [12/20]         || Step: [400/1172]      || Average Training Loss: 2.4602\n",
      "Epoch: [12/20]         || Step: [500/1172]      || Average Training Loss: 2.4551\n",
      "Epoch: [12/20]         || Step: [600/1172]      || Average Training Loss: 2.4551\n",
      "Epoch: [12/20]         || Step: [700/1172]      || Average Training Loss: 2.4549\n",
      "Epoch: [12/20]         || Step: [800/1172]      || Average Training Loss: 2.4577\n",
      "Epoch: [12/20]         || Step: [900/1172]      || Average Training Loss: 2.4601\n",
      "Epoch: [12/20]         || Step: [1000/1172]     || Average Training Loss: 2.4597\n",
      "Epoch: [12/20]         || Step: [1100/1172]     || Average Training Loss: 2.4627\n",
      "Epoch: [12/20]         || Step: [0/94]          || Average Validation Loss: 2.7505\n",
      "****************************************************************************************************\n",
      "Epoch: [12/20] || Training Loss = 2.46 || Validation Loss: 2.46 || Time: 23.205209\n",
      "****************************************************************************************************\n",
      "Epoch: [13/20]         || Step: [0/1172]        || Average Training Loss: 2.5739\n",
      "Epoch: [13/20]         || Step: [100/1172]      || Average Training Loss: 2.4687\n",
      "Epoch: [13/20]         || Step: [200/1172]      || Average Training Loss: 2.4706\n",
      "Epoch: [13/20]         || Step: [300/1172]      || Average Training Loss: 2.4625\n",
      "Epoch: [13/20]         || Step: [400/1172]      || Average Training Loss: 2.4609\n",
      "Epoch: [13/20]         || Step: [500/1172]      || Average Training Loss: 2.4600\n",
      "Epoch: [13/20]         || Step: [600/1172]      || Average Training Loss: 2.4589\n",
      "Epoch: [13/20]         || Step: [700/1172]      || Average Training Loss: 2.4602\n",
      "Epoch: [13/20]         || Step: [800/1172]      || Average Training Loss: 2.4591\n",
      "Epoch: [13/20]         || Step: [900/1172]      || Average Training Loss: 2.4593\n",
      "Epoch: [13/20]         || Step: [1000/1172]     || Average Training Loss: 2.4586\n",
      "Epoch: [13/20]         || Step: [1100/1172]     || Average Training Loss: 2.4595\n",
      "Epoch: [13/20]         || Step: [0/94]          || Average Validation Loss: 2.4290\n",
      "****************************************************************************************************\n",
      "Epoch: [13/20] || Training Loss = 2.46 || Validation Loss: 2.45 || Time: 23.070593\n",
      "****************************************************************************************************\n",
      "Epoch: [14/20]         || Step: [0/1172]        || Average Training Loss: 2.4774\n",
      "Epoch: [14/20]         || Step: [100/1172]      || Average Training Loss: 2.4684\n",
      "Epoch: [14/20]         || Step: [200/1172]      || Average Training Loss: 2.4652\n",
      "Epoch: [14/20]         || Step: [300/1172]      || Average Training Loss: 2.4563\n",
      "Epoch: [14/20]         || Step: [400/1172]      || Average Training Loss: 2.4597\n",
      "Epoch: [14/20]         || Step: [500/1172]      || Average Training Loss: 2.4601\n",
      "Epoch: [14/20]         || Step: [600/1172]      || Average Training Loss: 2.4604\n",
      "Epoch: [14/20]         || Step: [700/1172]      || Average Training Loss: 2.4592\n",
      "Epoch: [14/20]         || Step: [800/1172]      || Average Training Loss: 2.4570\n",
      "Epoch: [14/20]         || Step: [900/1172]      || Average Training Loss: 2.4598\n",
      "Epoch: [14/20]         || Step: [1000/1172]     || Average Training Loss: 2.4567\n",
      "Epoch: [14/20]         || Step: [1100/1172]     || Average Training Loss: 2.4577\n",
      "Epoch: [14/20]         || Step: [0/94]          || Average Validation Loss: 2.3111\n",
      "****************************************************************************************************\n",
      "Epoch: [14/20] || Training Loss = 2.46 || Validation Loss: 2.43 || Time: 23.480174\n",
      "****************************************************************************************************\n",
      "Epoch: [15/20]         || Step: [0/1172]        || Average Training Loss: 2.2121\n",
      "Epoch: [15/20]         || Step: [100/1172]      || Average Training Loss: 2.5278\n",
      "Epoch: [15/20]         || Step: [200/1172]      || Average Training Loss: 2.4980\n",
      "Epoch: [15/20]         || Step: [300/1172]      || Average Training Loss: 2.4805\n",
      "Epoch: [15/20]         || Step: [400/1172]      || Average Training Loss: 2.4761\n",
      "Epoch: [15/20]         || Step: [500/1172]      || Average Training Loss: 2.4735\n",
      "Epoch: [15/20]         || Step: [600/1172]      || Average Training Loss: 2.4732\n",
      "Epoch: [15/20]         || Step: [700/1172]      || Average Training Loss: 2.4730\n",
      "Epoch: [15/20]         || Step: [800/1172]      || Average Training Loss: 2.4709\n",
      "Epoch: [15/20]         || Step: [900/1172]      || Average Training Loss: 2.4740\n",
      "Epoch: [15/20]         || Step: [1000/1172]     || Average Training Loss: 2.4723\n",
      "Epoch: [15/20]         || Step: [1100/1172]     || Average Training Loss: 2.4707\n",
      "Epoch: [15/20]         || Step: [0/94]          || Average Validation Loss: 2.3083\n",
      "****************************************************************************************************\n",
      "Epoch: [15/20] || Training Loss = 2.47 || Validation Loss: 2.43 || Time: 23.200717\n",
      "****************************************************************************************************\n",
      "Epoch: [16/20]         || Step: [0/1172]        || Average Training Loss: 2.4225\n",
      "Epoch: [16/20]         || Step: [100/1172]      || Average Training Loss: 2.4391\n",
      "Epoch: [16/20]         || Step: [200/1172]      || Average Training Loss: 2.4503\n",
      "Epoch: [16/20]         || Step: [300/1172]      || Average Training Loss: 2.4489\n",
      "Epoch: [16/20]         || Step: [400/1172]      || Average Training Loss: 2.4476\n",
      "Epoch: [16/20]         || Step: [500/1172]      || Average Training Loss: 2.4537\n",
      "Epoch: [16/20]         || Step: [600/1172]      || Average Training Loss: 2.4563\n",
      "Epoch: [16/20]         || Step: [700/1172]      || Average Training Loss: 2.4592\n",
      "Epoch: [16/20]         || Step: [800/1172]      || Average Training Loss: 2.4571\n",
      "Epoch: [16/20]         || Step: [900/1172]      || Average Training Loss: 2.4562\n",
      "Epoch: [16/20]         || Step: [1000/1172]     || Average Training Loss: 2.4561\n",
      "Epoch: [16/20]         || Step: [1100/1172]     || Average Training Loss: 2.4575\n",
      "Epoch: [16/20]         || Step: [0/94]          || Average Validation Loss: 2.6180\n",
      "****************************************************************************************************\n",
      "Epoch: [16/20] || Training Loss = 2.46 || Validation Loss: 2.42 || Time: 23.103212\n",
      "****************************************************************************************************\n",
      "Epoch: [17/20]         || Step: [0/1172]        || Average Training Loss: 2.3992\n",
      "Epoch: [17/20]         || Step: [100/1172]      || Average Training Loss: 2.4698\n",
      "Epoch: [17/20]         || Step: [200/1172]      || Average Training Loss: 2.4619\n",
      "Epoch: [17/20]         || Step: [300/1172]      || Average Training Loss: 2.4633\n",
      "Epoch: [17/20]         || Step: [400/1172]      || Average Training Loss: 2.4562\n",
      "Epoch: [17/20]         || Step: [500/1172]      || Average Training Loss: 2.4557\n",
      "Epoch: [17/20]         || Step: [600/1172]      || Average Training Loss: 2.4568\n",
      "Epoch: [17/20]         || Step: [700/1172]      || Average Training Loss: 2.4549\n",
      "Epoch: [17/20]         || Step: [800/1172]      || Average Training Loss: 2.4540\n",
      "Epoch: [17/20]         || Step: [900/1172]      || Average Training Loss: 2.4553\n",
      "Epoch: [17/20]         || Step: [1000/1172]     || Average Training Loss: 2.4540\n",
      "Epoch: [17/20]         || Step: [1100/1172]     || Average Training Loss: 2.4551\n",
      "Epoch: [17/20]         || Step: [0/94]          || Average Validation Loss: 2.3671\n",
      "****************************************************************************************************\n",
      "Epoch: [17/20] || Training Loss = 2.46 || Validation Loss: 2.42 || Time: 23.173589\n",
      "****************************************************************************************************\n",
      "Epoch: [18/20]         || Step: [0/1172]        || Average Training Loss: 2.3651\n",
      "Epoch: [18/20]         || Step: [100/1172]      || Average Training Loss: 2.4599\n",
      "Epoch: [18/20]         || Step: [200/1172]      || Average Training Loss: 2.4533\n",
      "Epoch: [18/20]         || Step: [300/1172]      || Average Training Loss: 2.4500\n",
      "Epoch: [18/20]         || Step: [400/1172]      || Average Training Loss: 2.4511\n",
      "Epoch: [18/20]         || Step: [500/1172]      || Average Training Loss: 2.4535\n",
      "Epoch: [18/20]         || Step: [600/1172]      || Average Training Loss: 2.4528\n",
      "Epoch: [18/20]         || Step: [700/1172]      || Average Training Loss: 2.4507\n",
      "Epoch: [18/20]         || Step: [800/1172]      || Average Training Loss: 2.4492\n",
      "Epoch: [18/20]         || Step: [900/1172]      || Average Training Loss: 2.4515\n",
      "Epoch: [18/20]         || Step: [1000/1172]     || Average Training Loss: 2.4529\n",
      "Epoch: [18/20]         || Step: [1100/1172]     || Average Training Loss: 2.4533\n",
      "Epoch: [18/20]         || Step: [0/94]          || Average Validation Loss: 2.3801\n",
      "****************************************************************************************************\n",
      "Epoch: [18/20] || Training Loss = 2.45 || Validation Loss: 2.44 || Time: 23.373509\n",
      "****************************************************************************************************\n",
      "Epoch: [19/20]         || Step: [0/1172]        || Average Training Loss: 2.5565\n",
      "Epoch: [19/20]         || Step: [100/1172]      || Average Training Loss: 2.4281\n",
      "Epoch: [19/20]         || Step: [200/1172]      || Average Training Loss: 2.4489\n",
      "Epoch: [19/20]         || Step: [300/1172]      || Average Training Loss: 2.4494\n",
      "Epoch: [19/20]         || Step: [400/1172]      || Average Training Loss: 2.4363\n",
      "Epoch: [19/20]         || Step: [500/1172]      || Average Training Loss: 2.4398\n",
      "Epoch: [19/20]         || Step: [600/1172]      || Average Training Loss: 2.4446\n",
      "Epoch: [19/20]         || Step: [700/1172]      || Average Training Loss: 2.4443\n",
      "Epoch: [19/20]         || Step: [800/1172]      || Average Training Loss: 2.4477\n",
      "Epoch: [19/20]         || Step: [900/1172]      || Average Training Loss: 2.4476\n",
      "Epoch: [19/20]         || Step: [1000/1172]     || Average Training Loss: 2.4493\n",
      "Epoch: [19/20]         || Step: [1100/1172]     || Average Training Loss: 2.4489\n",
      "Epoch: [19/20]         || Step: [0/94]          || Average Validation Loss: 2.3247\n",
      "****************************************************************************************************\n",
      "Epoch: [19/20] || Training Loss = 2.45 || Validation Loss: 2.44 || Time: 23.445234\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f06f3-1a22-4b53-a87a-9e797a8e861d",
   "metadata": {},
   "source": [
    "## Modify the parameters\n",
    "\n",
    "Use entire train2017 data set to build the vocab\n",
    "\n",
    "And use entire dataset to train. Why not? What the hell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6d730d-0882-424f-b520-e330d1baeb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'everything'\n",
    "SUPER_CATEGORIES = None # should be list of eligible coco super categories, or None to include all images\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "PRINT_EVERY = 100\n",
    "TOTAL_EPOCH = 20\n",
    "CHECKPOINT = '../model/model_everything' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf61d9c2-bcbd-4c14-9090-57b8fe5daa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 102021 images\n",
      " val dataset has 15245 images\n",
      " test dataset has 4952 images\n",
      "There are 591753 captions in the data set\n",
      "With FREQ_THRESHOLD = 5, vocab size is 10192\n"
     ]
    }
   ],
   "source": [
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=SUPER_CATEGORIES,\n",
    "                 max_train=150000, max_val=50000, max_test=50000,\n",
    "                 save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file='captions_train2017.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc75f567-28d3-46fb-94df-02828d3dcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b81f5de-ea54-4f49-85a2-a282ba1c8bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 7971, Length of testing dataloader: 715\n",
      "Length of vocabulary: 10192\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_loader)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b43c55-c36a-4666-8a00-e9af6f9e2afe",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4951dffc-3082-4c18-8b71-b61f2f0774c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f58731e-1029-4a29-a023-5f4fe0845aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd6b7c6f-648b-4495-a457-022d011a0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "111f1d95-658a-41e9-aff3-fa5476cd6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a593bc2a-7354-4bc7-9e90-777451001ffd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bd6ee-a4c4-426f-894b-64ac184cf004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]          || Step: [0/7971]        || Average Training Loss: 9.2285\n",
      "Epoch: [0/20]          || Step: [100/7971]      || Average Training Loss: 5.4096\n",
      "Epoch: [0/20]          || Step: [200/7971]      || Average Training Loss: 4.9976\n",
      "Epoch: [0/20]          || Step: [300/7971]      || Average Training Loss: 4.7313\n",
      "Epoch: [0/20]          || Step: [400/7971]      || Average Training Loss: 4.5499\n",
      "Epoch: [0/20]          || Step: [500/7971]      || Average Training Loss: 4.4200\n",
      "Epoch: [0/20]          || Step: [600/7971]      || Average Training Loss: 4.3241\n",
      "Epoch: [0/20]          || Step: [700/7971]      || Average Training Loss: 4.2454\n",
      "Epoch: [0/20]          || Step: [800/7971]      || Average Training Loss: 4.1795\n",
      "Epoch: [0/20]          || Step: [900/7971]      || Average Training Loss: 4.1217\n",
      "Epoch: [0/20]          || Step: [1000/7971]     || Average Training Loss: 4.0718\n",
      "Epoch: [0/20]          || Step: [1100/7971]     || Average Training Loss: 4.0272\n",
      "Epoch: [0/20]          || Step: [1200/7971]     || Average Training Loss: 3.9841\n",
      "Epoch: [0/20]          || Step: [1300/7971]     || Average Training Loss: 3.9469\n",
      "Epoch: [0/20]          || Step: [1400/7971]     || Average Training Loss: 3.9145\n",
      "Epoch: [0/20]          || Step: [1500/7971]     || Average Training Loss: 3.8836\n",
      "Epoch: [0/20]          || Step: [1600/7971]     || Average Training Loss: 3.8549\n",
      "Epoch: [0/20]          || Step: [1700/7971]     || Average Training Loss: 3.8303\n",
      "Epoch: [0/20]          || Step: [1800/7971]     || Average Training Loss: 3.8059\n",
      "Epoch: [0/20]          || Step: [1900/7971]     || Average Training Loss: 3.7840\n",
      "Epoch: [0/20]          || Step: [2000/7971]     || Average Training Loss: 3.7629\n",
      "Epoch: [0/20]          || Step: [2100/7971]     || Average Training Loss: 3.7451\n",
      "Epoch: [0/20]          || Step: [2200/7971]     || Average Training Loss: 3.7266\n",
      "Epoch: [0/20]          || Step: [2300/7971]     || Average Training Loss: 3.7108\n",
      "Epoch: [0/20]          || Step: [2400/7971]     || Average Training Loss: 3.6951\n",
      "Epoch: [0/20]          || Step: [2500/7971]     || Average Training Loss: 3.6800\n",
      "Epoch: [0/20]          || Step: [2600/7971]     || Average Training Loss: 3.6658\n",
      "Epoch: [0/20]          || Step: [2700/7971]     || Average Training Loss: 3.6534\n",
      "Epoch: [0/20]          || Step: [2800/7971]     || Average Training Loss: 3.6411\n",
      "Epoch: [0/20]          || Step: [2900/7971]     || Average Training Loss: 3.6295\n",
      "Epoch: [0/20]          || Step: [3000/7971]     || Average Training Loss: 3.6183\n",
      "Epoch: [0/20]          || Step: [3100/7971]     || Average Training Loss: 3.6073\n",
      "Epoch: [0/20]          || Step: [3200/7971]     || Average Training Loss: 3.5972\n",
      "Epoch: [0/20]          || Step: [3300/7971]     || Average Training Loss: 3.5878\n",
      "Epoch: [0/20]          || Step: [3400/7971]     || Average Training Loss: 3.5792\n",
      "Epoch: [0/20]          || Step: [3500/7971]     || Average Training Loss: 3.5708\n",
      "Epoch: [0/20]          || Step: [3600/7971]     || Average Training Loss: 3.5617\n",
      "Epoch: [0/20]          || Step: [3700/7971]     || Average Training Loss: 3.5528\n",
      "Epoch: [0/20]          || Step: [3800/7971]     || Average Training Loss: 3.5452\n",
      "Epoch: [0/20]          || Step: [3900/7971]     || Average Training Loss: 3.5379\n",
      "Epoch: [0/20]          || Step: [4000/7971]     || Average Training Loss: 3.5305\n",
      "Epoch: [0/20]          || Step: [4100/7971]     || Average Training Loss: 3.5232\n",
      "Epoch: [0/20]          || Step: [4200/7971]     || Average Training Loss: 3.5170\n",
      "Epoch: [0/20]          || Step: [4300/7971]     || Average Training Loss: 3.5103\n",
      "Epoch: [0/20]          || Step: [4400/7971]     || Average Training Loss: 3.5042\n",
      "Epoch: [0/20]          || Step: [4500/7971]     || Average Training Loss: 3.4978\n",
      "Epoch: [0/20]          || Step: [4600/7971]     || Average Training Loss: 3.4921\n",
      "Epoch: [0/20]          || Step: [4700/7971]     || Average Training Loss: 3.4862\n",
      "Epoch: [0/20]          || Step: [4800/7971]     || Average Training Loss: 3.4811\n",
      "Epoch: [0/20]          || Step: [4900/7971]     || Average Training Loss: 3.4759\n",
      "Epoch: [0/20]          || Step: [5000/7971]     || Average Training Loss: 3.4709\n",
      "Epoch: [0/20]          || Step: [5100/7971]     || Average Training Loss: 3.4661\n",
      "Epoch: [0/20]          || Step: [5200/7971]     || Average Training Loss: 3.4611\n",
      "Epoch: [0/20]          || Step: [5300/7971]     || Average Training Loss: 3.4562\n",
      "Epoch: [0/20]          || Step: [5400/7971]     || Average Training Loss: 3.4515\n",
      "Epoch: [0/20]          || Step: [5500/7971]     || Average Training Loss: 3.4470\n",
      "Epoch: [0/20]          || Step: [5600/7971]     || Average Training Loss: 3.4429\n",
      "Epoch: [0/20]          || Step: [5700/7971]     || Average Training Loss: 3.4388\n",
      "Epoch: [0/20]          || Step: [5800/7971]     || Average Training Loss: 3.4345\n",
      "Epoch: [0/20]          || Step: [5900/7971]     || Average Training Loss: 3.4305\n",
      "Epoch: [0/20]          || Step: [6000/7971]     || Average Training Loss: 3.4266\n",
      "Epoch: [0/20]          || Step: [6100/7971]     || Average Training Loss: 3.4228\n",
      "Epoch: [0/20]          || Step: [6200/7971]     || Average Training Loss: 3.4192\n",
      "Epoch: [0/20]          || Step: [6300/7971]     || Average Training Loss: 3.4157\n",
      "Epoch: [0/20]          || Step: [6400/7971]     || Average Training Loss: 3.4119\n",
      "Epoch: [0/20]          || Step: [6500/7971]     || Average Training Loss: 3.4084\n",
      "Epoch: [0/20]          || Step: [6600/7971]     || Average Training Loss: 3.4054\n",
      "Epoch: [0/20]          || Step: [6700/7971]     || Average Training Loss: 3.4018\n",
      "Epoch: [0/20]          || Step: [6800/7971]     || Average Training Loss: 3.3985\n",
      "Epoch: [0/20]          || Step: [6900/7971]     || Average Training Loss: 3.3955\n",
      "Epoch: [0/20]          || Step: [7000/7971]     || Average Training Loss: 3.3921\n",
      "Epoch: [0/20]          || Step: [7100/7971]     || Average Training Loss: 3.3891\n",
      "Epoch: [0/20]          || Step: [7200/7971]     || Average Training Loss: 3.3862\n",
      "Epoch: [0/20]          || Step: [7300/7971]     || Average Training Loss: 3.3837\n",
      "Epoch: [0/20]          || Step: [7400/7971]     || Average Training Loss: 3.3814\n",
      "Epoch: [0/20]          || Step: [7500/7971]     || Average Training Loss: 3.3785\n",
      "Epoch: [0/20]          || Step: [7600/7971]     || Average Training Loss: 3.3758\n",
      "Epoch: [0/20]          || Step: [7700/7971]     || Average Training Loss: 3.3730\n",
      "Epoch: [0/20]          || Step: [7800/7971]     || Average Training Loss: 3.3701\n",
      "Epoch: [0/20]          || Step: [7900/7971]     || Average Training Loss: 3.3675\n",
      "Epoch: [0/20]          || Step: [0/715]         || Average Validation Loss: 3.2936\n",
      "Epoch: [0/20]          || Step: [100/715]       || Average Validation Loss: 3.1465\n",
      "Epoch: [0/20]          || Step: [200/715]       || Average Validation Loss: 3.1545\n",
      "Epoch: [0/20]          || Step: [300/715]       || Average Validation Loss: 3.1545\n",
      "Epoch: [0/20]          || Step: [400/715]       || Average Validation Loss: 3.1561\n",
      "Epoch: [0/20]          || Step: [500/715]       || Average Validation Loss: 3.1538\n",
      "Epoch: [0/20]          || Step: [600/715]       || Average Validation Loss: 3.1549\n",
      "Epoch: [0/20]          || Step: [700/715]       || Average Validation Loss: 3.1573\n",
      "****************************************************************************************************\n",
      "Epoch: [0/20] || Training Loss = 3.37 || Validation Loss: 3.16 || Time: 154.330686\n",
      "****************************************************************************************************\n",
      "Epoch: [1/20]          || Step: [0/7971]        || Average Training Loss: 3.1757\n",
      "Epoch: [1/20]          || Step: [100/7971]      || Average Training Loss: 3.1674\n",
      "Epoch: [1/20]          || Step: [200/7971]      || Average Training Loss: 3.1715\n",
      "Epoch: [1/20]          || Step: [300/7971]      || Average Training Loss: 3.1686\n",
      "Epoch: [1/20]          || Step: [400/7971]      || Average Training Loss: 3.1663\n",
      "Epoch: [1/20]          || Step: [500/7971]      || Average Training Loss: 3.1688\n",
      "Epoch: [1/20]          || Step: [600/7971]      || Average Training Loss: 3.1689\n",
      "Epoch: [1/20]          || Step: [700/7971]      || Average Training Loss: 3.1671\n",
      "Epoch: [1/20]          || Step: [800/7971]      || Average Training Loss: 3.1673\n",
      "Epoch: [1/20]          || Step: [900/7971]      || Average Training Loss: 3.1666\n",
      "Epoch: [1/20]          || Step: [1000/7971]     || Average Training Loss: 3.1663\n",
      "Epoch: [1/20]          || Step: [1100/7971]     || Average Training Loss: 3.1644\n",
      "Epoch: [1/20]          || Step: [1200/7971]     || Average Training Loss: 3.1655\n",
      "Epoch: [1/20]          || Step: [1300/7971]     || Average Training Loss: 3.1643\n",
      "Epoch: [1/20]          || Step: [1400/7971]     || Average Training Loss: 3.1630\n",
      "Epoch: [1/20]          || Step: [1500/7971]     || Average Training Loss: 3.1630\n",
      "Epoch: [1/20]          || Step: [1600/7971]     || Average Training Loss: 3.1623\n",
      "Epoch: [1/20]          || Step: [1700/7971]     || Average Training Loss: 3.1620\n",
      "Epoch: [1/20]          || Step: [1800/7971]     || Average Training Loss: 3.1611\n",
      "Epoch: [1/20]          || Step: [1900/7971]     || Average Training Loss: 3.1597\n",
      "Epoch: [1/20]          || Step: [2000/7971]     || Average Training Loss: 3.1595\n",
      "Epoch: [1/20]          || Step: [2100/7971]     || Average Training Loss: 3.1593\n",
      "Epoch: [1/20]          || Step: [2200/7971]     || Average Training Loss: 3.1575\n",
      "Epoch: [1/20]          || Step: [2300/7971]     || Average Training Loss: 3.1567\n",
      "Epoch: [1/20]          || Step: [2400/7971]     || Average Training Loss: 3.1570\n",
      "Epoch: [1/20]          || Step: [2500/7971]     || Average Training Loss: 3.1562\n",
      "Epoch: [1/20]          || Step: [2600/7971]     || Average Training Loss: 3.1564\n",
      "Epoch: [1/20]          || Step: [2700/7971]     || Average Training Loss: 3.1561\n",
      "Epoch: [1/20]          || Step: [2800/7971]     || Average Training Loss: 3.1553\n",
      "Epoch: [1/20]          || Step: [2900/7971]     || Average Training Loss: 3.1546\n",
      "Epoch: [1/20]          || Step: [3000/7971]     || Average Training Loss: 3.1552\n",
      "Epoch: [1/20]          || Step: [3100/7971]     || Average Training Loss: 3.1553\n",
      "Epoch: [1/20]          || Step: [3200/7971]     || Average Training Loss: 3.1553\n",
      "Epoch: [1/20]          || Step: [3300/7971]     || Average Training Loss: 3.1542\n",
      "Epoch: [1/20]          || Step: [3400/7971]     || Average Training Loss: 3.1538\n",
      "Epoch: [1/20]          || Step: [3500/7971]     || Average Training Loss: 3.1529\n",
      "Epoch: [1/20]          || Step: [3600/7971]     || Average Training Loss: 3.1519\n",
      "Epoch: [1/20]          || Step: [3700/7971]     || Average Training Loss: 3.1514\n",
      "Epoch: [1/20]          || Step: [3800/7971]     || Average Training Loss: 3.1512\n",
      "Epoch: [1/20]          || Step: [3900/7971]     || Average Training Loss: 3.1508\n",
      "Epoch: [1/20]          || Step: [4000/7971]     || Average Training Loss: 3.1501\n",
      "Epoch: [1/20]          || Step: [4100/7971]     || Average Training Loss: 3.1497\n",
      "Epoch: [1/20]          || Step: [4200/7971]     || Average Training Loss: 3.1488\n",
      "Epoch: [1/20]          || Step: [4300/7971]     || Average Training Loss: 3.1484\n",
      "Epoch: [1/20]          || Step: [4400/7971]     || Average Training Loss: 3.1484\n",
      "Epoch: [1/20]          || Step: [4500/7971]     || Average Training Loss: 3.1483\n",
      "Epoch: [1/20]          || Step: [4600/7971]     || Average Training Loss: 3.1480\n",
      "Epoch: [1/20]          || Step: [4700/7971]     || Average Training Loss: 3.1479\n",
      "Epoch: [1/20]          || Step: [4800/7971]     || Average Training Loss: 3.1476\n",
      "Epoch: [1/20]          || Step: [4900/7971]     || Average Training Loss: 3.1473\n",
      "Epoch: [1/20]          || Step: [5000/7971]     || Average Training Loss: 3.1471\n",
      "Epoch: [1/20]          || Step: [5100/7971]     || Average Training Loss: 3.1470\n",
      "Epoch: [1/20]          || Step: [5200/7971]     || Average Training Loss: 3.1463\n",
      "Epoch: [1/20]          || Step: [5300/7971]     || Average Training Loss: 3.1465\n",
      "Epoch: [1/20]          || Step: [5400/7971]     || Average Training Loss: 3.1462\n",
      "Epoch: [1/20]          || Step: [5500/7971]     || Average Training Loss: 3.1454\n",
      "Epoch: [1/20]          || Step: [5600/7971]     || Average Training Loss: 3.1447\n",
      "Epoch: [1/20]          || Step: [5700/7971]     || Average Training Loss: 3.1442\n",
      "Epoch: [1/20]          || Step: [5800/7971]     || Average Training Loss: 3.1440\n",
      "Epoch: [1/20]          || Step: [5900/7971]     || Average Training Loss: 3.1438\n",
      "Epoch: [1/20]          || Step: [6000/7971]     || Average Training Loss: 3.1434\n",
      "Epoch: [1/20]          || Step: [6100/7971]     || Average Training Loss: 3.1433\n",
      "Epoch: [1/20]          || Step: [6200/7971]     || Average Training Loss: 3.1428\n",
      "Epoch: [1/20]          || Step: [6300/7971]     || Average Training Loss: 3.1428\n",
      "Epoch: [1/20]          || Step: [6400/7971]     || Average Training Loss: 3.1423\n",
      "Epoch: [1/20]          || Step: [6500/7971]     || Average Training Loss: 3.1422\n",
      "Epoch: [1/20]          || Step: [6600/7971]     || Average Training Loss: 3.1422\n",
      "Epoch: [1/20]          || Step: [6700/7971]     || Average Training Loss: 3.1419\n",
      "Epoch: [1/20]          || Step: [6800/7971]     || Average Training Loss: 3.1416\n",
      "Epoch: [1/20]          || Step: [6900/7971]     || Average Training Loss: 3.1413\n",
      "Epoch: [1/20]          || Step: [7000/7971]     || Average Training Loss: 3.1409\n",
      "Epoch: [1/20]          || Step: [7100/7971]     || Average Training Loss: 3.1409\n",
      "Epoch: [1/20]          || Step: [7200/7971]     || Average Training Loss: 3.1407\n",
      "Epoch: [1/20]          || Step: [7300/7971]     || Average Training Loss: 3.1403\n",
      "Epoch: [1/20]          || Step: [7400/7971]     || Average Training Loss: 3.1402\n",
      "Epoch: [1/20]          || Step: [7500/7971]     || Average Training Loss: 3.1397\n",
      "Epoch: [1/20]          || Step: [7600/7971]     || Average Training Loss: 3.1397\n",
      "Epoch: [1/20]          || Step: [7700/7971]     || Average Training Loss: 3.1392\n",
      "Epoch: [1/20]          || Step: [7800/7971]     || Average Training Loss: 3.1390\n",
      "Epoch: [1/20]          || Step: [7900/7971]     || Average Training Loss: 3.1384\n",
      "Epoch: [1/20]          || Step: [0/715]         || Average Validation Loss: 3.1811\n",
      "Epoch: [1/20]          || Step: [100/715]       || Average Validation Loss: 3.0966\n",
      "Epoch: [1/20]          || Step: [200/715]       || Average Validation Loss: 3.0930\n",
      "Epoch: [1/20]          || Step: [300/715]       || Average Validation Loss: 3.0961\n",
      "Epoch: [1/20]          || Step: [400/715]       || Average Validation Loss: 3.0960\n",
      "Epoch: [1/20]          || Step: [500/715]       || Average Validation Loss: 3.0964\n",
      "Epoch: [1/20]          || Step: [600/715]       || Average Validation Loss: 3.0995\n",
      "Epoch: [1/20]          || Step: [700/715]       || Average Validation Loss: 3.1007\n",
      "****************************************************************************************************\n",
      "Epoch: [1/20] || Training Loss = 3.14 || Validation Loss: 3.10 || Time: 113.911075\n",
      "****************************************************************************************************\n",
      "Epoch: [2/20]          || Step: [0/7971]        || Average Training Loss: 3.0873\n",
      "Epoch: [2/20]          || Step: [100/7971]      || Average Training Loss: 3.1192\n",
      "Epoch: [2/20]          || Step: [200/7971]      || Average Training Loss: 3.1106\n",
      "Epoch: [2/20]          || Step: [300/7971]      || Average Training Loss: 3.1057\n",
      "Epoch: [2/20]          || Step: [400/7971]      || Average Training Loss: 3.1152\n",
      "Epoch: [2/20]          || Step: [500/7971]      || Average Training Loss: 3.1131\n",
      "Epoch: [2/20]          || Step: [600/7971]      || Average Training Loss: 3.1120\n",
      "Epoch: [2/20]          || Step: [700/7971]      || Average Training Loss: 3.1102\n",
      "Epoch: [2/20]          || Step: [800/7971]      || Average Training Loss: 3.1128\n",
      "Epoch: [2/20]          || Step: [900/7971]      || Average Training Loss: 3.1109\n",
      "Epoch: [2/20]          || Step: [1000/7971]     || Average Training Loss: 3.1106\n",
      "Epoch: [2/20]          || Step: [1100/7971]     || Average Training Loss: 3.1113\n",
      "Epoch: [2/20]          || Step: [1200/7971]     || Average Training Loss: 3.1108\n",
      "Epoch: [2/20]          || Step: [1300/7971]     || Average Training Loss: 3.1115\n",
      "Epoch: [2/20]          || Step: [1400/7971]     || Average Training Loss: 3.1123\n",
      "Epoch: [2/20]          || Step: [1500/7971]     || Average Training Loss: 3.1113\n",
      "Epoch: [2/20]          || Step: [1600/7971]     || Average Training Loss: 3.1122\n",
      "Epoch: [2/20]          || Step: [1700/7971]     || Average Training Loss: 3.1113\n",
      "Epoch: [2/20]          || Step: [1800/7971]     || Average Training Loss: 3.1124\n",
      "Epoch: [2/20]          || Step: [1900/7971]     || Average Training Loss: 3.1125\n",
      "Epoch: [2/20]          || Step: [2000/7971]     || Average Training Loss: 3.1134\n",
      "Epoch: [2/20]          || Step: [2100/7971]     || Average Training Loss: 3.1139\n",
      "Epoch: [2/20]          || Step: [2200/7971]     || Average Training Loss: 3.1138\n",
      "Epoch: [2/20]          || Step: [2300/7971]     || Average Training Loss: 3.1130\n",
      "Epoch: [2/20]          || Step: [2400/7971]     || Average Training Loss: 3.1130\n",
      "Epoch: [2/20]          || Step: [2500/7971]     || Average Training Loss: 3.1134\n",
      "Epoch: [2/20]          || Step: [2600/7971]     || Average Training Loss: 3.1135\n",
      "Epoch: [2/20]          || Step: [2700/7971]     || Average Training Loss: 3.1139\n",
      "Epoch: [2/20]          || Step: [2800/7971]     || Average Training Loss: 3.1141\n",
      "Epoch: [2/20]          || Step: [2900/7971]     || Average Training Loss: 3.1135\n",
      "Epoch: [2/20]          || Step: [3000/7971]     || Average Training Loss: 3.1136\n",
      "Epoch: [2/20]          || Step: [3100/7971]     || Average Training Loss: 3.1139\n",
      "Epoch: [2/20]          || Step: [3200/7971]     || Average Training Loss: 3.1139\n",
      "Epoch: [2/20]          || Step: [3300/7971]     || Average Training Loss: 3.1137\n",
      "Epoch: [2/20]          || Step: [3400/7971]     || Average Training Loss: 3.1136\n",
      "Epoch: [2/20]          || Step: [3500/7971]     || Average Training Loss: 3.1137\n",
      "Epoch: [2/20]          || Step: [3600/7971]     || Average Training Loss: 3.1131\n",
      "Epoch: [2/20]          || Step: [3700/7971]     || Average Training Loss: 3.1129\n",
      "Epoch: [2/20]          || Step: [3800/7971]     || Average Training Loss: 3.1126\n",
      "Epoch: [2/20]          || Step: [3900/7971]     || Average Training Loss: 3.1125\n",
      "Epoch: [2/20]          || Step: [4000/7971]     || Average Training Loss: 3.1125\n",
      "Epoch: [2/20]          || Step: [4100/7971]     || Average Training Loss: 3.1121\n",
      "Epoch: [2/20]          || Step: [4200/7971]     || Average Training Loss: 3.1122\n",
      "Epoch: [2/20]          || Step: [4300/7971]     || Average Training Loss: 3.1120\n",
      "Epoch: [2/20]          || Step: [4400/7971]     || Average Training Loss: 3.1117\n",
      "Epoch: [2/20]          || Step: [4500/7971]     || Average Training Loss: 3.1112\n",
      "Epoch: [2/20]          || Step: [4600/7971]     || Average Training Loss: 3.1109\n",
      "Epoch: [2/20]          || Step: [4700/7971]     || Average Training Loss: 3.1107\n",
      "Epoch: [2/20]          || Step: [4800/7971]     || Average Training Loss: 3.1103\n",
      "Epoch: [2/20]          || Step: [4900/7971]     || Average Training Loss: 3.1096\n",
      "Epoch: [2/20]          || Step: [5000/7971]     || Average Training Loss: 3.1092\n",
      "Epoch: [2/20]          || Step: [5100/7971]     || Average Training Loss: 3.1092\n",
      "Epoch: [2/20]          || Step: [5200/7971]     || Average Training Loss: 3.1091\n",
      "Epoch: [2/20]          || Step: [5300/7971]     || Average Training Loss: 3.1091\n",
      "Epoch: [2/20]          || Step: [5400/7971]     || Average Training Loss: 3.1087\n",
      "Epoch: [2/20]          || Step: [5500/7971]     || Average Training Loss: 3.1084\n",
      "Epoch: [2/20]          || Step: [5600/7971]     || Average Training Loss: 3.1081\n",
      "Epoch: [2/20]          || Step: [5700/7971]     || Average Training Loss: 3.1082\n",
      "Epoch: [2/20]          || Step: [5800/7971]     || Average Training Loss: 3.1082\n",
      "Epoch: [2/20]          || Step: [5900/7971]     || Average Training Loss: 3.1079\n",
      "Epoch: [2/20]          || Step: [6000/7971]     || Average Training Loss: 3.1081\n",
      "Epoch: [2/20]          || Step: [6100/7971]     || Average Training Loss: 3.1086\n",
      "Epoch: [2/20]          || Step: [6200/7971]     || Average Training Loss: 3.1083\n",
      "Epoch: [2/20]          || Step: [6300/7971]     || Average Training Loss: 3.1078\n",
      "Epoch: [2/20]          || Step: [6400/7971]     || Average Training Loss: 3.1079\n",
      "Epoch: [2/20]          || Step: [6500/7971]     || Average Training Loss: 3.1079\n",
      "Epoch: [2/20]          || Step: [6600/7971]     || Average Training Loss: 3.1077\n",
      "Epoch: [2/20]          || Step: [6700/7971]     || Average Training Loss: 3.1076\n",
      "Epoch: [2/20]          || Step: [6800/7971]     || Average Training Loss: 3.1077\n",
      "Epoch: [2/20]          || Step: [6900/7971]     || Average Training Loss: 3.1079\n",
      "Epoch: [2/20]          || Step: [7000/7971]     || Average Training Loss: 3.1083\n",
      "Epoch: [2/20]          || Step: [7100/7971]     || Average Training Loss: 3.1084\n",
      "Epoch: [2/20]          || Step: [7200/7971]     || Average Training Loss: 3.1081\n",
      "Epoch: [2/20]          || Step: [7300/7971]     || Average Training Loss: 3.1083\n",
      "Epoch: [2/20]          || Step: [7400/7971]     || Average Training Loss: 3.1080\n",
      "Epoch: [2/20]          || Step: [7500/7971]     || Average Training Loss: 3.1082\n",
      "Epoch: [2/20]          || Step: [7600/7971]     || Average Training Loss: 3.1080\n",
      "Epoch: [2/20]          || Step: [7700/7971]     || Average Training Loss: 3.1081\n",
      "Epoch: [2/20]          || Step: [7800/7971]     || Average Training Loss: 3.1080\n",
      "Epoch: [2/20]          || Step: [7900/7971]     || Average Training Loss: 3.1079\n",
      "Epoch: [2/20]          || Step: [0/715]         || Average Validation Loss: 2.9021\n",
      "Epoch: [2/20]          || Step: [100/715]       || Average Validation Loss: 3.0786\n",
      "Epoch: [2/20]          || Step: [200/715]       || Average Validation Loss: 3.0819\n",
      "Epoch: [2/20]          || Step: [300/715]       || Average Validation Loss: 3.0743\n",
      "Epoch: [2/20]          || Step: [400/715]       || Average Validation Loss: 3.0675\n",
      "Epoch: [2/20]          || Step: [500/715]       || Average Validation Loss: 3.0677\n",
      "Epoch: [2/20]          || Step: [600/715]       || Average Validation Loss: 3.0671\n",
      "Epoch: [2/20]          || Step: [700/715]       || Average Validation Loss: 3.0686\n",
      "****************************************************************************************************\n",
      "Epoch: [2/20] || Training Loss = 3.11 || Validation Loss: 3.07 || Time: 117.050364\n",
      "****************************************************************************************************\n",
      "Epoch: [3/20]          || Step: [0/7971]        || Average Training Loss: 3.0901\n",
      "Epoch: [3/20]          || Step: [100/7971]      || Average Training Loss: 3.1129\n",
      "Epoch: [3/20]          || Step: [200/7971]      || Average Training Loss: 3.1070\n",
      "Epoch: [3/20]          || Step: [300/7971]      || Average Training Loss: 3.1075\n",
      "Epoch: [3/20]          || Step: [400/7971]      || Average Training Loss: 3.1046\n",
      "Epoch: [3/20]          || Step: [500/7971]      || Average Training Loss: 3.0964\n",
      "Epoch: [3/20]          || Step: [600/7971]      || Average Training Loss: 3.0985\n",
      "Epoch: [3/20]          || Step: [700/7971]      || Average Training Loss: 3.0959\n",
      "Epoch: [3/20]          || Step: [800/7971]      || Average Training Loss: 3.0959\n",
      "Epoch: [3/20]          || Step: [900/7971]      || Average Training Loss: 3.0956\n",
      "Epoch: [3/20]          || Step: [1000/7971]     || Average Training Loss: 3.0964\n",
      "Epoch: [3/20]          || Step: [1100/7971]     || Average Training Loss: 3.0967\n",
      "Epoch: [3/20]          || Step: [1200/7971]     || Average Training Loss: 3.0960\n",
      "Epoch: [3/20]          || Step: [1300/7971]     || Average Training Loss: 3.0948\n",
      "Epoch: [3/20]          || Step: [1400/7971]     || Average Training Loss: 3.0952\n",
      "Epoch: [3/20]          || Step: [1500/7971]     || Average Training Loss: 3.0941\n",
      "Epoch: [3/20]          || Step: [1600/7971]     || Average Training Loss: 3.0941\n",
      "Epoch: [3/20]          || Step: [1700/7971]     || Average Training Loss: 3.0940\n",
      "Epoch: [3/20]          || Step: [1800/7971]     || Average Training Loss: 3.0934\n",
      "Epoch: [3/20]          || Step: [1900/7971]     || Average Training Loss: 3.0932\n",
      "Epoch: [3/20]          || Step: [2000/7971]     || Average Training Loss: 3.0935\n",
      "Epoch: [3/20]          || Step: [2100/7971]     || Average Training Loss: 3.0933\n",
      "Epoch: [3/20]          || Step: [2200/7971]     || Average Training Loss: 3.0947\n",
      "Epoch: [3/20]          || Step: [2300/7971]     || Average Training Loss: 3.0955\n",
      "Epoch: [3/20]          || Step: [2400/7971]     || Average Training Loss: 3.0963\n",
      "Epoch: [3/20]          || Step: [2500/7971]     || Average Training Loss: 3.0962\n",
      "Epoch: [3/20]          || Step: [2600/7971]     || Average Training Loss: 3.0958\n",
      "Epoch: [3/20]          || Step: [2700/7971]     || Average Training Loss: 3.0953\n",
      "Epoch: [3/20]          || Step: [2800/7971]     || Average Training Loss: 3.0955\n",
      "Epoch: [3/20]          || Step: [2900/7971]     || Average Training Loss: 3.0958\n",
      "Epoch: [3/20]          || Step: [3000/7971]     || Average Training Loss: 3.0951\n",
      "Epoch: [3/20]          || Step: [3100/7971]     || Average Training Loss: 3.0955\n",
      "Epoch: [3/20]          || Step: [3200/7971]     || Average Training Loss: 3.0944\n",
      "Epoch: [3/20]          || Step: [3300/7971]     || Average Training Loss: 3.0948\n",
      "Epoch: [3/20]          || Step: [3400/7971]     || Average Training Loss: 3.0942\n",
      "Epoch: [3/20]          || Step: [3500/7971]     || Average Training Loss: 3.0942\n",
      "Epoch: [3/20]          || Step: [3600/7971]     || Average Training Loss: 3.0948\n",
      "Epoch: [3/20]          || Step: [3700/7971]     || Average Training Loss: 3.0944\n",
      "Epoch: [3/20]          || Step: [3800/7971]     || Average Training Loss: 3.0944\n",
      "Epoch: [3/20]          || Step: [3900/7971]     || Average Training Loss: 3.0947\n",
      "Epoch: [3/20]          || Step: [4000/7971]     || Average Training Loss: 3.0944\n",
      "Epoch: [3/20]          || Step: [4100/7971]     || Average Training Loss: 3.0944\n",
      "Epoch: [3/20]          || Step: [4200/7971]     || Average Training Loss: 3.0947\n",
      "Epoch: [3/20]          || Step: [4300/7971]     || Average Training Loss: 3.0948\n",
      "Epoch: [3/20]          || Step: [4400/7971]     || Average Training Loss: 3.0948\n",
      "Epoch: [3/20]          || Step: [4500/7971]     || Average Training Loss: 3.0946\n",
      "Epoch: [3/20]          || Step: [4600/7971]     || Average Training Loss: 3.0942\n",
      "Epoch: [3/20]          || Step: [4700/7971]     || Average Training Loss: 3.0941\n",
      "Epoch: [3/20]          || Step: [4800/7971]     || Average Training Loss: 3.0938\n",
      "Epoch: [3/20]          || Step: [4900/7971]     || Average Training Loss: 3.0936\n",
      "Epoch: [3/20]          || Step: [5000/7971]     || Average Training Loss: 3.0938\n",
      "Epoch: [3/20]          || Step: [5100/7971]     || Average Training Loss: 3.0941\n",
      "Epoch: [3/20]          || Step: [5200/7971]     || Average Training Loss: 3.0941\n",
      "Epoch: [3/20]          || Step: [5300/7971]     || Average Training Loss: 3.0936\n",
      "Epoch: [3/20]          || Step: [5400/7971]     || Average Training Loss: 3.0930\n",
      "Epoch: [3/20]          || Step: [5500/7971]     || Average Training Loss: 3.0932\n",
      "Epoch: [3/20]          || Step: [5600/7971]     || Average Training Loss: 3.0933\n",
      "Epoch: [3/20]          || Step: [5700/7971]     || Average Training Loss: 3.0935\n",
      "Epoch: [3/20]          || Step: [5800/7971]     || Average Training Loss: 3.0931\n",
      "Epoch: [3/20]          || Step: [5900/7971]     || Average Training Loss: 3.0928\n",
      "Epoch: [3/20]          || Step: [6000/7971]     || Average Training Loss: 3.0930\n",
      "Epoch: [3/20]          || Step: [6100/7971]     || Average Training Loss: 3.0931\n",
      "Epoch: [3/20]          || Step: [6200/7971]     || Average Training Loss: 3.0931\n",
      "Epoch: [3/20]          || Step: [6300/7971]     || Average Training Loss: 3.0930\n",
      "Epoch: [3/20]          || Step: [6400/7971]     || Average Training Loss: 3.0930\n",
      "Epoch: [3/20]          || Step: [6500/7971]     || Average Training Loss: 3.0931\n",
      "Epoch: [3/20]          || Step: [6600/7971]     || Average Training Loss: 3.0930\n",
      "Epoch: [3/20]          || Step: [6700/7971]     || Average Training Loss: 3.0931\n",
      "Epoch: [3/20]          || Step: [6800/7971]     || Average Training Loss: 3.0934\n",
      "Epoch: [3/20]          || Step: [6900/7971]     || Average Training Loss: 3.0935\n",
      "Epoch: [3/20]          || Step: [7000/7971]     || Average Training Loss: 3.0936\n",
      "Epoch: [3/20]          || Step: [7100/7971]     || Average Training Loss: 3.0937\n",
      "Epoch: [3/20]          || Step: [7200/7971]     || Average Training Loss: 3.0939\n",
      "Epoch: [3/20]          || Step: [7300/7971]     || Average Training Loss: 3.0937\n",
      "Epoch: [3/20]          || Step: [7400/7971]     || Average Training Loss: 3.0937\n",
      "Epoch: [3/20]          || Step: [7500/7971]     || Average Training Loss: 3.0937\n",
      "Epoch: [3/20]          || Step: [7600/7971]     || Average Training Loss: 3.0939\n",
      "Epoch: [3/20]          || Step: [7700/7971]     || Average Training Loss: 3.0938\n",
      "Epoch: [3/20]          || Step: [7800/7971]     || Average Training Loss: 3.0937\n",
      "Epoch: [3/20]          || Step: [7900/7971]     || Average Training Loss: 3.0935\n",
      "Epoch: [3/20]          || Step: [0/715]         || Average Validation Loss: 2.9335\n",
      "Epoch: [3/20]          || Step: [100/715]       || Average Validation Loss: 3.0396\n",
      "Epoch: [3/20]          || Step: [200/715]       || Average Validation Loss: 3.0494\n",
      "Epoch: [3/20]          || Step: [300/715]       || Average Validation Loss: 3.0479\n",
      "Epoch: [3/20]          || Step: [400/715]       || Average Validation Loss: 3.0472\n",
      "Epoch: [3/20]          || Step: [500/715]       || Average Validation Loss: 3.0467\n",
      "Epoch: [3/20]          || Step: [600/715]       || Average Validation Loss: 3.0464\n",
      "Epoch: [3/20]          || Step: [700/715]       || Average Validation Loss: 3.0481\n",
      "****************************************************************************************************\n",
      "Epoch: [3/20] || Training Loss = 3.09 || Validation Loss: 3.05 || Time: 99.071148\n",
      "****************************************************************************************************\n",
      "Epoch: [4/20]          || Step: [0/7971]        || Average Training Loss: 3.0914\n",
      "Epoch: [4/20]          || Step: [100/7971]      || Average Training Loss: 3.0828\n",
      "Epoch: [4/20]          || Step: [200/7971]      || Average Training Loss: 3.0935\n",
      "Epoch: [4/20]          || Step: [300/7971]      || Average Training Loss: 3.0947\n",
      "Epoch: [4/20]          || Step: [400/7971]      || Average Training Loss: 3.0976\n",
      "Epoch: [4/20]          || Step: [500/7971]      || Average Training Loss: 3.0975\n",
      "Epoch: [4/20]          || Step: [600/7971]      || Average Training Loss: 3.0940\n",
      "Epoch: [4/20]          || Step: [700/7971]      || Average Training Loss: 3.0956\n",
      "Epoch: [4/20]          || Step: [800/7971]      || Average Training Loss: 3.0926\n",
      "Epoch: [4/20]          || Step: [900/7971]      || Average Training Loss: 3.0935\n",
      "Epoch: [4/20]          || Step: [1000/7971]     || Average Training Loss: 3.0920\n",
      "Epoch: [4/20]          || Step: [1100/7971]     || Average Training Loss: 3.0912\n",
      "Epoch: [4/20]          || Step: [1200/7971]     || Average Training Loss: 3.0905\n",
      "Epoch: [4/20]          || Step: [1300/7971]     || Average Training Loss: 3.0912\n",
      "Epoch: [4/20]          || Step: [1400/7971]     || Average Training Loss: 3.0909\n",
      "Epoch: [4/20]          || Step: [1500/7971]     || Average Training Loss: 3.0905\n",
      "Epoch: [4/20]          || Step: [1600/7971]     || Average Training Loss: 3.0908\n",
      "Epoch: [4/20]          || Step: [1700/7971]     || Average Training Loss: 3.0902\n",
      "Epoch: [4/20]          || Step: [1800/7971]     || Average Training Loss: 3.0895\n",
      "Epoch: [4/20]          || Step: [1900/7971]     || Average Training Loss: 3.0887\n",
      "Epoch: [4/20]          || Step: [2000/7971]     || Average Training Loss: 3.0878\n",
      "Epoch: [4/20]          || Step: [2100/7971]     || Average Training Loss: 3.0885\n",
      "Epoch: [4/20]          || Step: [2200/7971]     || Average Training Loss: 3.0877\n",
      "Epoch: [4/20]          || Step: [2300/7971]     || Average Training Loss: 3.0879\n",
      "Epoch: [4/20]          || Step: [2400/7971]     || Average Training Loss: 3.0882\n",
      "Epoch: [4/20]          || Step: [2500/7971]     || Average Training Loss: 3.0890\n",
      "Epoch: [4/20]          || Step: [2600/7971]     || Average Training Loss: 3.0880\n",
      "Epoch: [4/20]          || Step: [2700/7971]     || Average Training Loss: 3.0874\n",
      "Epoch: [4/20]          || Step: [2800/7971]     || Average Training Loss: 3.0882\n",
      "Epoch: [4/20]          || Step: [2900/7971]     || Average Training Loss: 3.0890\n",
      "Epoch: [4/20]          || Step: [3000/7971]     || Average Training Loss: 3.0893\n",
      "Epoch: [4/20]          || Step: [3100/7971]     || Average Training Loss: 3.0895\n",
      "Epoch: [4/20]          || Step: [3200/7971]     || Average Training Loss: 3.0907\n",
      "Epoch: [4/20]          || Step: [3300/7971]     || Average Training Loss: 3.0914\n",
      "Epoch: [4/20]          || Step: [3400/7971]     || Average Training Loss: 3.0913\n",
      "Epoch: [4/20]          || Step: [3500/7971]     || Average Training Loss: 3.0910\n",
      "Epoch: [4/20]          || Step: [3600/7971]     || Average Training Loss: 3.0910\n",
      "Epoch: [4/20]          || Step: [3700/7971]     || Average Training Loss: 3.0912\n",
      "Epoch: [4/20]          || Step: [3800/7971]     || Average Training Loss: 3.0910\n",
      "Epoch: [4/20]          || Step: [3900/7971]     || Average Training Loss: 3.0912\n",
      "Epoch: [4/20]          || Step: [4000/7971]     || Average Training Loss: 3.0908\n",
      "Epoch: [4/20]          || Step: [4100/7971]     || Average Training Loss: 3.0905\n",
      "Epoch: [4/20]          || Step: [4200/7971]     || Average Training Loss: 3.0900\n",
      "Epoch: [4/20]          || Step: [4300/7971]     || Average Training Loss: 3.0891\n",
      "Epoch: [4/20]          || Step: [4400/7971]     || Average Training Loss: 3.0886\n",
      "Epoch: [4/20]          || Step: [4500/7971]     || Average Training Loss: 3.0885\n",
      "Epoch: [4/20]          || Step: [4600/7971]     || Average Training Loss: 3.0884\n",
      "Epoch: [4/20]          || Step: [4700/7971]     || Average Training Loss: 3.0882\n",
      "Epoch: [4/20]          || Step: [4800/7971]     || Average Training Loss: 3.0884\n",
      "Epoch: [4/20]          || Step: [4900/7971]     || Average Training Loss: 3.0881\n",
      "Epoch: [4/20]          || Step: [5000/7971]     || Average Training Loss: 3.0878\n",
      "Epoch: [4/20]          || Step: [5100/7971]     || Average Training Loss: 3.0876\n",
      "Epoch: [4/20]          || Step: [5200/7971]     || Average Training Loss: 3.0875\n",
      "Epoch: [4/20]          || Step: [5300/7971]     || Average Training Loss: 3.0873\n",
      "Epoch: [4/20]          || Step: [5400/7971]     || Average Training Loss: 3.0875\n",
      "Epoch: [4/20]          || Step: [5500/7971]     || Average Training Loss: 3.0869\n",
      "Epoch: [4/20]          || Step: [5600/7971]     || Average Training Loss: 3.0871\n",
      "Epoch: [4/20]          || Step: [5700/7971]     || Average Training Loss: 3.0874\n",
      "Epoch: [4/20]          || Step: [5800/7971]     || Average Training Loss: 3.0876\n",
      "Epoch: [4/20]          || Step: [5900/7971]     || Average Training Loss: 3.0871\n",
      "Epoch: [4/20]          || Step: [6000/7971]     || Average Training Loss: 3.0871\n",
      "Epoch: [4/20]          || Step: [6100/7971]     || Average Training Loss: 3.0870\n",
      "Epoch: [4/20]          || Step: [6200/7971]     || Average Training Loss: 3.0871\n",
      "Epoch: [4/20]          || Step: [6300/7971]     || Average Training Loss: 3.0869\n",
      "Epoch: [4/20]          || Step: [6400/7971]     || Average Training Loss: 3.0867\n",
      "Epoch: [4/20]          || Step: [6500/7971]     || Average Training Loss: 3.0867\n",
      "Epoch: [4/20]          || Step: [6600/7971]     || Average Training Loss: 3.0868\n",
      "Epoch: [4/20]          || Step: [6700/7971]     || Average Training Loss: 3.0865\n",
      "Epoch: [4/20]          || Step: [6800/7971]     || Average Training Loss: 3.0865\n",
      "Epoch: [4/20]          || Step: [6900/7971]     || Average Training Loss: 3.0862\n",
      "Epoch: [4/20]          || Step: [7000/7971]     || Average Training Loss: 3.0865\n",
      "Epoch: [4/20]          || Step: [7100/7971]     || Average Training Loss: 3.0868\n",
      "Epoch: [4/20]          || Step: [7200/7971]     || Average Training Loss: 3.0869\n",
      "Epoch: [4/20]          || Step: [7300/7971]     || Average Training Loss: 3.0869\n",
      "Epoch: [4/20]          || Step: [7400/7971]     || Average Training Loss: 3.0866\n",
      "Epoch: [4/20]          || Step: [7500/7971]     || Average Training Loss: 3.0866\n",
      "Epoch: [4/20]          || Step: [7600/7971]     || Average Training Loss: 3.0862\n",
      "Epoch: [4/20]          || Step: [7700/7971]     || Average Training Loss: 3.0865\n",
      "Epoch: [4/20]          || Step: [7800/7971]     || Average Training Loss: 3.0864\n",
      "Epoch: [4/20]          || Step: [7900/7971]     || Average Training Loss: 3.0862\n",
      "Epoch: [4/20]          || Step: [0/715]         || Average Validation Loss: 3.1593\n",
      "Epoch: [4/20]          || Step: [100/715]       || Average Validation Loss: 3.0426\n",
      "Epoch: [4/20]          || Step: [200/715]       || Average Validation Loss: 3.0517\n",
      "Epoch: [4/20]          || Step: [300/715]       || Average Validation Loss: 3.0521\n",
      "Epoch: [4/20]          || Step: [400/715]       || Average Validation Loss: 3.0494\n",
      "Epoch: [4/20]          || Step: [500/715]       || Average Validation Loss: 3.0512\n",
      "Epoch: [4/20]          || Step: [600/715]       || Average Validation Loss: 3.0486\n",
      "Epoch: [4/20]          || Step: [700/715]       || Average Validation Loss: 3.0512\n",
      "****************************************************************************************************\n",
      "Epoch: [4/20] || Training Loss = 3.09 || Validation Loss: 3.05 || Time: 90.970179\n",
      "****************************************************************************************************\n",
      "Epoch: [5/20]          || Step: [0/7971]        || Average Training Loss: 3.2760\n",
      "Epoch: [5/20]          || Step: [100/7971]      || Average Training Loss: 3.0696\n",
      "Epoch: [5/20]          || Step: [200/7971]      || Average Training Loss: 3.0759\n",
      "Epoch: [5/20]          || Step: [300/7971]      || Average Training Loss: 3.0797\n",
      "Epoch: [5/20]          || Step: [400/7971]      || Average Training Loss: 3.0840\n",
      "Epoch: [5/20]          || Step: [500/7971]      || Average Training Loss: 3.0798\n",
      "Epoch: [5/20]          || Step: [600/7971]      || Average Training Loss: 3.0797\n",
      "Epoch: [5/20]          || Step: [700/7971]      || Average Training Loss: 3.0800\n",
      "Epoch: [5/20]          || Step: [800/7971]      || Average Training Loss: 3.0785\n",
      "Epoch: [5/20]          || Step: [900/7971]      || Average Training Loss: 3.0779\n",
      "Epoch: [5/20]          || Step: [1000/7971]     || Average Training Loss: 3.0805\n",
      "Epoch: [5/20]          || Step: [1100/7971]     || Average Training Loss: 3.0825\n",
      "Epoch: [5/20]          || Step: [1200/7971]     || Average Training Loss: 3.0842\n",
      "Epoch: [5/20]          || Step: [1300/7971]     || Average Training Loss: 3.0844\n",
      "Epoch: [5/20]          || Step: [1400/7971]     || Average Training Loss: 3.0852\n",
      "Epoch: [5/20]          || Step: [1500/7971]     || Average Training Loss: 3.0848\n",
      "Epoch: [5/20]          || Step: [1600/7971]     || Average Training Loss: 3.0852\n",
      "Epoch: [5/20]          || Step: [1700/7971]     || Average Training Loss: 3.0862\n",
      "Epoch: [5/20]          || Step: [1800/7971]     || Average Training Loss: 3.0862\n",
      "Epoch: [5/20]          || Step: [1900/7971]     || Average Training Loss: 3.0854\n",
      "Epoch: [5/20]          || Step: [2000/7971]     || Average Training Loss: 3.0845\n",
      "Epoch: [5/20]          || Step: [2100/7971]     || Average Training Loss: 3.0845\n",
      "Epoch: [5/20]          || Step: [2200/7971]     || Average Training Loss: 3.0849\n",
      "Epoch: [5/20]          || Step: [2300/7971]     || Average Training Loss: 3.0833\n",
      "Epoch: [5/20]          || Step: [2400/7971]     || Average Training Loss: 3.0840\n",
      "Epoch: [5/20]          || Step: [2500/7971]     || Average Training Loss: 3.0842\n",
      "Epoch: [5/20]          || Step: [2600/7971]     || Average Training Loss: 3.0838\n",
      "Epoch: [5/20]          || Step: [2700/7971]     || Average Training Loss: 3.0850\n",
      "Epoch: [5/20]          || Step: [2800/7971]     || Average Training Loss: 3.0845\n",
      "Epoch: [5/20]          || Step: [2900/7971]     || Average Training Loss: 3.0845\n",
      "Epoch: [5/20]          || Step: [3000/7971]     || Average Training Loss: 3.0837\n",
      "Epoch: [5/20]          || Step: [3100/7971]     || Average Training Loss: 3.0836\n",
      "Epoch: [5/20]          || Step: [3200/7971]     || Average Training Loss: 3.0832\n",
      "Epoch: [5/20]          || Step: [3300/7971]     || Average Training Loss: 3.0838\n",
      "Epoch: [5/20]          || Step: [3400/7971]     || Average Training Loss: 3.0832\n",
      "Epoch: [5/20]          || Step: [3500/7971]     || Average Training Loss: 3.0828\n",
      "Epoch: [5/20]          || Step: [3600/7971]     || Average Training Loss: 3.0828\n",
      "Epoch: [5/20]          || Step: [3700/7971]     || Average Training Loss: 3.0828\n",
      "Epoch: [5/20]          || Step: [3800/7971]     || Average Training Loss: 3.0834\n",
      "Epoch: [5/20]          || Step: [3900/7971]     || Average Training Loss: 3.0834\n",
      "Epoch: [5/20]          || Step: [4000/7971]     || Average Training Loss: 3.0831\n",
      "Epoch: [5/20]          || Step: [4100/7971]     || Average Training Loss: 3.0827\n",
      "Epoch: [5/20]          || Step: [4200/7971]     || Average Training Loss: 3.0826\n",
      "Epoch: [5/20]          || Step: [4300/7971]     || Average Training Loss: 3.0824\n",
      "Epoch: [5/20]          || Step: [4400/7971]     || Average Training Loss: 3.0823\n",
      "Epoch: [5/20]          || Step: [4500/7971]     || Average Training Loss: 3.0822\n",
      "Epoch: [5/20]          || Step: [4600/7971]     || Average Training Loss: 3.0821\n",
      "Epoch: [5/20]          || Step: [4700/7971]     || Average Training Loss: 3.0817\n",
      "Epoch: [5/20]          || Step: [4800/7971]     || Average Training Loss: 3.0817\n",
      "Epoch: [5/20]          || Step: [4900/7971]     || Average Training Loss: 3.0816\n",
      "Epoch: [5/20]          || Step: [5000/7971]     || Average Training Loss: 3.0819\n",
      "Epoch: [5/20]          || Step: [5100/7971]     || Average Training Loss: 3.0819\n",
      "Epoch: [5/20]          || Step: [5200/7971]     || Average Training Loss: 3.0818\n",
      "Epoch: [5/20]          || Step: [5300/7971]     || Average Training Loss: 3.0819\n",
      "Epoch: [5/20]          || Step: [5400/7971]     || Average Training Loss: 3.0819\n",
      "Epoch: [5/20]          || Step: [5500/7971]     || Average Training Loss: 3.0822\n",
      "Epoch: [5/20]          || Step: [5600/7971]     || Average Training Loss: 3.0825\n",
      "Epoch: [5/20]          || Step: [5700/7971]     || Average Training Loss: 3.0824\n",
      "Epoch: [5/20]          || Step: [5800/7971]     || Average Training Loss: 3.0821\n",
      "Epoch: [5/20]          || Step: [5900/7971]     || Average Training Loss: 3.0820\n",
      "Epoch: [5/20]          || Step: [6000/7971]     || Average Training Loss: 3.0820\n",
      "Epoch: [5/20]          || Step: [6100/7971]     || Average Training Loss: 3.0816\n",
      "Epoch: [5/20]          || Step: [6200/7971]     || Average Training Loss: 3.0815\n",
      "Epoch: [5/20]          || Step: [6300/7971]     || Average Training Loss: 3.0814\n",
      "Epoch: [5/20]          || Step: [6400/7971]     || Average Training Loss: 3.0812\n",
      "Epoch: [5/20]          || Step: [6500/7971]     || Average Training Loss: 3.0810\n",
      "Epoch: [5/20]          || Step: [6600/7971]     || Average Training Loss: 3.0809\n",
      "Epoch: [5/20]          || Step: [6700/7971]     || Average Training Loss: 3.0806\n",
      "Epoch: [5/20]          || Step: [6800/7971]     || Average Training Loss: 3.0807\n",
      "Epoch: [5/20]          || Step: [6900/7971]     || Average Training Loss: 3.0810\n",
      "Epoch: [5/20]          || Step: [7000/7971]     || Average Training Loss: 3.0808\n",
      "Epoch: [5/20]          || Step: [7100/7971]     || Average Training Loss: 3.0806\n",
      "Epoch: [5/20]          || Step: [7200/7971]     || Average Training Loss: 3.0806\n",
      "Epoch: [5/20]          || Step: [7300/7971]     || Average Training Loss: 3.0804\n",
      "Epoch: [5/20]          || Step: [7400/7971]     || Average Training Loss: 3.0802\n",
      "Epoch: [5/20]          || Step: [7500/7971]     || Average Training Loss: 3.0802\n",
      "Epoch: [5/20]          || Step: [7600/7971]     || Average Training Loss: 3.0804\n",
      "Epoch: [5/20]          || Step: [7700/7971]     || Average Training Loss: 3.0806\n",
      "Epoch: [5/20]          || Step: [7800/7971]     || Average Training Loss: 3.0806\n",
      "Epoch: [5/20]          || Step: [7900/7971]     || Average Training Loss: 3.0808\n",
      "Epoch: [5/20]          || Step: [0/715]         || Average Validation Loss: 3.0314\n",
      "Epoch: [5/20]          || Step: [100/715]       || Average Validation Loss: 3.0628\n",
      "Epoch: [5/20]          || Step: [200/715]       || Average Validation Loss: 3.0538\n",
      "Epoch: [5/20]          || Step: [300/715]       || Average Validation Loss: 3.0493\n",
      "Epoch: [5/20]          || Step: [400/715]       || Average Validation Loss: 3.0455\n",
      "Epoch: [5/20]          || Step: [500/715]       || Average Validation Loss: 3.0446\n",
      "Epoch: [5/20]          || Step: [600/715]       || Average Validation Loss: 3.0465\n",
      "Epoch: [5/20]          || Step: [700/715]       || Average Validation Loss: 3.0489\n",
      "****************************************************************************************************\n",
      "Epoch: [5/20] || Training Loss = 3.08 || Validation Loss: 3.05 || Time: 91.392191\n",
      "****************************************************************************************************\n",
      "Epoch: [6/20]          || Step: [0/7971]        || Average Training Loss: 2.9787\n",
      "Epoch: [6/20]          || Step: [100/7971]      || Average Training Loss: 3.0654\n",
      "Epoch: [6/20]          || Step: [200/7971]      || Average Training Loss: 3.0722\n",
      "Epoch: [6/20]          || Step: [300/7971]      || Average Training Loss: 3.0727\n",
      "Epoch: [6/20]          || Step: [400/7971]      || Average Training Loss: 3.0697\n",
      "Epoch: [6/20]          || Step: [500/7971]      || Average Training Loss: 3.0728\n",
      "Epoch: [6/20]          || Step: [600/7971]      || Average Training Loss: 3.0732\n",
      "Epoch: [6/20]          || Step: [700/7971]      || Average Training Loss: 3.0709\n",
      "Epoch: [6/20]          || Step: [800/7971]      || Average Training Loss: 3.0708\n",
      "Epoch: [6/20]          || Step: [900/7971]      || Average Training Loss: 3.0727\n",
      "Epoch: [6/20]          || Step: [1000/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [6/20]          || Step: [1100/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [1200/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [6/20]          || Step: [1300/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [6/20]          || Step: [1400/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [6/20]          || Step: [1500/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [6/20]          || Step: [1600/7971]     || Average Training Loss: 3.0754\n",
      "Epoch: [6/20]          || Step: [1700/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [1800/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [1900/7971]     || Average Training Loss: 3.0742\n",
      "Epoch: [6/20]          || Step: [2000/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [6/20]          || Step: [2100/7971]     || Average Training Loss: 3.0756\n",
      "Epoch: [6/20]          || Step: [2200/7971]     || Average Training Loss: 3.0755\n",
      "Epoch: [6/20]          || Step: [2300/7971]     || Average Training Loss: 3.0755\n",
      "Epoch: [6/20]          || Step: [2400/7971]     || Average Training Loss: 3.0752\n",
      "Epoch: [6/20]          || Step: [2500/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [6/20]          || Step: [2600/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [2700/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [2800/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [6/20]          || Step: [2900/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [6/20]          || Step: [3000/7971]     || Average Training Loss: 3.0740\n",
      "Epoch: [6/20]          || Step: [3100/7971]     || Average Training Loss: 3.0736\n",
      "Epoch: [6/20]          || Step: [3200/7971]     || Average Training Loss: 3.0736\n",
      "Epoch: [6/20]          || Step: [3300/7971]     || Average Training Loss: 3.0733\n",
      "Epoch: [6/20]          || Step: [3400/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [6/20]          || Step: [3500/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [6/20]          || Step: [3600/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [6/20]          || Step: [3700/7971]     || Average Training Loss: 3.0743\n",
      "Epoch: [6/20]          || Step: [3800/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [6/20]          || Step: [3900/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [6/20]          || Step: [4000/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [6/20]          || Step: [4100/7971]     || Average Training Loss: 3.0750\n",
      "Epoch: [6/20]          || Step: [4200/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [6/20]          || Step: [4300/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [6/20]          || Step: [4400/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [6/20]          || Step: [4500/7971]     || Average Training Loss: 3.0750\n",
      "Epoch: [6/20]          || Step: [4600/7971]     || Average Training Loss: 3.0752\n",
      "Epoch: [6/20]          || Step: [4700/7971]     || Average Training Loss: 3.0757\n",
      "Epoch: [6/20]          || Step: [4800/7971]     || Average Training Loss: 3.0758\n",
      "Epoch: [6/20]          || Step: [4900/7971]     || Average Training Loss: 3.0759\n",
      "Epoch: [6/20]          || Step: [5000/7971]     || Average Training Loss: 3.0759\n",
      "Epoch: [6/20]          || Step: [5100/7971]     || Average Training Loss: 3.0758\n",
      "Epoch: [6/20]          || Step: [5200/7971]     || Average Training Loss: 3.0759\n",
      "Epoch: [6/20]          || Step: [5300/7971]     || Average Training Loss: 3.0765\n",
      "Epoch: [6/20]          || Step: [5400/7971]     || Average Training Loss: 3.0767\n",
      "Epoch: [6/20]          || Step: [5500/7971]     || Average Training Loss: 3.0761\n",
      "Epoch: [6/20]          || Step: [5600/7971]     || Average Training Loss: 3.0765\n",
      "Epoch: [6/20]          || Step: [5700/7971]     || Average Training Loss: 3.0767\n",
      "Epoch: [6/20]          || Step: [5800/7971]     || Average Training Loss: 3.0768\n",
      "Epoch: [6/20]          || Step: [5900/7971]     || Average Training Loss: 3.0767\n",
      "Epoch: [6/20]          || Step: [6000/7971]     || Average Training Loss: 3.0768\n",
      "Epoch: [6/20]          || Step: [6100/7971]     || Average Training Loss: 3.0768\n",
      "Epoch: [6/20]          || Step: [6200/7971]     || Average Training Loss: 3.0769\n",
      "Epoch: [6/20]          || Step: [6300/7971]     || Average Training Loss: 3.0770\n",
      "Epoch: [6/20]          || Step: [6400/7971]     || Average Training Loss: 3.0767\n",
      "Epoch: [6/20]          || Step: [6500/7971]     || Average Training Loss: 3.0766\n",
      "Epoch: [6/20]          || Step: [6600/7971]     || Average Training Loss: 3.0770\n",
      "Epoch: [6/20]          || Step: [6700/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [6/20]          || Step: [6800/7971]     || Average Training Loss: 3.0774\n",
      "Epoch: [6/20]          || Step: [6900/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [6/20]          || Step: [7000/7971]     || Average Training Loss: 3.0774\n",
      "Epoch: [6/20]          || Step: [7100/7971]     || Average Training Loss: 3.0775\n",
      "Epoch: [6/20]          || Step: [7200/7971]     || Average Training Loss: 3.0771\n",
      "Epoch: [6/20]          || Step: [7300/7971]     || Average Training Loss: 3.0772\n",
      "Epoch: [6/20]          || Step: [7400/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [6/20]          || Step: [7500/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [6/20]          || Step: [7600/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [6/20]          || Step: [7700/7971]     || Average Training Loss: 3.0774\n",
      "Epoch: [6/20]          || Step: [7800/7971]     || Average Training Loss: 3.0770\n",
      "Epoch: [6/20]          || Step: [7900/7971]     || Average Training Loss: 3.0768\n",
      "Epoch: [6/20]          || Step: [0/715]         || Average Validation Loss: 3.2806\n",
      "Epoch: [6/20]          || Step: [100/715]       || Average Validation Loss: 3.0912\n",
      "Epoch: [6/20]          || Step: [200/715]       || Average Validation Loss: 3.0765\n",
      "Epoch: [6/20]          || Step: [300/715]       || Average Validation Loss: 3.0771\n",
      "Epoch: [6/20]          || Step: [400/715]       || Average Validation Loss: 3.0719\n",
      "Epoch: [6/20]          || Step: [500/715]       || Average Validation Loss: 3.0736\n",
      "Epoch: [6/20]          || Step: [600/715]       || Average Validation Loss: 3.0723\n",
      "Epoch: [6/20]          || Step: [700/715]       || Average Validation Loss: 3.0755\n",
      "****************************************************************************************************\n",
      "Epoch: [6/20] || Training Loss = 3.08 || Validation Loss: 3.08 || Time: 95.277626\n",
      "****************************************************************************************************\n",
      "Epoch: [7/20]          || Step: [0/7971]        || Average Training Loss: 2.9136\n",
      "Epoch: [7/20]          || Step: [100/7971]      || Average Training Loss: 3.0826\n",
      "Epoch: [7/20]          || Step: [200/7971]      || Average Training Loss: 3.0939\n",
      "Epoch: [7/20]          || Step: [300/7971]      || Average Training Loss: 3.0834\n",
      "Epoch: [7/20]          || Step: [400/7971]      || Average Training Loss: 3.0812\n",
      "Epoch: [7/20]          || Step: [500/7971]      || Average Training Loss: 3.0806\n",
      "Epoch: [7/20]          || Step: [600/7971]      || Average Training Loss: 3.0758\n",
      "Epoch: [7/20]          || Step: [700/7971]      || Average Training Loss: 3.0750\n",
      "Epoch: [7/20]          || Step: [800/7971]      || Average Training Loss: 3.0719\n",
      "Epoch: [7/20]          || Step: [900/7971]      || Average Training Loss: 3.0729\n",
      "Epoch: [7/20]          || Step: [1000/7971]     || Average Training Loss: 3.0742\n",
      "Epoch: [7/20]          || Step: [1100/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [7/20]          || Step: [1200/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [1300/7971]     || Average Training Loss: 3.0761\n",
      "Epoch: [7/20]          || Step: [1400/7971]     || Average Training Loss: 3.0760\n",
      "Epoch: [7/20]          || Step: [1500/7971]     || Average Training Loss: 3.0764\n",
      "Epoch: [7/20]          || Step: [1600/7971]     || Average Training Loss: 3.0763\n",
      "Epoch: [7/20]          || Step: [1700/7971]     || Average Training Loss: 3.0757\n",
      "Epoch: [7/20]          || Step: [1800/7971]     || Average Training Loss: 3.0756\n",
      "Epoch: [7/20]          || Step: [1900/7971]     || Average Training Loss: 3.0760\n",
      "Epoch: [7/20]          || Step: [2000/7971]     || Average Training Loss: 3.0757\n",
      "Epoch: [7/20]          || Step: [2100/7971]     || Average Training Loss: 3.0768\n",
      "Epoch: [7/20]          || Step: [2200/7971]     || Average Training Loss: 3.0769\n",
      "Epoch: [7/20]          || Step: [2300/7971]     || Average Training Loss: 3.0765\n",
      "Epoch: [7/20]          || Step: [2400/7971]     || Average Training Loss: 3.0775\n",
      "Epoch: [7/20]          || Step: [2500/7971]     || Average Training Loss: 3.0770\n",
      "Epoch: [7/20]          || Step: [2600/7971]     || Average Training Loss: 3.0766\n",
      "Epoch: [7/20]          || Step: [2700/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [7/20]          || Step: [2800/7971]     || Average Training Loss: 3.0750\n",
      "Epoch: [7/20]          || Step: [2900/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [3000/7971]     || Average Training Loss: 3.0752\n",
      "Epoch: [7/20]          || Step: [3100/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [7/20]          || Step: [3200/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [7/20]          || Step: [3300/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [3400/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [3500/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [3600/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [7/20]          || Step: [3700/7971]     || Average Training Loss: 3.0742\n",
      "Epoch: [7/20]          || Step: [3800/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [7/20]          || Step: [3900/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [7/20]          || Step: [4000/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [7/20]          || Step: [4100/7971]     || Average Training Loss: 3.0758\n",
      "Epoch: [7/20]          || Step: [4200/7971]     || Average Training Loss: 3.0758\n",
      "Epoch: [7/20]          || Step: [4300/7971]     || Average Training Loss: 3.0754\n",
      "Epoch: [7/20]          || Step: [4400/7971]     || Average Training Loss: 3.0754\n",
      "Epoch: [7/20]          || Step: [4500/7971]     || Average Training Loss: 3.0755\n",
      "Epoch: [7/20]          || Step: [4600/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [7/20]          || Step: [4700/7971]     || Average Training Loss: 3.0749\n",
      "Epoch: [7/20]          || Step: [4800/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [7/20]          || Step: [4900/7971]     || Average Training Loss: 3.0743\n",
      "Epoch: [7/20]          || Step: [5000/7971]     || Average Training Loss: 3.0743\n",
      "Epoch: [7/20]          || Step: [5100/7971]     || Average Training Loss: 3.0746\n",
      "Epoch: [7/20]          || Step: [5200/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [7/20]          || Step: [5300/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [5400/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [5500/7971]     || Average Training Loss: 3.0742\n",
      "Epoch: [7/20]          || Step: [5600/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [7/20]          || Step: [5700/7971]     || Average Training Loss: 3.0738\n",
      "Epoch: [7/20]          || Step: [5800/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [7/20]          || Step: [5900/7971]     || Average Training Loss: 3.0738\n",
      "Epoch: [7/20]          || Step: [6000/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [7/20]          || Step: [6100/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [6200/7971]     || Average Training Loss: 3.0743\n",
      "Epoch: [7/20]          || Step: [6300/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [6400/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [6500/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [6600/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [6700/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [7/20]          || Step: [6800/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [7/20]          || Step: [6900/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [7/20]          || Step: [7000/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [7/20]          || Step: [7100/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [7/20]          || Step: [7200/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [7/20]          || Step: [7300/7971]     || Average Training Loss: 3.0752\n",
      "Epoch: [7/20]          || Step: [7400/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [7/20]          || Step: [7500/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [7/20]          || Step: [7600/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [7/20]          || Step: [7700/7971]     || Average Training Loss: 3.0751\n",
      "Epoch: [7/20]          || Step: [7800/7971]     || Average Training Loss: 3.0749\n",
      "Epoch: [7/20]          || Step: [7900/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [7/20]          || Step: [0/715]         || Average Validation Loss: 3.0255\n",
      "Epoch: [7/20]          || Step: [100/715]       || Average Validation Loss: 3.0376\n",
      "Epoch: [7/20]          || Step: [200/715]       || Average Validation Loss: 3.0485\n",
      "Epoch: [7/20]          || Step: [300/715]       || Average Validation Loss: 3.0479\n",
      "Epoch: [7/20]          || Step: [400/715]       || Average Validation Loss: 3.0450\n",
      "Epoch: [7/20]          || Step: [500/715]       || Average Validation Loss: 3.0427\n",
      "Epoch: [7/20]          || Step: [600/715]       || Average Validation Loss: 3.0408\n",
      "Epoch: [7/20]          || Step: [700/715]       || Average Validation Loss: 3.0442\n",
      "****************************************************************************************************\n",
      "Epoch: [7/20] || Training Loss = 3.07 || Validation Loss: 3.04 || Time: 108.780173\n",
      "****************************************************************************************************\n",
      "Epoch: [8/20]          || Step: [0/7971]        || Average Training Loss: 2.9852\n",
      "Epoch: [8/20]          || Step: [100/7971]      || Average Training Loss: 3.0566\n",
      "Epoch: [8/20]          || Step: [200/7971]      || Average Training Loss: 3.0710\n",
      "Epoch: [8/20]          || Step: [300/7971]      || Average Training Loss: 3.0735\n",
      "Epoch: [8/20]          || Step: [400/7971]      || Average Training Loss: 3.0773\n",
      "Epoch: [8/20]          || Step: [500/7971]      || Average Training Loss: 3.0796\n",
      "Epoch: [8/20]          || Step: [600/7971]      || Average Training Loss: 3.0808\n",
      "Epoch: [8/20]          || Step: [700/7971]      || Average Training Loss: 3.0787\n",
      "Epoch: [8/20]          || Step: [800/7971]      || Average Training Loss: 3.0776\n",
      "Epoch: [8/20]          || Step: [900/7971]      || Average Training Loss: 3.0797\n",
      "Epoch: [8/20]          || Step: [1000/7971]     || Average Training Loss: 3.0802\n",
      "Epoch: [8/20]          || Step: [1100/7971]     || Average Training Loss: 3.0773\n",
      "Epoch: [8/20]          || Step: [1200/7971]     || Average Training Loss: 3.0760\n",
      "Epoch: [8/20]          || Step: [1300/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [8/20]          || Step: [1400/7971]     || Average Training Loss: 3.0759\n",
      "Epoch: [8/20]          || Step: [1500/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [8/20]          || Step: [1600/7971]     || Average Training Loss: 3.0747\n",
      "Epoch: [8/20]          || Step: [1700/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [8/20]          || Step: [1800/7971]     || Average Training Loss: 3.0729\n",
      "Epoch: [8/20]          || Step: [1900/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [2000/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [2100/7971]     || Average Training Loss: 3.0723\n",
      "Epoch: [8/20]          || Step: [2200/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [8/20]          || Step: [2300/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [8/20]          || Step: [2400/7971]     || Average Training Loss: 3.0737\n",
      "Epoch: [8/20]          || Step: [2500/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [8/20]          || Step: [2600/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [2700/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [8/20]          || Step: [2800/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [2900/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [8/20]          || Step: [3000/7971]     || Average Training Loss: 3.0738\n",
      "Epoch: [8/20]          || Step: [3100/7971]     || Average Training Loss: 3.0731\n",
      "Epoch: [8/20]          || Step: [3200/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [8/20]          || Step: [3300/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [3400/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [8/20]          || Step: [3500/7971]     || Average Training Loss: 3.0731\n",
      "Epoch: [8/20]          || Step: [3600/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [8/20]          || Step: [3700/7971]     || Average Training Loss: 3.0729\n",
      "Epoch: [8/20]          || Step: [3800/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [8/20]          || Step: [3900/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [8/20]          || Step: [4000/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [4100/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [4200/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [8/20]          || Step: [4300/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [8/20]          || Step: [4400/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [8/20]          || Step: [4500/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [8/20]          || Step: [4600/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [8/20]          || Step: [4700/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [8/20]          || Step: [4800/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [8/20]          || Step: [4900/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [8/20]          || Step: [5000/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [8/20]          || Step: [5100/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [5200/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [5300/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [5400/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [5500/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [5600/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [5700/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [5800/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [8/20]          || Step: [5900/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [8/20]          || Step: [6000/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [6100/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [6200/7971]     || Average Training Loss: 3.0731\n",
      "Epoch: [8/20]          || Step: [6300/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [6400/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [6500/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [6600/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [6700/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [6800/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [6900/7971]     || Average Training Loss: 3.0729\n",
      "Epoch: [8/20]          || Step: [7000/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [7100/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [8/20]          || Step: [7200/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [8/20]          || Step: [7300/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [8/20]          || Step: [7400/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [8/20]          || Step: [7500/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [7600/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [8/20]          || Step: [7700/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [8/20]          || Step: [7800/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [8/20]          || Step: [7900/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [8/20]          || Step: [0/715]         || Average Validation Loss: 3.2538\n",
      "Epoch: [8/20]          || Step: [100/715]       || Average Validation Loss: 3.0620\n",
      "Epoch: [8/20]          || Step: [200/715]       || Average Validation Loss: 3.0501\n",
      "Epoch: [8/20]          || Step: [300/715]       || Average Validation Loss: 3.0486\n",
      "Epoch: [8/20]          || Step: [400/715]       || Average Validation Loss: 3.0534\n",
      "Epoch: [8/20]          || Step: [500/715]       || Average Validation Loss: 3.0514\n",
      "Epoch: [8/20]          || Step: [600/715]       || Average Validation Loss: 3.0504\n",
      "Epoch: [8/20]          || Step: [700/715]       || Average Validation Loss: 3.0502\n",
      "****************************************************************************************************\n",
      "Epoch: [8/20] || Training Loss = 3.07 || Validation Loss: 3.05 || Time: 109.411215\n",
      "****************************************************************************************************\n",
      "Epoch: [9/20]          || Step: [0/7971]        || Average Training Loss: 3.2771\n",
      "Epoch: [9/20]          || Step: [100/7971]      || Average Training Loss: 3.0825\n",
      "Epoch: [9/20]          || Step: [200/7971]      || Average Training Loss: 3.0783\n",
      "Epoch: [9/20]          || Step: [300/7971]      || Average Training Loss: 3.0700\n",
      "Epoch: [9/20]          || Step: [400/7971]      || Average Training Loss: 3.0700\n",
      "Epoch: [9/20]          || Step: [500/7971]      || Average Training Loss: 3.0662\n",
      "Epoch: [9/20]          || Step: [600/7971]      || Average Training Loss: 3.0673\n",
      "Epoch: [9/20]          || Step: [700/7971]      || Average Training Loss: 3.0653\n",
      "Epoch: [9/20]          || Step: [800/7971]      || Average Training Loss: 3.0671\n",
      "Epoch: [9/20]          || Step: [900/7971]      || Average Training Loss: 3.0685\n",
      "Epoch: [9/20]          || Step: [1000/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [9/20]          || Step: [1100/7971]     || Average Training Loss: 3.0708\n",
      "Epoch: [9/20]          || Step: [1200/7971]     || Average Training Loss: 3.0704\n",
      "Epoch: [9/20]          || Step: [1300/7971]     || Average Training Loss: 3.0708\n",
      "Epoch: [9/20]          || Step: [1400/7971]     || Average Training Loss: 3.0704\n",
      "Epoch: [9/20]          || Step: [1500/7971]     || Average Training Loss: 3.0704\n",
      "Epoch: [9/20]          || Step: [1600/7971]     || Average Training Loss: 3.0699\n",
      "Epoch: [9/20]          || Step: [1700/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [1800/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [9/20]          || Step: [1900/7971]     || Average Training Loss: 3.0706\n",
      "Epoch: [9/20]          || Step: [2000/7971]     || Average Training Loss: 3.0699\n",
      "Epoch: [9/20]          || Step: [2100/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [9/20]          || Step: [2200/7971]     || Average Training Loss: 3.0699\n",
      "Epoch: [9/20]          || Step: [2300/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [9/20]          || Step: [2400/7971]     || Average Training Loss: 3.0705\n",
      "Epoch: [9/20]          || Step: [2500/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [9/20]          || Step: [2600/7971]     || Average Training Loss: 3.0717\n",
      "Epoch: [9/20]          || Step: [2700/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [9/20]          || Step: [2800/7971]     || Average Training Loss: 3.0710\n",
      "Epoch: [9/20]          || Step: [2900/7971]     || Average Training Loss: 3.0707\n",
      "Epoch: [9/20]          || Step: [3000/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [9/20]          || Step: [3100/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [3200/7971]     || Average Training Loss: 3.0717\n",
      "Epoch: [9/20]          || Step: [3300/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [9/20]          || Step: [3400/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [9/20]          || Step: [3500/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [3600/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [9/20]          || Step: [3700/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [3800/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [9/20]          || Step: [3900/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [4000/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [9/20]          || Step: [4100/7971]     || Average Training Loss: 3.0717\n",
      "Epoch: [9/20]          || Step: [4200/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [9/20]          || Step: [4300/7971]     || Average Training Loss: 3.0710\n",
      "Epoch: [9/20]          || Step: [4400/7971]     || Average Training Loss: 3.0712\n",
      "Epoch: [9/20]          || Step: [4500/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [9/20]          || Step: [4600/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [4700/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [9/20]          || Step: [4800/7971]     || Average Training Loss: 3.0712\n",
      "Epoch: [9/20]          || Step: [4900/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [9/20]          || Step: [5000/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [9/20]          || Step: [5100/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [9/20]          || Step: [5200/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [9/20]          || Step: [5300/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [5400/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [5500/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [9/20]          || Step: [5600/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [9/20]          || Step: [5700/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [9/20]          || Step: [5800/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [9/20]          || Step: [5900/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [9/20]          || Step: [6000/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [9/20]          || Step: [6100/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [9/20]          || Step: [6200/7971]     || Average Training Loss: 3.0723\n",
      "Epoch: [9/20]          || Step: [6300/7971]     || Average Training Loss: 3.0723\n",
      "Epoch: [9/20]          || Step: [6400/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [9/20]          || Step: [6500/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [9/20]          || Step: [6600/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [9/20]          || Step: [6700/7971]     || Average Training Loss: 3.0719\n",
      "Epoch: [9/20]          || Step: [6800/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [9/20]          || Step: [6900/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [9/20]          || Step: [7000/7971]     || Average Training Loss: 3.0723\n",
      "Epoch: [9/20]          || Step: [7100/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [9/20]          || Step: [7200/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [9/20]          || Step: [7300/7971]     || Average Training Loss: 3.0728\n",
      "Epoch: [9/20]          || Step: [7400/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [9/20]          || Step: [7500/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [7600/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [9/20]          || Step: [7700/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [7800/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [7900/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [9/20]          || Step: [0/715]         || Average Validation Loss: 2.9421\n",
      "Epoch: [9/20]          || Step: [100/715]       || Average Validation Loss: 3.0147\n",
      "Epoch: [9/20]          || Step: [200/715]       || Average Validation Loss: 3.0339\n",
      "Epoch: [9/20]          || Step: [300/715]       || Average Validation Loss: 3.0329\n",
      "Epoch: [9/20]          || Step: [400/715]       || Average Validation Loss: 3.0371\n",
      "Epoch: [9/20]          || Step: [500/715]       || Average Validation Loss: 3.0357\n",
      "Epoch: [9/20]          || Step: [600/715]       || Average Validation Loss: 3.0338\n",
      "Epoch: [9/20]          || Step: [700/715]       || Average Validation Loss: 3.0360\n",
      "****************************************************************************************************\n",
      "Epoch: [9/20] || Training Loss = 3.07 || Validation Loss: 3.04 || Time: 108.309496\n",
      "****************************************************************************************************\n",
      "Epoch: [10/20]         || Step: [0/7971]        || Average Training Loss: 3.1072\n",
      "Epoch: [10/20]         || Step: [100/7971]      || Average Training Loss: 3.0533\n",
      "Epoch: [10/20]         || Step: [200/7971]      || Average Training Loss: 3.0620\n",
      "Epoch: [10/20]         || Step: [300/7971]      || Average Training Loss: 3.0618\n",
      "Epoch: [10/20]         || Step: [400/7971]      || Average Training Loss: 3.0600\n",
      "Epoch: [10/20]         || Step: [500/7971]      || Average Training Loss: 3.0657\n",
      "Epoch: [10/20]         || Step: [600/7971]      || Average Training Loss: 3.0681\n",
      "Epoch: [10/20]         || Step: [700/7971]      || Average Training Loss: 3.0650\n",
      "Epoch: [10/20]         || Step: [800/7971]      || Average Training Loss: 3.0645\n",
      "Epoch: [10/20]         || Step: [900/7971]      || Average Training Loss: 3.0670\n",
      "Epoch: [10/20]         || Step: [1000/7971]     || Average Training Loss: 3.0677\n",
      "Epoch: [10/20]         || Step: [1100/7971]     || Average Training Loss: 3.0675\n",
      "Epoch: [10/20]         || Step: [1200/7971]     || Average Training Loss: 3.0656\n",
      "Epoch: [10/20]         || Step: [1300/7971]     || Average Training Loss: 3.0655\n",
      "Epoch: [10/20]         || Step: [1400/7971]     || Average Training Loss: 3.0674\n",
      "Epoch: [10/20]         || Step: [1500/7971]     || Average Training Loss: 3.0675\n",
      "Epoch: [10/20]         || Step: [1600/7971]     || Average Training Loss: 3.0681\n",
      "Epoch: [10/20]         || Step: [1700/7971]     || Average Training Loss: 3.0691\n",
      "Epoch: [10/20]         || Step: [1800/7971]     || Average Training Loss: 3.0701\n",
      "Epoch: [10/20]         || Step: [1900/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [10/20]         || Step: [2000/7971]     || Average Training Loss: 3.0708\n",
      "Epoch: [10/20]         || Step: [2100/7971]     || Average Training Loss: 3.0702\n",
      "Epoch: [10/20]         || Step: [2200/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [10/20]         || Step: [2300/7971]     || Average Training Loss: 3.0716\n",
      "Epoch: [10/20]         || Step: [2400/7971]     || Average Training Loss: 3.0707\n",
      "Epoch: [10/20]         || Step: [2500/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [10/20]         || Step: [2600/7971]     || Average Training Loss: 3.0710\n",
      "Epoch: [10/20]         || Step: [2700/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [10/20]         || Step: [2800/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [10/20]         || Step: [2900/7971]     || Average Training Loss: 3.0723\n",
      "Epoch: [10/20]         || Step: [3000/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [10/20]         || Step: [3100/7971]     || Average Training Loss: 3.0740\n",
      "Epoch: [10/20]         || Step: [3200/7971]     || Average Training Loss: 3.0734\n",
      "Epoch: [10/20]         || Step: [3300/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [10/20]         || Step: [3400/7971]     || Average Training Loss: 3.0748\n",
      "Epoch: [10/20]         || Step: [3500/7971]     || Average Training Loss: 3.0741\n",
      "Epoch: [10/20]         || Step: [3600/7971]     || Average Training Loss: 3.0736\n",
      "Epoch: [10/20]         || Step: [3700/7971]     || Average Training Loss: 3.0738\n",
      "Epoch: [10/20]         || Step: [3800/7971]     || Average Training Loss: 3.0737\n",
      "Epoch: [10/20]         || Step: [3900/7971]     || Average Training Loss: 3.0735\n",
      "Epoch: [10/20]         || Step: [4000/7971]     || Average Training Loss: 3.0725\n",
      "Epoch: [10/20]         || Step: [4100/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [10/20]         || Step: [4200/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [10/20]         || Step: [4300/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [10/20]         || Step: [4400/7971]     || Average Training Loss: 3.0729\n",
      "Epoch: [10/20]         || Step: [4500/7971]     || Average Training Loss: 3.0732\n",
      "Epoch: [10/20]         || Step: [4600/7971]     || Average Training Loss: 3.0729\n",
      "Epoch: [10/20]         || Step: [4700/7971]     || Average Training Loss: 3.0727\n",
      "Epoch: [10/20]         || Step: [4800/7971]     || Average Training Loss: 3.0726\n",
      "Epoch: [10/20]         || Step: [4900/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [10/20]         || Step: [5000/7971]     || Average Training Loss: 3.0724\n",
      "Epoch: [10/20]         || Step: [5100/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [10/20]         || Step: [5200/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [10/20]         || Step: [5300/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [10/20]         || Step: [5400/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [10/20]         || Step: [5500/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [10/20]         || Step: [5600/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [10/20]         || Step: [5700/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [10/20]         || Step: [5800/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [5900/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [6000/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [6100/7971]     || Average Training Loss: 3.0716\n",
      "Epoch: [10/20]         || Step: [6200/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [6300/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [10/20]         || Step: [6400/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [10/20]         || Step: [6500/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [10/20]         || Step: [6600/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [6700/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [10/20]         || Step: [6800/7971]     || Average Training Loss: 3.0710\n",
      "Epoch: [10/20]         || Step: [6900/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [10/20]         || Step: [7000/7971]     || Average Training Loss: 3.0709\n",
      "Epoch: [10/20]         || Step: [7100/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [10/20]         || Step: [7200/7971]     || Average Training Loss: 3.0712\n",
      "Epoch: [10/20]         || Step: [7300/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [10/20]         || Step: [7400/7971]     || Average Training Loss: 3.0712\n",
      "Epoch: [10/20]         || Step: [7500/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [10/20]         || Step: [7600/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [7700/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [10/20]         || Step: [7800/7971]     || Average Training Loss: 3.0714\n",
      "Epoch: [10/20]         || Step: [7900/7971]     || Average Training Loss: 3.0712\n",
      "Epoch: [10/20]         || Step: [0/715]         || Average Validation Loss: 3.2017\n",
      "Epoch: [10/20]         || Step: [100/715]       || Average Validation Loss: 3.0443\n",
      "Epoch: [10/20]         || Step: [200/715]       || Average Validation Loss: 3.0362\n",
      "Epoch: [10/20]         || Step: [300/715]       || Average Validation Loss: 3.0486\n",
      "Epoch: [10/20]         || Step: [400/715]       || Average Validation Loss: 3.0585\n",
      "Epoch: [10/20]         || Step: [500/715]       || Average Validation Loss: 3.0546\n",
      "Epoch: [10/20]         || Step: [600/715]       || Average Validation Loss: 3.0515\n",
      "Epoch: [10/20]         || Step: [700/715]       || Average Validation Loss: 3.0534\n",
      "****************************************************************************************************\n",
      "Epoch: [10/20] || Training Loss = 3.07 || Validation Loss: 3.05 || Time: 133.247122\n",
      "****************************************************************************************************\n",
      "Epoch: [11/20]         || Step: [0/7971]        || Average Training Loss: 3.0784\n",
      "Epoch: [11/20]         || Step: [100/7971]      || Average Training Loss: 3.0728\n",
      "Epoch: [11/20]         || Step: [200/7971]      || Average Training Loss: 3.0702\n",
      "Epoch: [11/20]         || Step: [300/7971]      || Average Training Loss: 3.0680\n",
      "Epoch: [11/20]         || Step: [400/7971]      || Average Training Loss: 3.0716\n",
      "Epoch: [11/20]         || Step: [500/7971]      || Average Training Loss: 3.0733\n",
      "Epoch: [11/20]         || Step: [600/7971]      || Average Training Loss: 3.0706\n",
      "Epoch: [11/20]         || Step: [700/7971]      || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [800/7971]      || Average Training Loss: 3.0721\n",
      "Epoch: [11/20]         || Step: [900/7971]      || Average Training Loss: 3.0723\n",
      "Epoch: [11/20]         || Step: [1000/7971]     || Average Training Loss: 3.0719\n",
      "Epoch: [11/20]         || Step: [1100/7971]     || Average Training Loss: 3.0731\n",
      "Epoch: [11/20]         || Step: [1200/7971]     || Average Training Loss: 3.0740\n",
      "Epoch: [11/20]         || Step: [1300/7971]     || Average Training Loss: 3.0749\n",
      "Epoch: [11/20]         || Step: [1400/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [11/20]         || Step: [1500/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [11/20]         || Step: [1600/7971]     || Average Training Loss: 3.0742\n",
      "Epoch: [11/20]         || Step: [1700/7971]     || Average Training Loss: 3.0762\n",
      "Epoch: [11/20]         || Step: [1800/7971]     || Average Training Loss: 3.0754\n",
      "Epoch: [11/20]         || Step: [1900/7971]     || Average Training Loss: 3.0745\n",
      "Epoch: [11/20]         || Step: [2000/7971]     || Average Training Loss: 3.0744\n",
      "Epoch: [11/20]         || Step: [2100/7971]     || Average Training Loss: 3.0753\n",
      "Epoch: [11/20]         || Step: [2200/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [11/20]         || Step: [2300/7971]     || Average Training Loss: 3.0738\n",
      "Epoch: [11/20]         || Step: [2400/7971]     || Average Training Loss: 3.0739\n",
      "Epoch: [11/20]         || Step: [2500/7971]     || Average Training Loss: 3.0736\n",
      "Epoch: [11/20]         || Step: [2600/7971]     || Average Training Loss: 3.0730\n",
      "Epoch: [11/20]         || Step: [2700/7971]     || Average Training Loss: 3.0722\n",
      "Epoch: [11/20]         || Step: [2800/7971]     || Average Training Loss: 3.0717\n",
      "Epoch: [11/20]         || Step: [2900/7971]     || Average Training Loss: 3.0718\n",
      "Epoch: [11/20]         || Step: [3000/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [11/20]         || Step: [3100/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [11/20]         || Step: [3200/7971]     || Average Training Loss: 3.0719\n",
      "Epoch: [11/20]         || Step: [3300/7971]     || Average Training Loss: 3.0716\n",
      "Epoch: [11/20]         || Step: [3400/7971]     || Average Training Loss: 3.0719\n",
      "Epoch: [11/20]         || Step: [3500/7971]     || Average Training Loss: 3.0720\n",
      "Epoch: [11/20]         || Step: [3600/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [11/20]         || Step: [3700/7971]     || Average Training Loss: 3.0715\n",
      "Epoch: [11/20]         || Step: [3800/7971]     || Average Training Loss: 3.0719\n",
      "Epoch: [11/20]         || Step: [3900/7971]     || Average Training Loss: 3.0721\n",
      "Epoch: [11/20]         || Step: [4000/7971]     || Average Training Loss: 3.0717\n",
      "Epoch: [11/20]         || Step: [4100/7971]     || Average Training Loss: 3.0713\n",
      "Epoch: [11/20]         || Step: [4200/7971]     || Average Training Loss: 3.0708\n",
      "Epoch: [11/20]         || Step: [4300/7971]     || Average Training Loss: 3.0707\n",
      "Epoch: [11/20]         || Step: [4400/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [4500/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [11/20]         || Step: [4600/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [4700/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [4800/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [11/20]         || Step: [4900/7971]     || Average Training Loss: 3.0688\n",
      "Epoch: [11/20]         || Step: [5000/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [11/20]         || Step: [5100/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [11/20]         || Step: [5200/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [11/20]         || Step: [5300/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [11/20]         || Step: [5400/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [11/20]         || Step: [5500/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [11/20]         || Step: [5600/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [11/20]         || Step: [5700/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [11/20]         || Step: [5800/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [11/20]         || Step: [5900/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [11/20]         || Step: [6000/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [11/20]         || Step: [6100/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [11/20]         || Step: [6200/7971]     || Average Training Loss: 3.0691\n",
      "Epoch: [11/20]         || Step: [6300/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [11/20]         || Step: [6400/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [11/20]         || Step: [6500/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [11/20]         || Step: [6600/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [11/20]         || Step: [6700/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [11/20]         || Step: [6800/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [11/20]         || Step: [6900/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [11/20]         || Step: [7000/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [11/20]         || Step: [7100/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [11/20]         || Step: [7200/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [7300/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [11/20]         || Step: [7400/7971]     || Average Training Loss: 3.0701\n",
      "Epoch: [11/20]         || Step: [7500/7971]     || Average Training Loss: 3.0702\n",
      "Epoch: [11/20]         || Step: [7600/7971]     || Average Training Loss: 3.0702\n",
      "Epoch: [11/20]         || Step: [7700/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [11/20]         || Step: [7800/7971]     || Average Training Loss: 3.0706\n",
      "Epoch: [11/20]         || Step: [7900/7971]     || Average Training Loss: 3.0707\n",
      "Epoch: [11/20]         || Step: [0/715]         || Average Validation Loss: 3.3300\n",
      "Epoch: [11/20]         || Step: [100/715]       || Average Validation Loss: 3.0574\n",
      "Epoch: [11/20]         || Step: [200/715]       || Average Validation Loss: 3.0520\n",
      "Epoch: [11/20]         || Step: [300/715]       || Average Validation Loss: 3.0568\n",
      "Epoch: [11/20]         || Step: [400/715]       || Average Validation Loss: 3.0532\n",
      "Epoch: [11/20]         || Step: [500/715]       || Average Validation Loss: 3.0504\n",
      "Epoch: [11/20]         || Step: [600/715]       || Average Validation Loss: 3.0484\n",
      "Epoch: [11/20]         || Step: [700/715]       || Average Validation Loss: 3.0494\n",
      "****************************************************************************************************\n",
      "Epoch: [11/20] || Training Loss = 3.07 || Validation Loss: 3.05 || Time: 97.178168\n",
      "****************************************************************************************************\n",
      "Epoch: [12/20]         || Step: [0/7971]        || Average Training Loss: 3.1393\n",
      "Epoch: [12/20]         || Step: [100/7971]      || Average Training Loss: 3.0681\n",
      "Epoch: [12/20]         || Step: [200/7971]      || Average Training Loss: 3.0665\n",
      "Epoch: [12/20]         || Step: [300/7971]      || Average Training Loss: 3.0712\n",
      "Epoch: [12/20]         || Step: [400/7971]      || Average Training Loss: 3.0721\n",
      "Epoch: [12/20]         || Step: [500/7971]      || Average Training Loss: 3.0739\n",
      "Epoch: [12/20]         || Step: [600/7971]      || Average Training Loss: 3.0717\n",
      "Epoch: [12/20]         || Step: [700/7971]      || Average Training Loss: 3.0714\n",
      "Epoch: [12/20]         || Step: [800/7971]      || Average Training Loss: 3.0707\n",
      "Epoch: [12/20]         || Step: [900/7971]      || Average Training Loss: 3.0701\n",
      "Epoch: [12/20]         || Step: [1000/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [12/20]         || Step: [1100/7971]     || Average Training Loss: 3.0680\n",
      "Epoch: [12/20]         || Step: [1200/7971]     || Average Training Loss: 3.0685\n",
      "Epoch: [12/20]         || Step: [1300/7971]     || Average Training Loss: 3.0683\n",
      "Epoch: [12/20]         || Step: [1400/7971]     || Average Training Loss: 3.0684\n",
      "Epoch: [12/20]         || Step: [1500/7971]     || Average Training Loss: 3.0683\n",
      "Epoch: [12/20]         || Step: [1600/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [12/20]         || Step: [1700/7971]     || Average Training Loss: 3.0688\n",
      "Epoch: [12/20]         || Step: [1800/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [12/20]         || Step: [1900/7971]     || Average Training Loss: 3.0691\n",
      "Epoch: [12/20]         || Step: [2000/7971]     || Average Training Loss: 3.0680\n",
      "Epoch: [12/20]         || Step: [2100/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [12/20]         || Step: [2200/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [12/20]         || Step: [2300/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [12/20]         || Step: [2400/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [12/20]         || Step: [2500/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [2600/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [12/20]         || Step: [2700/7971]     || Average Training Loss: 3.0701\n",
      "Epoch: [12/20]         || Step: [2800/7971]     || Average Training Loss: 3.0702\n",
      "Epoch: [12/20]         || Step: [2900/7971]     || Average Training Loss: 3.0700\n",
      "Epoch: [12/20]         || Step: [3000/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [12/20]         || Step: [3100/7971]     || Average Training Loss: 3.0701\n",
      "Epoch: [12/20]         || Step: [3200/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [12/20]         || Step: [3300/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [12/20]         || Step: [3400/7971]     || Average Training Loss: 3.0686\n",
      "Epoch: [12/20]         || Step: [3500/7971]     || Average Training Loss: 3.0685\n",
      "Epoch: [12/20]         || Step: [3600/7971]     || Average Training Loss: 3.0679\n",
      "Epoch: [12/20]         || Step: [3700/7971]     || Average Training Loss: 3.0678\n",
      "Epoch: [12/20]         || Step: [3800/7971]     || Average Training Loss: 3.0671\n",
      "Epoch: [12/20]         || Step: [3900/7971]     || Average Training Loss: 3.0678\n",
      "Epoch: [12/20]         || Step: [4000/7971]     || Average Training Loss: 3.0676\n",
      "Epoch: [12/20]         || Step: [4100/7971]     || Average Training Loss: 3.0684\n",
      "Epoch: [12/20]         || Step: [4200/7971]     || Average Training Loss: 3.0682\n",
      "Epoch: [12/20]         || Step: [4300/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [12/20]         || Step: [4400/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [12/20]         || Step: [4500/7971]     || Average Training Loss: 3.0688\n",
      "Epoch: [12/20]         || Step: [4600/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [12/20]         || Step: [4700/7971]     || Average Training Loss: 3.0685\n",
      "Epoch: [12/20]         || Step: [4800/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [12/20]         || Step: [4900/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [12/20]         || Step: [5000/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [12/20]         || Step: [5100/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [12/20]         || Step: [5200/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [12/20]         || Step: [5300/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [12/20]         || Step: [5400/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [12/20]         || Step: [5500/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [5600/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [5700/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [5800/7971]     || Average Training Loss: 3.0690\n",
      "Epoch: [12/20]         || Step: [5900/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [12/20]         || Step: [6000/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [6100/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [12/20]         || Step: [6200/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [12/20]         || Step: [6300/7971]     || Average Training Loss: 3.0697\n",
      "Epoch: [12/20]         || Step: [6400/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [12/20]         || Step: [6500/7971]     || Average Training Loss: 3.0692\n",
      "Epoch: [12/20]         || Step: [6600/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [12/20]         || Step: [6700/7971]     || Average Training Loss: 3.0697\n",
      "Epoch: [12/20]         || Step: [6800/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [6900/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [7000/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [7100/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [12/20]         || Step: [7200/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [7300/7971]     || Average Training Loss: 3.0694\n",
      "Epoch: [12/20]         || Step: [7400/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [12/20]         || Step: [7500/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [12/20]         || Step: [7600/7971]     || Average Training Loss: 3.0697\n",
      "Epoch: [12/20]         || Step: [7700/7971]     || Average Training Loss: 3.0699\n",
      "Epoch: [12/20]         || Step: [7800/7971]     || Average Training Loss: 3.0699\n",
      "Epoch: [12/20]         || Step: [7900/7971]     || Average Training Loss: 3.0697\n",
      "Epoch: [12/20]         || Step: [0/715]         || Average Validation Loss: 3.3881\n",
      "Epoch: [12/20]         || Step: [100/715]       || Average Validation Loss: 3.0431\n",
      "Epoch: [12/20]         || Step: [200/715]       || Average Validation Loss: 3.0397\n",
      "Epoch: [12/20]         || Step: [300/715]       || Average Validation Loss: 3.0404\n",
      "Epoch: [12/20]         || Step: [400/715]       || Average Validation Loss: 3.0384\n",
      "Epoch: [12/20]         || Step: [500/715]       || Average Validation Loss: 3.0389\n",
      "Epoch: [12/20]         || Step: [600/715]       || Average Validation Loss: 3.0404\n",
      "Epoch: [12/20]         || Step: [700/715]       || Average Validation Loss: 3.0410\n",
      "****************************************************************************************************\n",
      "Epoch: [12/20] || Training Loss = 3.07 || Validation Loss: 3.04 || Time: 93.512446\n",
      "****************************************************************************************************\n",
      "Epoch: [13/20]         || Step: [0/7971]        || Average Training Loss: 3.0867\n",
      "Epoch: [13/20]         || Step: [100/7971]      || Average Training Loss: 3.0506\n",
      "Epoch: [13/20]         || Step: [200/7971]      || Average Training Loss: 3.0685\n",
      "Epoch: [13/20]         || Step: [300/7971]      || Average Training Loss: 3.0651\n",
      "Epoch: [13/20]         || Step: [400/7971]      || Average Training Loss: 3.0721\n",
      "Epoch: [13/20]         || Step: [500/7971]      || Average Training Loss: 3.0712\n",
      "Epoch: [13/20]         || Step: [600/7971]      || Average Training Loss: 3.0688\n",
      "Epoch: [13/20]         || Step: [700/7971]      || Average Training Loss: 3.0688\n",
      "Epoch: [13/20]         || Step: [800/7971]      || Average Training Loss: 3.0696\n",
      "Epoch: [13/20]         || Step: [900/7971]      || Average Training Loss: 3.0694\n",
      "Epoch: [13/20]         || Step: [1000/7971]     || Average Training Loss: 3.0695\n",
      "Epoch: [13/20]         || Step: [1100/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [13/20]         || Step: [1200/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [13/20]         || Step: [1300/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [13/20]         || Step: [1400/7971]     || Average Training Loss: 3.0703\n",
      "Epoch: [13/20]         || Step: [1500/7971]     || Average Training Loss: 3.0705\n",
      "Epoch: [13/20]         || Step: [1600/7971]     || Average Training Loss: 3.0706\n",
      "Epoch: [13/20]         || Step: [1700/7971]     || Average Training Loss: 3.0693\n",
      "Epoch: [13/20]         || Step: [1800/7971]     || Average Training Loss: 3.0698\n",
      "Epoch: [13/20]         || Step: [1900/7971]     || Average Training Loss: 3.0711\n",
      "Epoch: [13/20]         || Step: [2000/7971]     || Average Training Loss: 3.0704\n",
      "Epoch: [13/20]         || Step: [2100/7971]     || Average Training Loss: 3.0696\n",
      "Epoch: [13/20]         || Step: [2200/7971]     || Average Training Loss: 3.0688\n",
      "Epoch: [13/20]         || Step: [2300/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [13/20]         || Step: [2400/7971]     || Average Training Loss: 3.0680\n",
      "Epoch: [13/20]         || Step: [2500/7971]     || Average Training Loss: 3.0684\n",
      "Epoch: [13/20]         || Step: [2600/7971]     || Average Training Loss: 3.0679\n",
      "Epoch: [13/20]         || Step: [2700/7971]     || Average Training Loss: 3.0682\n",
      "Epoch: [13/20]         || Step: [2800/7971]     || Average Training Loss: 3.0686\n",
      "Epoch: [13/20]         || Step: [2900/7971]     || Average Training Loss: 3.0689\n",
      "Epoch: [13/20]         || Step: [3000/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [13/20]         || Step: [3100/7971]     || Average Training Loss: 3.0687\n",
      "Epoch: [13/20]         || Step: [3200/7971]     || Average Training Loss: 3.0691\n",
      "Epoch: [13/20]         || Step: [3300/7971]     || Average Training Loss: 3.0685\n",
      "Epoch: [13/20]         || Step: [3400/7971]     || Average Training Loss: 3.0676\n",
      "Epoch: [13/20]         || Step: [3500/7971]     || Average Training Loss: 3.0682\n",
      "Epoch: [13/20]         || Step: [3600/7971]     || Average Training Loss: 3.0678\n",
      "Epoch: [13/20]         || Step: [3700/7971]     || Average Training Loss: 3.0673\n",
      "Epoch: [13/20]         || Step: [3800/7971]     || Average Training Loss: 3.0670\n",
      "Epoch: [13/20]         || Step: [3900/7971]     || Average Training Loss: 3.0675\n",
      "Epoch: [13/20]         || Step: [4000/7971]     || Average Training Loss: 3.0675\n",
      "Epoch: [13/20]         || Step: [4100/7971]     || Average Training Loss: 3.0676\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ea319-7ec4-4d50-88d9-7f14d809724b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
