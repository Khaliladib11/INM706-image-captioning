{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09b4c42",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a3db4",
   "metadata": {},
   "source": [
    "This notebook narrows images to only those with sports in them. We then randomly sample these to get 10k / 2k / 2k. This gives us a slightly smaller vocab, but same size dataset. \n",
    "\n",
    "We only train on images including objects in the sports super-category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606af7ca",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fccfe284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_loader import get_loader\n",
    "from models import Encoder, Decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from data_prep_utils import *\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff406e5e",
   "metadata": {},
   "source": [
    "## Load train and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd484d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'sports_v1'\n",
    "SUPER_CATEGORIES = ['sports'] # should be list of eligible coco super categories, or None to include all images\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 1024  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "PRINT_EVERY = 100\n",
    "TOTAL_EPOCH = 25\n",
    "CHECKPOINT = '../model/model_sport_v1' # there is no v1 for sports:\n",
    "# v2 is consistent with previous tests as v2 parameters are shared across data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a247cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 10000 images\n",
      " val dataset has 2000 images\n",
      " test dataset has 938 images\n",
      "There are 50028 captions in the data set\n",
      "With FREQ_THRESHOLD = 5, vocab size is 2110\n"
     ]
    }
   ],
   "source": [
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=['sports'],\n",
    "                 max_train=10000, max_val=2000, max_test=2000,\n",
    "                 save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file=f'{CAPTIONS_NAME}_captions_train.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d977f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4115c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 391, Length of testing dataloader: 47\n",
      "Length of vocabulary: 2110\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_loader)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde1d50",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593a54d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b038a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4fc7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceaa922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fd73a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a62141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/25]          || Step: [0/391]         || Average Training Loss: 7.6540\n",
      "Epoch: [0/25]          || Step: [100/391]       || Average Training Loss: 4.4997\n",
      "Epoch: [0/25]          || Step: [200/391]       || Average Training Loss: 3.9466\n",
      "Epoch: [0/25]          || Step: [300/391]       || Average Training Loss: 3.6482\n",
      "Epoch: [0/25]          || Step: [0/47]          || Average Validation Loss: 2.7726\n",
      "****************************************************************************************************\n",
      "Epoch: [0/25] || Training Loss = 3.46 || Validation Loss: 2.70 || Time: 16.117596\n",
      "****************************************************************************************************\n",
      "Epoch: [1/25]          || Step: [0/391]         || Average Training Loss: 2.7278\n",
      "Epoch: [1/25]          || Step: [100/391]       || Average Training Loss: 2.6814\n",
      "Epoch: [1/25]          || Step: [200/391]       || Average Training Loss: 2.6398\n",
      "Epoch: [1/25]          || Step: [300/391]       || Average Training Loss: 2.6012\n",
      "Epoch: [1/25]          || Step: [0/47]          || Average Validation Loss: 2.5275\n",
      "****************************************************************************************************\n",
      "Epoch: [1/25] || Training Loss = 2.58 || Validation Loss: 2.44 || Time: 16.001164\n",
      "****************************************************************************************************\n",
      "Epoch: [2/25]          || Step: [0/391]         || Average Training Loss: 2.3190\n",
      "Epoch: [2/25]          || Step: [100/391]       || Average Training Loss: 2.4395\n",
      "Epoch: [2/25]          || Step: [200/391]       || Average Training Loss: 2.4364\n",
      "Epoch: [2/25]          || Step: [300/391]       || Average Training Loss: 2.4307\n",
      "Epoch: [2/25]          || Step: [0/47]          || Average Validation Loss: 2.3672\n",
      "****************************************************************************************************\n",
      "Epoch: [2/25] || Training Loss = 2.42 || Validation Loss: 2.35 || Time: 15.723138\n",
      "****************************************************************************************************\n",
      "Epoch: [3/25]          || Step: [0/391]         || Average Training Loss: 2.2887\n",
      "Epoch: [3/25]          || Step: [100/391]       || Average Training Loss: 2.3838\n",
      "Epoch: [3/25]          || Step: [200/391]       || Average Training Loss: 2.3880\n",
      "Epoch: [3/25]          || Step: [300/391]       || Average Training Loss: 2.3823\n",
      "Epoch: [3/25]          || Step: [0/47]          || Average Validation Loss: 2.3284\n",
      "****************************************************************************************************\n",
      "Epoch: [3/25] || Training Loss = 2.38 || Validation Loss: 2.32 || Time: 15.761048\n",
      "****************************************************************************************************\n",
      "Epoch: [4/25]          || Step: [0/391]         || Average Training Loss: 2.4462\n",
      "Epoch: [4/25]          || Step: [100/391]       || Average Training Loss: 2.3608\n",
      "Epoch: [4/25]          || Step: [200/391]       || Average Training Loss: 2.3600\n",
      "Epoch: [4/25]          || Step: [300/391]       || Average Training Loss: 2.3601\n",
      "Epoch: [4/25]          || Step: [0/47]          || Average Validation Loss: 2.0904\n",
      "****************************************************************************************************\n",
      "Epoch: [4/25] || Training Loss = 2.36 || Validation Loss: 2.30 || Time: 15.595607\n",
      "****************************************************************************************************\n",
      "Epoch: [5/25]          || Step: [0/391]         || Average Training Loss: 2.2163\n",
      "Epoch: [5/25]          || Step: [100/391]       || Average Training Loss: 2.3615\n",
      "Epoch: [5/25]          || Step: [200/391]       || Average Training Loss: 2.3507\n",
      "Epoch: [5/25]          || Step: [300/391]       || Average Training Loss: 2.3534\n",
      "Epoch: [5/25]          || Step: [0/47]          || Average Validation Loss: 2.3005\n",
      "****************************************************************************************************\n",
      "Epoch: [5/25] || Training Loss = 2.35 || Validation Loss: 2.30 || Time: 15.579444\n",
      "****************************************************************************************************\n",
      "Epoch: [6/25]          || Step: [0/391]         || Average Training Loss: 2.3524\n",
      "Epoch: [6/25]          || Step: [100/391]       || Average Training Loss: 2.3439\n",
      "Epoch: [6/25]          || Step: [200/391]       || Average Training Loss: 2.3404\n",
      "Epoch: [6/25]          || Step: [300/391]       || Average Training Loss: 2.3465\n",
      "Epoch: [6/25]          || Step: [0/47]          || Average Validation Loss: 2.1972\n",
      "****************************************************************************************************\n",
      "Epoch: [6/25] || Training Loss = 2.35 || Validation Loss: 2.31 || Time: 19.680840\n",
      "****************************************************************************************************\n",
      "Epoch: [7/25]          || Step: [0/391]         || Average Training Loss: 2.3279\n",
      "Epoch: [7/25]          || Step: [100/391]       || Average Training Loss: 2.3242\n",
      "Epoch: [7/25]          || Step: [200/391]       || Average Training Loss: 2.3338\n",
      "Epoch: [7/25]          || Step: [300/391]       || Average Training Loss: 2.3351\n",
      "Epoch: [7/25]          || Step: [0/47]          || Average Validation Loss: 2.3045\n",
      "****************************************************************************************************\n",
      "Epoch: [7/25] || Training Loss = 2.34 || Validation Loss: 2.30 || Time: 21.391604\n",
      "****************************************************************************************************\n",
      "Epoch: [8/25]          || Step: [0/391]         || Average Training Loss: 2.3180\n",
      "Epoch: [8/25]          || Step: [100/391]       || Average Training Loss: 2.3298\n",
      "Epoch: [8/25]          || Step: [200/391]       || Average Training Loss: 2.3371\n",
      "Epoch: [8/25]          || Step: [300/391]       || Average Training Loss: 2.3364\n",
      "Epoch: [8/25]          || Step: [0/47]          || Average Validation Loss: 2.2498\n",
      "****************************************************************************************************\n",
      "Epoch: [8/25] || Training Loss = 2.34 || Validation Loss: 2.34 || Time: 21.585696\n",
      "****************************************************************************************************\n",
      "Epoch: [9/25]          || Step: [0/391]         || Average Training Loss: 2.3170\n",
      "Epoch: [9/25]          || Step: [100/391]       || Average Training Loss: 2.3235\n",
      "Epoch: [9/25]          || Step: [200/391]       || Average Training Loss: 2.3287\n",
      "Epoch: [9/25]          || Step: [300/391]       || Average Training Loss: 2.3357\n",
      "Epoch: [9/25]          || Step: [0/47]          || Average Validation Loss: 2.2079\n",
      "****************************************************************************************************\n",
      "Epoch: [9/25] || Training Loss = 2.34 || Validation Loss: 2.31 || Time: 21.100249\n",
      "****************************************************************************************************\n",
      "Epoch: [10/25]         || Step: [0/391]         || Average Training Loss: 2.4239\n",
      "Epoch: [10/25]         || Step: [100/391]       || Average Training Loss: 2.3384\n",
      "Epoch: [10/25]         || Step: [200/391]       || Average Training Loss: 2.3364\n",
      "Epoch: [10/25]         || Step: [300/391]       || Average Training Loss: 2.3351\n",
      "Epoch: [10/25]         || Step: [0/47]          || Average Validation Loss: 2.3069\n",
      "****************************************************************************************************\n",
      "Epoch: [10/25] || Training Loss = 2.34 || Validation Loss: 2.31 || Time: 20.885389\n",
      "****************************************************************************************************\n",
      "Epoch: [11/25]         || Step: [0/391]         || Average Training Loss: 1.9858\n",
      "Epoch: [11/25]         || Step: [100/391]       || Average Training Loss: 2.3280\n",
      "Epoch: [11/25]         || Step: [200/391]       || Average Training Loss: 2.3302\n",
      "Epoch: [11/25]         || Step: [300/391]       || Average Training Loss: 2.3348\n",
      "Epoch: [11/25]         || Step: [0/47]          || Average Validation Loss: 2.1917\n",
      "****************************************************************************************************\n",
      "Epoch: [11/25] || Training Loss = 2.34 || Validation Loss: 2.31 || Time: 16.201842\n",
      "****************************************************************************************************\n",
      "Epoch: [12/25]         || Step: [0/391]         || Average Training Loss: 2.3934\n",
      "Epoch: [12/25]         || Step: [100/391]       || Average Training Loss: 2.3302\n",
      "Epoch: [12/25]         || Step: [200/391]       || Average Training Loss: 2.3299\n",
      "Epoch: [12/25]         || Step: [300/391]       || Average Training Loss: 2.3316\n",
      "Epoch: [12/25]         || Step: [0/47]          || Average Validation Loss: 2.1204\n",
      "****************************************************************************************************\n",
      "Epoch: [12/25] || Training Loss = 2.33 || Validation Loss: 2.28 || Time: 16.216083\n",
      "****************************************************************************************************\n",
      "Epoch: [13/25]         || Step: [0/391]         || Average Training Loss: 2.3069\n",
      "Epoch: [13/25]         || Step: [100/391]       || Average Training Loss: 2.3215\n",
      "Epoch: [13/25]         || Step: [200/391]       || Average Training Loss: 2.3204\n",
      "Epoch: [13/25]         || Step: [300/391]       || Average Training Loss: 2.3208\n",
      "Epoch: [13/25]         || Step: [0/47]          || Average Validation Loss: 2.2526\n",
      "****************************************************************************************************\n",
      "Epoch: [13/25] || Training Loss = 2.33 || Validation Loss: 2.29 || Time: 16.360369\n",
      "****************************************************************************************************\n",
      "Epoch: [14/25]         || Step: [0/391]         || Average Training Loss: 2.2241\n",
      "Epoch: [14/25]         || Step: [100/391]       || Average Training Loss: 2.3150\n",
      "Epoch: [14/25]         || Step: [200/391]       || Average Training Loss: 2.3191\n",
      "Epoch: [14/25]         || Step: [300/391]       || Average Training Loss: 2.3226\n",
      "Epoch: [14/25]         || Step: [0/47]          || Average Validation Loss: 2.2958\n",
      "****************************************************************************************************\n",
      "Epoch: [14/25] || Training Loss = 2.32 || Validation Loss: 2.28 || Time: 16.047169\n",
      "****************************************************************************************************\n",
      "Epoch: [15/25]         || Step: [0/391]         || Average Training Loss: 2.2492\n",
      "Epoch: [15/25]         || Step: [100/391]       || Average Training Loss: 2.3032\n",
      "Epoch: [15/25]         || Step: [200/391]       || Average Training Loss: 2.3121\n",
      "Epoch: [15/25]         || Step: [300/391]       || Average Training Loss: 2.3126\n",
      "Epoch: [15/25]         || Step: [0/47]          || Average Validation Loss: 2.2662\n",
      "****************************************************************************************************\n",
      "Epoch: [15/25] || Training Loss = 2.32 || Validation Loss: 2.28 || Time: 15.924113\n",
      "****************************************************************************************************\n",
      "Epoch: [16/25]         || Step: [0/391]         || Average Training Loss: 2.3151\n",
      "Epoch: [16/25]         || Step: [100/391]       || Average Training Loss: 2.3047\n",
      "Epoch: [16/25]         || Step: [200/391]       || Average Training Loss: 2.3068\n",
      "Epoch: [16/25]         || Step: [300/391]       || Average Training Loss: 2.3151\n",
      "Epoch: [16/25]         || Step: [0/47]          || Average Validation Loss: 2.2448\n",
      "****************************************************************************************************\n",
      "Epoch: [16/25] || Training Loss = 2.31 || Validation Loss: 2.27 || Time: 15.833187\n",
      "****************************************************************************************************\n",
      "Epoch: [17/25]         || Step: [0/391]         || Average Training Loss: 2.3292\n",
      "Epoch: [17/25]         || Step: [100/391]       || Average Training Loss: 2.3210\n",
      "Epoch: [17/25]         || Step: [200/391]       || Average Training Loss: 2.3082\n",
      "Epoch: [17/25]         || Step: [300/391]       || Average Training Loss: 2.3112\n",
      "Epoch: [17/25]         || Step: [0/47]          || Average Validation Loss: 2.4172\n",
      "****************************************************************************************************\n",
      "Epoch: [17/25] || Training Loss = 2.31 || Validation Loss: 2.29 || Time: 18.473055\n",
      "****************************************************************************************************\n",
      "Epoch: [18/25]         || Step: [0/391]         || Average Training Loss: 2.2205\n",
      "Epoch: [18/25]         || Step: [100/391]       || Average Training Loss: 2.2986\n",
      "Epoch: [18/25]         || Step: [200/391]       || Average Training Loss: 2.3066\n",
      "Epoch: [18/25]         || Step: [300/391]       || Average Training Loss: 2.3092\n",
      "Epoch: [18/25]         || Step: [0/47]          || Average Validation Loss: 2.2817\n",
      "****************************************************************************************************\n",
      "Epoch: [18/25] || Training Loss = 2.31 || Validation Loss: 2.29 || Time: 15.920079\n",
      "****************************************************************************************************\n",
      "Epoch: [19/25]         || Step: [0/391]         || Average Training Loss: 2.2374\n",
      "Epoch: [19/25]         || Step: [100/391]       || Average Training Loss: 2.3121\n",
      "Epoch: [19/25]         || Step: [200/391]       || Average Training Loss: 2.3094\n",
      "Epoch: [19/25]         || Step: [300/391]       || Average Training Loss: 2.3042\n",
      "Epoch: [19/25]         || Step: [0/47]          || Average Validation Loss: 2.2149\n",
      "****************************************************************************************************\n",
      "Epoch: [19/25] || Training Loss = 2.30 || Validation Loss: 2.27 || Time: 16.127930\n",
      "****************************************************************************************************\n",
      "Epoch: [20/25]         || Step: [0/391]         || Average Training Loss: 2.2195\n",
      "Epoch: [20/25]         || Step: [100/391]       || Average Training Loss: 2.2846\n",
      "Epoch: [20/25]         || Step: [200/391]       || Average Training Loss: 2.2979\n",
      "Epoch: [20/25]         || Step: [300/391]       || Average Training Loss: 2.2957\n",
      "Epoch: [20/25]         || Step: [0/47]          || Average Validation Loss: 2.2633\n",
      "****************************************************************************************************\n",
      "Epoch: [20/25] || Training Loss = 2.30 || Validation Loss: 2.28 || Time: 16.251460\n",
      "****************************************************************************************************\n",
      "Epoch: [21/25]         || Step: [0/391]         || Average Training Loss: 2.1266\n",
      "Epoch: [21/25]         || Step: [100/391]       || Average Training Loss: 2.2891\n",
      "Epoch: [21/25]         || Step: [200/391]       || Average Training Loss: 2.2919\n",
      "Epoch: [21/25]         || Step: [300/391]       || Average Training Loss: 2.2968\n",
      "Epoch: [21/25]         || Step: [0/47]          || Average Validation Loss: 2.3892\n",
      "****************************************************************************************************\n",
      "Epoch: [21/25] || Training Loss = 2.30 || Validation Loss: 2.28 || Time: 16.013433\n",
      "****************************************************************************************************\n",
      "Epoch: [22/25]         || Step: [0/391]         || Average Training Loss: 2.3619\n",
      "Epoch: [22/25]         || Step: [100/391]       || Average Training Loss: 2.2991\n",
      "Epoch: [22/25]         || Step: [200/391]       || Average Training Loss: 2.2914\n",
      "Epoch: [22/25]         || Step: [300/391]       || Average Training Loss: 2.2990\n",
      "Epoch: [22/25]         || Step: [0/47]          || Average Validation Loss: 2.2026\n",
      "****************************************************************************************************\n",
      "Epoch: [22/25] || Training Loss = 2.30 || Validation Loss: 2.26 || Time: 15.945114\n",
      "****************************************************************************************************\n",
      "Epoch: [23/25]         || Step: [0/391]         || Average Training Loss: 2.3868\n",
      "Epoch: [23/25]         || Step: [100/391]       || Average Training Loss: 2.2958\n",
      "Epoch: [23/25]         || Step: [200/391]       || Average Training Loss: 2.2969\n",
      "Epoch: [23/25]         || Step: [300/391]       || Average Training Loss: 2.2957\n",
      "Epoch: [23/25]         || Step: [0/47]          || Average Validation Loss: 2.3293\n",
      "****************************************************************************************************\n",
      "Epoch: [23/25] || Training Loss = 2.30 || Validation Loss: 2.26 || Time: 15.978912\n",
      "****************************************************************************************************\n",
      "Epoch: [24/25]         || Step: [0/391]         || Average Training Loss: 2.3617\n",
      "Epoch: [24/25]         || Step: [100/391]       || Average Training Loss: 2.2665\n",
      "Epoch: [24/25]         || Step: [200/391]       || Average Training Loss: 2.2865\n",
      "Epoch: [24/25]         || Step: [300/391]       || Average Training Loss: 2.2872\n",
      "Epoch: [24/25]         || Step: [0/47]          || Average Validation Loss: 2.4269\n",
      "****************************************************************************************************\n",
      "Epoch: [24/25] || Training Loss = 2.30 || Validation Loss: 2.28 || Time: 16.649065\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7047f3-1063-4bac-b8c5-9877356c2b94",
   "metadata": {},
   "source": [
    "## Train with different parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2725e8ce-74b1-4441-88c1-7859b2e5cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'sports_v5'\n",
    "SUPER_CATEGORIES = ['sports'] # should be list of eligible coco super categories, or None to include all images\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 1024  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "PRINT_EVERY = 20\n",
    "TOTAL_EPOCH = 50\n",
    "CHECKPOINT = '../model/model_sport_v5' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c94c4bc-066b-4b25-a79f-c9087a0ef40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 20199 images\n",
      " val dataset has 3019 images\n",
      " test dataset has 938 images\n",
      "There are 101047 captions in the data set\n",
      "With FREQ_THRESHOLD = 5, vocab size is 2988\n"
     ]
    }
   ],
   "source": [
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=['sports'],\n",
    "                 max_train=30000, max_val=5000, max_test=3000,\n",
    "                 save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file=f'{CAPTIONS_NAME}_captions_train.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21e50e87-395b-4537-b0d4-6c009a7c366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3d45097-cc03-489d-a643-eb8619b8a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 1579, Length of testing dataloader: 142\n",
      "Length of vocabulary: 2988\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_loader)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a327fdb-197a-4c17-88a8-f0e8dad329a6",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c8cceb-a9a2-484a-9e85-8a0335253ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8dbeb5d-c94b-4824-8d00-0232d23eb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2802bda3-380c-4e2d-b373-611e44abf7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be4fc33c-2111-4807-899c-dcda4c78fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aac54c-d306-4d98-bcea-16388463a9ad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e5b1c-1afb-47a6-b49d-5212bf37f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/50]          || Step: [0/1579]        || Average Training Loss: 7.9992\n",
      "Epoch: [0/50]          || Step: [20/1579]       || Average Training Loss: 5.8488\n",
      "Epoch: [0/50]          || Step: [40/1579]       || Average Training Loss: 5.2201\n",
      "Epoch: [0/50]          || Step: [60/1579]       || Average Training Loss: 4.9390\n",
      "Epoch: [0/50]          || Step: [80/1579]       || Average Training Loss: 4.7623\n",
      "Epoch: [0/50]          || Step: [100/1579]      || Average Training Loss: 4.6116\n",
      "Epoch: [0/50]          || Step: [120/1579]      || Average Training Loss: 4.4815\n",
      "Epoch: [0/50]          || Step: [140/1579]      || Average Training Loss: 4.3697\n",
      "Epoch: [0/50]          || Step: [160/1579]      || Average Training Loss: 4.2715\n",
      "Epoch: [0/50]          || Step: [180/1579]      || Average Training Loss: 4.1804\n",
      "Epoch: [0/50]          || Step: [200/1579]      || Average Training Loss: 4.0981\n",
      "Epoch: [0/50]          || Step: [220/1579]      || Average Training Loss: 4.0275\n",
      "Epoch: [0/50]          || Step: [240/1579]      || Average Training Loss: 3.9599\n",
      "Epoch: [0/50]          || Step: [260/1579]      || Average Training Loss: 3.9056\n",
      "Epoch: [0/50]          || Step: [280/1579]      || Average Training Loss: 3.8528\n",
      "Epoch: [0/50]          || Step: [300/1579]      || Average Training Loss: 3.8040\n",
      "Epoch: [0/50]          || Step: [320/1579]      || Average Training Loss: 3.7594\n",
      "Epoch: [0/50]          || Step: [340/1579]      || Average Training Loss: 3.7188\n",
      "Epoch: [0/50]          || Step: [360/1579]      || Average Training Loss: 3.6819\n",
      "Epoch: [0/50]          || Step: [380/1579]      || Average Training Loss: 3.6428\n",
      "Epoch: [0/50]          || Step: [400/1579]      || Average Training Loss: 3.6074\n",
      "Epoch: [0/50]          || Step: [420/1579]      || Average Training Loss: 3.5769\n",
      "Epoch: [0/50]          || Step: [440/1579]      || Average Training Loss: 3.5457\n",
      "Epoch: [0/50]          || Step: [460/1579]      || Average Training Loss: 3.5193\n",
      "Epoch: [0/50]          || Step: [480/1579]      || Average Training Loss: 3.4935\n",
      "Epoch: [0/50]          || Step: [500/1579]      || Average Training Loss: 3.4680\n",
      "Epoch: [0/50]          || Step: [520/1579]      || Average Training Loss: 3.4428\n",
      "Epoch: [0/50]          || Step: [540/1579]      || Average Training Loss: 3.4167\n",
      "Epoch: [0/50]          || Step: [560/1579]      || Average Training Loss: 3.3943\n",
      "Epoch: [0/50]          || Step: [580/1579]      || Average Training Loss: 3.3724\n",
      "Epoch: [0/50]          || Step: [600/1579]      || Average Training Loss: 3.3531\n",
      "Epoch: [0/50]          || Step: [620/1579]      || Average Training Loss: 3.3359\n",
      "Epoch: [0/50]          || Step: [640/1579]      || Average Training Loss: 3.3177\n",
      "Epoch: [0/50]          || Step: [660/1579]      || Average Training Loss: 3.2992\n",
      "Epoch: [0/50]          || Step: [680/1579]      || Average Training Loss: 3.2819\n",
      "Epoch: [0/50]          || Step: [700/1579]      || Average Training Loss: 3.2664\n",
      "Epoch: [0/50]          || Step: [720/1579]      || Average Training Loss: 3.2511\n",
      "Epoch: [0/50]          || Step: [740/1579]      || Average Training Loss: 3.2356\n",
      "Epoch: [0/50]          || Step: [760/1579]      || Average Training Loss: 3.2216\n",
      "Epoch: [0/50]          || Step: [780/1579]      || Average Training Loss: 3.2070\n",
      "Epoch: [0/50]          || Step: [800/1579]      || Average Training Loss: 3.1934\n",
      "Epoch: [0/50]          || Step: [820/1579]      || Average Training Loss: 3.1799\n",
      "Epoch: [0/50]          || Step: [840/1579]      || Average Training Loss: 3.1671\n",
      "Epoch: [0/50]          || Step: [860/1579]      || Average Training Loss: 3.1550\n",
      "Epoch: [0/50]          || Step: [880/1579]      || Average Training Loss: 3.1438\n",
      "Epoch: [0/50]          || Step: [900/1579]      || Average Training Loss: 3.1311\n",
      "Epoch: [0/50]          || Step: [920/1579]      || Average Training Loss: 3.1203\n",
      "Epoch: [0/50]          || Step: [940/1579]      || Average Training Loss: 3.1101\n",
      "Epoch: [0/50]          || Step: [960/1579]      || Average Training Loss: 3.0999\n",
      "Epoch: [0/50]          || Step: [980/1579]      || Average Training Loss: 3.0894\n",
      "Epoch: [0/50]          || Step: [1000/1579]     || Average Training Loss: 3.0802\n",
      "Epoch: [0/50]          || Step: [1020/1579]     || Average Training Loss: 3.0717\n",
      "Epoch: [0/50]          || Step: [1040/1579]     || Average Training Loss: 3.0629\n",
      "Epoch: [0/50]          || Step: [1060/1579]     || Average Training Loss: 3.0547\n",
      "Epoch: [0/50]          || Step: [1080/1579]     || Average Training Loss: 3.0466\n",
      "Epoch: [0/50]          || Step: [1100/1579]     || Average Training Loss: 3.0380\n",
      "Epoch: [0/50]          || Step: [1120/1579]     || Average Training Loss: 3.0304\n",
      "Epoch: [0/50]          || Step: [1140/1579]     || Average Training Loss: 3.0226\n",
      "Epoch: [0/50]          || Step: [1160/1579]     || Average Training Loss: 3.0157\n",
      "Epoch: [0/50]          || Step: [1180/1579]     || Average Training Loss: 3.0080\n",
      "Epoch: [0/50]          || Step: [1200/1579]     || Average Training Loss: 3.0003\n",
      "Epoch: [0/50]          || Step: [1220/1579]     || Average Training Loss: 2.9934\n",
      "Epoch: [0/50]          || Step: [1240/1579]     || Average Training Loss: 2.9866\n",
      "Epoch: [0/50]          || Step: [1260/1579]     || Average Training Loss: 2.9806\n",
      "Epoch: [0/50]          || Step: [1280/1579]     || Average Training Loss: 2.9744\n",
      "Epoch: [0/50]          || Step: [1300/1579]     || Average Training Loss: 2.9673\n",
      "Epoch: [0/50]          || Step: [1320/1579]     || Average Training Loss: 2.9603\n",
      "Epoch: [0/50]          || Step: [1340/1579]     || Average Training Loss: 2.9536\n",
      "Epoch: [0/50]          || Step: [1360/1579]     || Average Training Loss: 2.9471\n",
      "Epoch: [0/50]          || Step: [1380/1579]     || Average Training Loss: 2.9410\n",
      "Epoch: [0/50]          || Step: [1400/1579]     || Average Training Loss: 2.9348\n",
      "Epoch: [0/50]          || Step: [1420/1579]     || Average Training Loss: 2.9293\n",
      "Epoch: [0/50]          || Step: [1440/1579]     || Average Training Loss: 2.9246\n",
      "Epoch: [0/50]          || Step: [1460/1579]     || Average Training Loss: 2.9194\n",
      "Epoch: [0/50]          || Step: [1480/1579]     || Average Training Loss: 2.9143\n",
      "Epoch: [0/50]          || Step: [1500/1579]     || Average Training Loss: 2.9092\n",
      "Epoch: [0/50]          || Step: [1520/1579]     || Average Training Loss: 2.9049\n",
      "Epoch: [0/50]          || Step: [1540/1579]     || Average Training Loss: 2.8998\n",
      "Epoch: [0/50]          || Step: [1560/1579]     || Average Training Loss: 2.8953\n",
      "Epoch: [0/50]          || Step: [0/142]         || Average Validation Loss: 2.6116\n",
      "Epoch: [0/50]          || Step: [20/142]        || Average Validation Loss: 2.4920\n",
      "Epoch: [0/50]          || Step: [40/142]        || Average Validation Loss: 2.4837\n",
      "Epoch: [0/50]          || Step: [60/142]        || Average Validation Loss: 2.4754\n",
      "Epoch: [0/50]          || Step: [80/142]        || Average Validation Loss: 2.4794\n",
      "Epoch: [0/50]          || Step: [100/142]       || Average Validation Loss: 2.4836\n",
      "Epoch: [0/50]          || Step: [120/142]       || Average Validation Loss: 2.4926\n",
      "Epoch: [0/50]          || Step: [140/142]       || Average Validation Loss: 2.4953\n",
      "****************************************************************************************************\n",
      "Epoch: [0/50] || Training Loss = 2.89 || Validation Loss: 2.50 || Time: 22.766089\n",
      "****************************************************************************************************\n",
      "Epoch: [1/50]          || Step: [0/1579]        || Average Training Loss: 2.6074\n",
      "Epoch: [1/50]          || Step: [20/1579]       || Average Training Loss: 2.5818\n",
      "Epoch: [1/50]          || Step: [40/1579]       || Average Training Loss: 2.5588\n",
      "Epoch: [1/50]          || Step: [60/1579]       || Average Training Loss: 2.5331\n",
      "Epoch: [1/50]          || Step: [80/1579]       || Average Training Loss: 2.5211\n",
      "Epoch: [1/50]          || Step: [100/1579]      || Average Training Loss: 2.5352\n",
      "Epoch: [1/50]          || Step: [120/1579]      || Average Training Loss: 2.5322\n",
      "Epoch: [1/50]          || Step: [140/1579]      || Average Training Loss: 2.5205\n",
      "Epoch: [1/50]          || Step: [160/1579]      || Average Training Loss: 2.5212\n",
      "Epoch: [1/50]          || Step: [180/1579]      || Average Training Loss: 2.5183\n",
      "Epoch: [1/50]          || Step: [200/1579]      || Average Training Loss: 2.5197\n",
      "Epoch: [1/50]          || Step: [220/1579]      || Average Training Loss: 2.5196\n",
      "Epoch: [1/50]          || Step: [240/1579]      || Average Training Loss: 2.5131\n",
      "Epoch: [1/50]          || Step: [260/1579]      || Average Training Loss: 2.5113\n",
      "Epoch: [1/50]          || Step: [280/1579]      || Average Training Loss: 2.5057\n",
      "Epoch: [1/50]          || Step: [300/1579]      || Average Training Loss: 2.5043\n",
      "Epoch: [1/50]          || Step: [320/1579]      || Average Training Loss: 2.5040\n",
      "Epoch: [1/50]          || Step: [340/1579]      || Average Training Loss: 2.5019\n",
      "Epoch: [1/50]          || Step: [360/1579]      || Average Training Loss: 2.5033\n",
      "Epoch: [1/50]          || Step: [380/1579]      || Average Training Loss: 2.5045\n",
      "Epoch: [1/50]          || Step: [400/1579]      || Average Training Loss: 2.5050\n",
      "Epoch: [1/50]          || Step: [420/1579]      || Average Training Loss: 2.5027\n",
      "Epoch: [1/50]          || Step: [440/1579]      || Average Training Loss: 2.5025\n",
      "Epoch: [1/50]          || Step: [460/1579]      || Average Training Loss: 2.4998\n",
      "Epoch: [1/50]          || Step: [480/1579]      || Average Training Loss: 2.5002\n",
      "Epoch: [1/50]          || Step: [500/1579]      || Average Training Loss: 2.4985\n",
      "Epoch: [1/50]          || Step: [520/1579]      || Average Training Loss: 2.4996\n",
      "Epoch: [1/50]          || Step: [540/1579]      || Average Training Loss: 2.5000\n",
      "Epoch: [1/50]          || Step: [560/1579]      || Average Training Loss: 2.4981\n",
      "Epoch: [1/50]          || Step: [580/1579]      || Average Training Loss: 2.4976\n",
      "Epoch: [1/50]          || Step: [600/1579]      || Average Training Loss: 2.4980\n",
      "Epoch: [1/50]          || Step: [620/1579]      || Average Training Loss: 2.4985\n",
      "Epoch: [1/50]          || Step: [640/1579]      || Average Training Loss: 2.4976\n",
      "Epoch: [1/50]          || Step: [660/1579]      || Average Training Loss: 2.4980\n",
      "Epoch: [1/50]          || Step: [680/1579]      || Average Training Loss: 2.4995\n",
      "Epoch: [1/50]          || Step: [700/1579]      || Average Training Loss: 2.5003\n",
      "Epoch: [1/50]          || Step: [720/1579]      || Average Training Loss: 2.5004\n",
      "Epoch: [1/50]          || Step: [740/1579]      || Average Training Loss: 2.4988\n",
      "Epoch: [1/50]          || Step: [760/1579]      || Average Training Loss: 2.4987\n",
      "Epoch: [1/50]          || Step: [780/1579]      || Average Training Loss: 2.4984\n",
      "Epoch: [1/50]          || Step: [800/1579]      || Average Training Loss: 2.4982\n",
      "Epoch: [1/50]          || Step: [820/1579]      || Average Training Loss: 2.4982\n",
      "Epoch: [1/50]          || Step: [840/1579]      || Average Training Loss: 2.4979\n",
      "Epoch: [1/50]          || Step: [860/1579]      || Average Training Loss: 2.4965\n",
      "Epoch: [1/50]          || Step: [880/1579]      || Average Training Loss: 2.4960\n",
      "Epoch: [1/50]          || Step: [900/1579]      || Average Training Loss: 2.4966\n",
      "Epoch: [1/50]          || Step: [920/1579]      || Average Training Loss: 2.4959\n",
      "Epoch: [1/50]          || Step: [940/1579]      || Average Training Loss: 2.4961\n",
      "Epoch: [1/50]          || Step: [960/1579]      || Average Training Loss: 2.4972\n",
      "Epoch: [1/50]          || Step: [980/1579]      || Average Training Loss: 2.4977\n",
      "Epoch: [1/50]          || Step: [1000/1579]     || Average Training Loss: 2.4965\n",
      "Epoch: [1/50]          || Step: [1020/1579]     || Average Training Loss: 2.4963\n",
      "Epoch: [1/50]          || Step: [1040/1579]     || Average Training Loss: 2.4966\n",
      "Epoch: [1/50]          || Step: [1060/1579]     || Average Training Loss: 2.4953\n",
      "Epoch: [1/50]          || Step: [1080/1579]     || Average Training Loss: 2.4949\n",
      "Epoch: [1/50]          || Step: [1100/1579]     || Average Training Loss: 2.4940\n",
      "Epoch: [1/50]          || Step: [1120/1579]     || Average Training Loss: 2.4939\n",
      "Epoch: [1/50]          || Step: [1140/1579]     || Average Training Loss: 2.4943\n",
      "Epoch: [1/50]          || Step: [1160/1579]     || Average Training Loss: 2.4927\n",
      "Epoch: [1/50]          || Step: [1180/1579]     || Average Training Loss: 2.4923\n",
      "Epoch: [1/50]          || Step: [1200/1579]     || Average Training Loss: 2.4920\n",
      "Epoch: [1/50]          || Step: [1220/1579]     || Average Training Loss: 2.4925\n",
      "Epoch: [1/50]          || Step: [1240/1579]     || Average Training Loss: 2.4921\n",
      "Epoch: [1/50]          || Step: [1260/1579]     || Average Training Loss: 2.4923\n",
      "Epoch: [1/50]          || Step: [1280/1579]     || Average Training Loss: 2.4922\n",
      "Epoch: [1/50]          || Step: [1300/1579]     || Average Training Loss: 2.4923\n",
      "Epoch: [1/50]          || Step: [1320/1579]     || Average Training Loss: 2.4919\n",
      "Epoch: [1/50]          || Step: [1340/1579]     || Average Training Loss: 2.4917\n",
      "Epoch: [1/50]          || Step: [1360/1579]     || Average Training Loss: 2.4914\n",
      "Epoch: [1/50]          || Step: [1380/1579]     || Average Training Loss: 2.4906\n",
      "Epoch: [1/50]          || Step: [1400/1579]     || Average Training Loss: 2.4905\n",
      "Epoch: [1/50]          || Step: [1420/1579]     || Average Training Loss: 2.4908\n",
      "Epoch: [1/50]          || Step: [1440/1579]     || Average Training Loss: 2.4908\n",
      "Epoch: [1/50]          || Step: [1460/1579]     || Average Training Loss: 2.4918\n",
      "Epoch: [1/50]          || Step: [1480/1579]     || Average Training Loss: 2.4924\n",
      "Epoch: [1/50]          || Step: [1500/1579]     || Average Training Loss: 2.4927\n",
      "Epoch: [1/50]          || Step: [1520/1579]     || Average Training Loss: 2.4930\n",
      "Epoch: [1/50]          || Step: [1540/1579]     || Average Training Loss: 2.4926\n",
      "Epoch: [1/50]          || Step: [1560/1579]     || Average Training Loss: 2.4922\n",
      "Epoch: [1/50]          || Step: [0/142]         || Average Validation Loss: 2.8798\n",
      "Epoch: [1/50]          || Step: [20/142]        || Average Validation Loss: 2.7328\n",
      "Epoch: [1/50]          || Step: [40/142]        || Average Validation Loss: 2.7528\n",
      "Epoch: [1/50]          || Step: [60/142]        || Average Validation Loss: 2.7617\n",
      "Epoch: [1/50]          || Step: [80/142]        || Average Validation Loss: 2.7505\n",
      "Epoch: [1/50]          || Step: [100/142]       || Average Validation Loss: 2.7497\n",
      "Epoch: [1/50]          || Step: [120/142]       || Average Validation Loss: 2.7483\n",
      "Epoch: [1/50]          || Step: [140/142]       || Average Validation Loss: 2.7456\n",
      "****************************************************************************************************\n",
      "Epoch: [1/50] || Training Loss = 2.49 || Validation Loss: 2.75 || Time: 22.371514\n",
      "****************************************************************************************************\n",
      "Epoch: [2/50]          || Step: [0/1579]        || Average Training Loss: 2.8584\n",
      "Epoch: [2/50]          || Step: [20/1579]       || Average Training Loss: 2.7022\n",
      "Epoch: [2/50]          || Step: [40/1579]       || Average Training Loss: 2.6450\n",
      "Epoch: [2/50]          || Step: [60/1579]       || Average Training Loss: 2.6076\n",
      "Epoch: [2/50]          || Step: [80/1579]       || Average Training Loss: 2.6020\n",
      "Epoch: [2/50]          || Step: [100/1579]      || Average Training Loss: 2.5837\n",
      "Epoch: [2/50]          || Step: [120/1579]      || Average Training Loss: 2.5771\n",
      "Epoch: [2/50]          || Step: [140/1579]      || Average Training Loss: 2.5613\n",
      "Epoch: [2/50]          || Step: [160/1579]      || Average Training Loss: 2.5545\n",
      "Epoch: [2/50]          || Step: [180/1579]      || Average Training Loss: 2.5533\n",
      "Epoch: [2/50]          || Step: [200/1579]      || Average Training Loss: 2.5500\n",
      "Epoch: [2/50]          || Step: [220/1579]      || Average Training Loss: 2.5429\n",
      "Epoch: [2/50]          || Step: [240/1579]      || Average Training Loss: 2.5415\n",
      "Epoch: [2/50]          || Step: [260/1579]      || Average Training Loss: 2.5395\n",
      "Epoch: [2/50]          || Step: [280/1579]      || Average Training Loss: 2.5382\n",
      "Epoch: [2/50]          || Step: [300/1579]      || Average Training Loss: 2.5366\n",
      "Epoch: [2/50]          || Step: [320/1579]      || Average Training Loss: 2.5338\n",
      "Epoch: [2/50]          || Step: [340/1579]      || Average Training Loss: 2.5308\n",
      "Epoch: [2/50]          || Step: [360/1579]      || Average Training Loss: 2.5248\n",
      "Epoch: [2/50]          || Step: [380/1579]      || Average Training Loss: 2.5231\n",
      "Epoch: [2/50]          || Step: [400/1579]      || Average Training Loss: 2.5233\n",
      "Epoch: [2/50]          || Step: [420/1579]      || Average Training Loss: 2.5188\n",
      "Epoch: [2/50]          || Step: [440/1579]      || Average Training Loss: 2.5168\n",
      "Epoch: [2/50]          || Step: [460/1579]      || Average Training Loss: 2.5162\n",
      "Epoch: [2/50]          || Step: [480/1579]      || Average Training Loss: 2.5142\n",
      "Epoch: [2/50]          || Step: [500/1579]      || Average Training Loss: 2.5125\n",
      "Epoch: [2/50]          || Step: [520/1579]      || Average Training Loss: 2.5116\n",
      "Epoch: [2/50]          || Step: [540/1579]      || Average Training Loss: 2.5131\n",
      "Epoch: [2/50]          || Step: [560/1579]      || Average Training Loss: 2.5122\n",
      "Epoch: [2/50]          || Step: [580/1579]      || Average Training Loss: 2.5126\n",
      "Epoch: [2/50]          || Step: [600/1579]      || Average Training Loss: 2.5101\n",
      "Epoch: [2/50]          || Step: [620/1579]      || Average Training Loss: 2.5069\n",
      "Epoch: [2/50]          || Step: [640/1579]      || Average Training Loss: 2.5059\n",
      "Epoch: [2/50]          || Step: [660/1579]      || Average Training Loss: 2.5053\n",
      "Epoch: [2/50]          || Step: [680/1579]      || Average Training Loss: 2.5031\n",
      "Epoch: [2/50]          || Step: [700/1579]      || Average Training Loss: 2.5012\n",
      "Epoch: [2/50]          || Step: [720/1579]      || Average Training Loss: 2.5001\n",
      "Epoch: [2/50]          || Step: [740/1579]      || Average Training Loss: 2.4996\n",
      "Epoch: [2/50]          || Step: [760/1579]      || Average Training Loss: 2.4996\n",
      "Epoch: [2/50]          || Step: [780/1579]      || Average Training Loss: 2.4986\n",
      "Epoch: [2/50]          || Step: [800/1579]      || Average Training Loss: 2.4992\n",
      "Epoch: [2/50]          || Step: [820/1579]      || Average Training Loss: 2.4979\n",
      "Epoch: [2/50]          || Step: [840/1579]      || Average Training Loss: 2.4971\n",
      "Epoch: [2/50]          || Step: [860/1579]      || Average Training Loss: 2.4961\n",
      "Epoch: [2/50]          || Step: [880/1579]      || Average Training Loss: 2.4954\n",
      "Epoch: [2/50]          || Step: [900/1579]      || Average Training Loss: 2.4951\n",
      "Epoch: [2/50]          || Step: [920/1579]      || Average Training Loss: 2.4944\n",
      "Epoch: [2/50]          || Step: [940/1579]      || Average Training Loss: 2.4943\n",
      "Epoch: [2/50]          || Step: [960/1579]      || Average Training Loss: 2.4935\n",
      "Epoch: [2/50]          || Step: [980/1579]      || Average Training Loss: 2.4928\n",
      "Epoch: [2/50]          || Step: [1000/1579]     || Average Training Loss: 2.4921\n",
      "Epoch: [2/50]          || Step: [1020/1579]     || Average Training Loss: 2.4924\n",
      "Epoch: [2/50]          || Step: [1040/1579]     || Average Training Loss: 2.4918\n",
      "Epoch: [2/50]          || Step: [1060/1579]     || Average Training Loss: 2.4915\n",
      "Epoch: [2/50]          || Step: [1080/1579]     || Average Training Loss: 2.4909\n",
      "Epoch: [2/50]          || Step: [1100/1579]     || Average Training Loss: 2.4907\n",
      "Epoch: [2/50]          || Step: [1120/1579]     || Average Training Loss: 2.4910\n",
      "Epoch: [2/50]          || Step: [1140/1579]     || Average Training Loss: 2.4905\n",
      "Epoch: [2/50]          || Step: [1160/1579]     || Average Training Loss: 2.4893\n",
      "Epoch: [2/50]          || Step: [1180/1579]     || Average Training Loss: 2.4888\n",
      "Epoch: [2/50]          || Step: [1200/1579]     || Average Training Loss: 2.4880\n",
      "Epoch: [2/50]          || Step: [1220/1579]     || Average Training Loss: 2.4879\n",
      "Epoch: [2/50]          || Step: [1240/1579]     || Average Training Loss: 2.4877\n",
      "Epoch: [2/50]          || Step: [1260/1579]     || Average Training Loss: 2.4872\n",
      "Epoch: [2/50]          || Step: [1280/1579]     || Average Training Loss: 2.4864\n",
      "Epoch: [2/50]          || Step: [1300/1579]     || Average Training Loss: 2.4865\n",
      "Epoch: [2/50]          || Step: [1320/1579]     || Average Training Loss: 2.4861\n",
      "Epoch: [2/50]          || Step: [1340/1579]     || Average Training Loss: 2.4863\n",
      "Epoch: [2/50]          || Step: [1360/1579]     || Average Training Loss: 2.4861\n",
      "Epoch: [2/50]          || Step: [1380/1579]     || Average Training Loss: 2.4857\n",
      "Epoch: [2/50]          || Step: [1400/1579]     || Average Training Loss: 2.4853\n",
      "Epoch: [2/50]          || Step: [1420/1579]     || Average Training Loss: 2.4854\n",
      "Epoch: [2/50]          || Step: [1440/1579]     || Average Training Loss: 2.4851\n",
      "Epoch: [2/50]          || Step: [1460/1579]     || Average Training Loss: 2.4841\n",
      "Epoch: [2/50]          || Step: [1480/1579]     || Average Training Loss: 2.4834\n",
      "Epoch: [2/50]          || Step: [1500/1579]     || Average Training Loss: 2.4835\n",
      "Epoch: [2/50]          || Step: [1520/1579]     || Average Training Loss: 2.4831\n",
      "Epoch: [2/50]          || Step: [1540/1579]     || Average Training Loss: 2.4825\n",
      "Epoch: [2/50]          || Step: [1560/1579]     || Average Training Loss: 2.4817\n",
      "Epoch: [2/50]          || Step: [0/142]         || Average Validation Loss: 2.5879\n",
      "Epoch: [2/50]          || Step: [20/142]        || Average Validation Loss: 2.5282\n",
      "Epoch: [2/50]          || Step: [40/142]        || Average Validation Loss: 2.5258\n",
      "Epoch: [2/50]          || Step: [60/142]        || Average Validation Loss: 2.5157\n",
      "Epoch: [2/50]          || Step: [80/142]        || Average Validation Loss: 2.5116\n",
      "Epoch: [2/50]          || Step: [100/142]       || Average Validation Loss: 2.5180\n",
      "Epoch: [2/50]          || Step: [120/142]       || Average Validation Loss: 2.5199\n",
      "Epoch: [2/50]          || Step: [140/142]       || Average Validation Loss: 2.5167\n",
      "****************************************************************************************************\n",
      "Epoch: [2/50] || Training Loss = 2.48 || Validation Loss: 2.52 || Time: 22.104769\n",
      "****************************************************************************************************\n",
      "Epoch: [3/50]          || Step: [0/1579]        || Average Training Loss: 2.4243\n",
      "Epoch: [3/50]          || Step: [20/1579]       || Average Training Loss: 2.5087\n",
      "Epoch: [3/50]          || Step: [40/1579]       || Average Training Loss: 2.4823\n",
      "Epoch: [3/50]          || Step: [60/1579]       || Average Training Loss: 2.4797\n",
      "Epoch: [3/50]          || Step: [80/1579]       || Average Training Loss: 2.4698\n",
      "Epoch: [3/50]          || Step: [100/1579]      || Average Training Loss: 2.4607\n",
      "Epoch: [3/50]          || Step: [120/1579]      || Average Training Loss: 2.4542\n",
      "Epoch: [3/50]          || Step: [140/1579]      || Average Training Loss: 2.4564\n",
      "Epoch: [3/50]          || Step: [160/1579]      || Average Training Loss: 2.4533\n",
      "Epoch: [3/50]          || Step: [180/1579]      || Average Training Loss: 2.4543\n",
      "Epoch: [3/50]          || Step: [200/1579]      || Average Training Loss: 2.4507\n",
      "Epoch: [3/50]          || Step: [220/1579]      || Average Training Loss: 2.4524\n",
      "Epoch: [3/50]          || Step: [240/1579]      || Average Training Loss: 2.4500\n",
      "Epoch: [3/50]          || Step: [260/1579]      || Average Training Loss: 2.4528\n",
      "Epoch: [3/50]          || Step: [280/1579]      || Average Training Loss: 2.4560\n",
      "Epoch: [3/50]          || Step: [300/1579]      || Average Training Loss: 2.4571\n",
      "Epoch: [3/50]          || Step: [320/1579]      || Average Training Loss: 2.4549\n",
      "Epoch: [3/50]          || Step: [340/1579]      || Average Training Loss: 2.4553\n",
      "Epoch: [3/50]          || Step: [360/1579]      || Average Training Loss: 2.4547\n",
      "Epoch: [3/50]          || Step: [380/1579]      || Average Training Loss: 2.4531\n",
      "Epoch: [3/50]          || Step: [400/1579]      || Average Training Loss: 2.4534\n",
      "Epoch: [3/50]          || Step: [420/1579]      || Average Training Loss: 2.4536\n",
      "Epoch: [3/50]          || Step: [440/1579]      || Average Training Loss: 2.4524\n",
      "Epoch: [3/50]          || Step: [460/1579]      || Average Training Loss: 2.4551\n",
      "Epoch: [3/50]          || Step: [480/1579]      || Average Training Loss: 2.4545\n",
      "Epoch: [3/50]          || Step: [500/1579]      || Average Training Loss: 2.4559\n",
      "Epoch: [3/50]          || Step: [520/1579]      || Average Training Loss: 2.4571\n",
      "Epoch: [3/50]          || Step: [540/1579]      || Average Training Loss: 2.4555\n",
      "Epoch: [3/50]          || Step: [560/1579]      || Average Training Loss: 2.4547\n",
      "Epoch: [3/50]          || Step: [580/1579]      || Average Training Loss: 2.4536\n",
      "Epoch: [3/50]          || Step: [600/1579]      || Average Training Loss: 2.4531\n",
      "Epoch: [3/50]          || Step: [620/1579]      || Average Training Loss: 2.4528\n",
      "Epoch: [3/50]          || Step: [640/1579]      || Average Training Loss: 2.4521\n",
      "Epoch: [3/50]          || Step: [660/1579]      || Average Training Loss: 2.4530\n",
      "Epoch: [3/50]          || Step: [680/1579]      || Average Training Loss: 2.4553\n",
      "Epoch: [3/50]          || Step: [700/1579]      || Average Training Loss: 2.4549\n",
      "Epoch: [3/50]          || Step: [720/1579]      || Average Training Loss: 2.4545\n",
      "Epoch: [3/50]          || Step: [740/1579]      || Average Training Loss: 2.4551\n",
      "Epoch: [3/50]          || Step: [760/1579]      || Average Training Loss: 2.4543\n",
      "Epoch: [3/50]          || Step: [780/1579]      || Average Training Loss: 2.4533\n",
      "Epoch: [3/50]          || Step: [800/1579]      || Average Training Loss: 2.4539\n",
      "Epoch: [3/50]          || Step: [820/1579]      || Average Training Loss: 2.4529\n",
      "Epoch: [3/50]          || Step: [840/1579]      || Average Training Loss: 2.4519\n",
      "Epoch: [3/50]          || Step: [860/1579]      || Average Training Loss: 2.4527\n",
      "Epoch: [3/50]          || Step: [880/1579]      || Average Training Loss: 2.4532\n",
      "Epoch: [3/50]          || Step: [900/1579]      || Average Training Loss: 2.4542\n",
      "Epoch: [3/50]          || Step: [920/1579]      || Average Training Loss: 2.4540\n",
      "Epoch: [3/50]          || Step: [940/1579]      || Average Training Loss: 2.4539\n",
      "Epoch: [3/50]          || Step: [960/1579]      || Average Training Loss: 2.4542\n",
      "Epoch: [3/50]          || Step: [980/1579]      || Average Training Loss: 2.4539\n",
      "Epoch: [3/50]          || Step: [1000/1579]     || Average Training Loss: 2.4537\n",
      "Epoch: [3/50]          || Step: [1020/1579]     || Average Training Loss: 2.4535\n",
      "Epoch: [3/50]          || Step: [1040/1579]     || Average Training Loss: 2.4538\n",
      "Epoch: [3/50]          || Step: [1060/1579]     || Average Training Loss: 2.4537\n",
      "Epoch: [3/50]          || Step: [1080/1579]     || Average Training Loss: 2.4537\n",
      "Epoch: [3/50]          || Step: [1100/1579]     || Average Training Loss: 2.4530\n",
      "Epoch: [3/50]          || Step: [1120/1579]     || Average Training Loss: 2.4532\n",
      "Epoch: [3/50]          || Step: [1140/1579]     || Average Training Loss: 2.4530\n",
      "Epoch: [3/50]          || Step: [1160/1579]     || Average Training Loss: 2.4527\n",
      "Epoch: [3/50]          || Step: [1180/1579]     || Average Training Loss: 2.4534\n",
      "Epoch: [3/50]          || Step: [1200/1579]     || Average Training Loss: 2.4535\n",
      "Epoch: [3/50]          || Step: [1220/1579]     || Average Training Loss: 2.4524\n",
      "Epoch: [3/50]          || Step: [1240/1579]     || Average Training Loss: 2.4523\n",
      "Epoch: [3/50]          || Step: [1260/1579]     || Average Training Loss: 2.4523\n",
      "Epoch: [3/50]          || Step: [1280/1579]     || Average Training Loss: 2.4525\n",
      "Epoch: [3/50]          || Step: [1300/1579]     || Average Training Loss: 2.4521\n",
      "Epoch: [3/50]          || Step: [1320/1579]     || Average Training Loss: 2.4516\n",
      "Epoch: [3/50]          || Step: [1340/1579]     || Average Training Loss: 2.4521\n",
      "Epoch: [3/50]          || Step: [1360/1579]     || Average Training Loss: 2.4514\n",
      "Epoch: [3/50]          || Step: [1380/1579]     || Average Training Loss: 2.4514\n",
      "Epoch: [3/50]          || Step: [1400/1579]     || Average Training Loss: 2.4509\n",
      "Epoch: [3/50]          || Step: [1420/1579]     || Average Training Loss: 2.4512\n",
      "Epoch: [3/50]          || Step: [1440/1579]     || Average Training Loss: 2.4519\n",
      "Epoch: [3/50]          || Step: [1460/1579]     || Average Training Loss: 2.4518\n",
      "Epoch: [3/50]          || Step: [1480/1579]     || Average Training Loss: 2.4515\n",
      "Epoch: [3/50]          || Step: [1500/1579]     || Average Training Loss: 2.4504\n",
      "Epoch: [3/50]          || Step: [1520/1579]     || Average Training Loss: 2.4498\n",
      "Epoch: [3/50]          || Step: [1540/1579]     || Average Training Loss: 2.4497\n",
      "Epoch: [3/50]          || Step: [1560/1579]     || Average Training Loss: 2.4496\n",
      "Epoch: [3/50]          || Step: [0/142]         || Average Validation Loss: 2.4672\n",
      "Epoch: [3/50]          || Step: [20/142]        || Average Validation Loss: 2.5885\n",
      "Epoch: [3/50]          || Step: [40/142]        || Average Validation Loss: 2.5585\n",
      "Epoch: [3/50]          || Step: [60/142]        || Average Validation Loss: 2.5376\n",
      "Epoch: [3/50]          || Step: [80/142]        || Average Validation Loss: 2.5401\n",
      "Epoch: [3/50]          || Step: [100/142]       || Average Validation Loss: 2.5421\n",
      "Epoch: [3/50]          || Step: [120/142]       || Average Validation Loss: 2.5465\n",
      "Epoch: [3/50]          || Step: [140/142]       || Average Validation Loss: 2.5530\n",
      "****************************************************************************************************\n",
      "Epoch: [3/50] || Training Loss = 2.45 || Validation Loss: 2.55 || Time: 21.899824\n",
      "****************************************************************************************************\n",
      "Epoch: [4/50]          || Step: [0/1579]        || Average Training Loss: 2.6947\n",
      "Epoch: [4/50]          || Step: [20/1579]       || Average Training Loss: 2.5279\n",
      "Epoch: [4/50]          || Step: [40/1579]       || Average Training Loss: 2.5006\n",
      "Epoch: [4/50]          || Step: [60/1579]       || Average Training Loss: 2.4734\n",
      "Epoch: [4/50]          || Step: [80/1579]       || Average Training Loss: 2.4560\n",
      "Epoch: [4/50]          || Step: [100/1579]      || Average Training Loss: 2.4573\n",
      "Epoch: [4/50]          || Step: [120/1579]      || Average Training Loss: 2.4508\n",
      "Epoch: [4/50]          || Step: [140/1579]      || Average Training Loss: 2.4470\n",
      "Epoch: [4/50]          || Step: [160/1579]      || Average Training Loss: 2.4422\n",
      "Epoch: [4/50]          || Step: [180/1579]      || Average Training Loss: 2.4351\n",
      "Epoch: [4/50]          || Step: [200/1579]      || Average Training Loss: 2.4342\n",
      "Epoch: [4/50]          || Step: [220/1579]      || Average Training Loss: 2.4399\n",
      "Epoch: [4/50]          || Step: [240/1579]      || Average Training Loss: 2.4373\n",
      "Epoch: [4/50]          || Step: [260/1579]      || Average Training Loss: 2.4381\n",
      "Epoch: [4/50]          || Step: [280/1579]      || Average Training Loss: 2.4363\n",
      "Epoch: [4/50]          || Step: [300/1579]      || Average Training Loss: 2.4347\n",
      "Epoch: [4/50]          || Step: [320/1579]      || Average Training Loss: 2.4360\n",
      "Epoch: [4/50]          || Step: [340/1579]      || Average Training Loss: 2.4374\n",
      "Epoch: [4/50]          || Step: [360/1579]      || Average Training Loss: 2.4358\n",
      "Epoch: [4/50]          || Step: [380/1579]      || Average Training Loss: 2.4336\n",
      "Epoch: [4/50]          || Step: [400/1579]      || Average Training Loss: 2.4333\n",
      "Epoch: [4/50]          || Step: [420/1579]      || Average Training Loss: 2.4347\n",
      "Epoch: [4/50]          || Step: [440/1579]      || Average Training Loss: 2.4337\n",
      "Epoch: [4/50]          || Step: [460/1579]      || Average Training Loss: 2.4331\n",
      "Epoch: [4/50]          || Step: [480/1579]      || Average Training Loss: 2.4338\n",
      "Epoch: [4/50]          || Step: [500/1579]      || Average Training Loss: 2.4328\n",
      "Epoch: [4/50]          || Step: [520/1579]      || Average Training Loss: 2.4316\n",
      "Epoch: [4/50]          || Step: [540/1579]      || Average Training Loss: 2.4329\n",
      "Epoch: [4/50]          || Step: [560/1579]      || Average Training Loss: 2.4340\n",
      "Epoch: [4/50]          || Step: [580/1579]      || Average Training Loss: 2.4336\n",
      "Epoch: [4/50]          || Step: [600/1579]      || Average Training Loss: 2.4342\n",
      "Epoch: [4/50]          || Step: [620/1579]      || Average Training Loss: 2.4334\n",
      "Epoch: [4/50]          || Step: [640/1579]      || Average Training Loss: 2.4325\n",
      "Epoch: [4/50]          || Step: [660/1579]      || Average Training Loss: 2.4316\n",
      "Epoch: [4/50]          || Step: [680/1579]      || Average Training Loss: 2.4307\n",
      "Epoch: [4/50]          || Step: [700/1579]      || Average Training Loss: 2.4298\n",
      "Epoch: [4/50]          || Step: [720/1579]      || Average Training Loss: 2.4312\n",
      "Epoch: [4/50]          || Step: [740/1579]      || Average Training Loss: 2.4301\n",
      "Epoch: [4/50]          || Step: [760/1579]      || Average Training Loss: 2.4282\n",
      "Epoch: [4/50]          || Step: [780/1579]      || Average Training Loss: 2.4279\n",
      "Epoch: [4/50]          || Step: [800/1579]      || Average Training Loss: 2.4281\n",
      "Epoch: [4/50]          || Step: [820/1579]      || Average Training Loss: 2.4287\n",
      "Epoch: [4/50]          || Step: [840/1579]      || Average Training Loss: 2.4285\n",
      "Epoch: [4/50]          || Step: [860/1579]      || Average Training Loss: 2.4294\n",
      "Epoch: [4/50]          || Step: [880/1579]      || Average Training Loss: 2.4304\n",
      "Epoch: [4/50]          || Step: [900/1579]      || Average Training Loss: 2.4310\n",
      "Epoch: [4/50]          || Step: [920/1579]      || Average Training Loss: 2.4308\n",
      "Epoch: [4/50]          || Step: [940/1579]      || Average Training Loss: 2.4309\n",
      "Epoch: [4/50]          || Step: [960/1579]      || Average Training Loss: 2.4311\n",
      "Epoch: [4/50]          || Step: [980/1579]      || Average Training Loss: 2.4318\n",
      "Epoch: [4/50]          || Step: [1000/1579]     || Average Training Loss: 2.4317\n",
      "Epoch: [4/50]          || Step: [1020/1579]     || Average Training Loss: 2.4314\n",
      "Epoch: [4/50]          || Step: [1040/1579]     || Average Training Loss: 2.4317\n",
      "Epoch: [4/50]          || Step: [1060/1579]     || Average Training Loss: 2.4324\n",
      "Epoch: [4/50]          || Step: [1080/1579]     || Average Training Loss: 2.4328\n",
      "Epoch: [4/50]          || Step: [1100/1579]     || Average Training Loss: 2.4323\n",
      "Epoch: [4/50]          || Step: [1120/1579]     || Average Training Loss: 2.4325\n",
      "Epoch: [4/50]          || Step: [1140/1579]     || Average Training Loss: 2.4334\n",
      "Epoch: [4/50]          || Step: [1160/1579]     || Average Training Loss: 2.4333\n",
      "Epoch: [4/50]          || Step: [1180/1579]     || Average Training Loss: 2.4333\n",
      "Epoch: [4/50]          || Step: [1200/1579]     || Average Training Loss: 2.4335\n",
      "Epoch: [4/50]          || Step: [1220/1579]     || Average Training Loss: 2.4328\n",
      "Epoch: [4/50]          || Step: [1240/1579]     || Average Training Loss: 2.4324\n",
      "Epoch: [4/50]          || Step: [1260/1579]     || Average Training Loss: 2.4326\n",
      "Epoch: [4/50]          || Step: [1280/1579]     || Average Training Loss: 2.4323\n",
      "Epoch: [4/50]          || Step: [1300/1579]     || Average Training Loss: 2.4325\n",
      "Epoch: [4/50]          || Step: [1320/1579]     || Average Training Loss: 2.4330\n",
      "Epoch: [4/50]          || Step: [1340/1579]     || Average Training Loss: 2.4329\n",
      "Epoch: [4/50]          || Step: [1360/1579]     || Average Training Loss: 2.4326\n",
      "Epoch: [4/50]          || Step: [1380/1579]     || Average Training Loss: 2.4327\n",
      "Epoch: [4/50]          || Step: [1400/1579]     || Average Training Loss: 2.4326\n",
      "Epoch: [4/50]          || Step: [1420/1579]     || Average Training Loss: 2.4330\n",
      "Epoch: [4/50]          || Step: [1440/1579]     || Average Training Loss: 2.4328\n",
      "Epoch: [4/50]          || Step: [1460/1579]     || Average Training Loss: 2.4325\n",
      "Epoch: [4/50]          || Step: [1480/1579]     || Average Training Loss: 2.4323\n",
      "Epoch: [4/50]          || Step: [1500/1579]     || Average Training Loss: 2.4316\n",
      "Epoch: [4/50]          || Step: [1520/1579]     || Average Training Loss: 2.4313\n",
      "Epoch: [4/50]          || Step: [1540/1579]     || Average Training Loss: 2.4313\n",
      "Epoch: [4/50]          || Step: [1560/1579]     || Average Training Loss: 2.4315\n",
      "Epoch: [4/50]          || Step: [0/142]         || Average Validation Loss: 2.5378\n",
      "Epoch: [4/50]          || Step: [20/142]        || Average Validation Loss: 2.5044\n",
      "Epoch: [4/50]          || Step: [40/142]        || Average Validation Loss: 2.4814\n",
      "Epoch: [4/50]          || Step: [60/142]        || Average Validation Loss: 2.4863\n",
      "Epoch: [4/50]          || Step: [80/142]        || Average Validation Loss: 2.4838\n",
      "Epoch: [4/50]          || Step: [100/142]       || Average Validation Loss: 2.4898\n",
      "Epoch: [4/50]          || Step: [120/142]       || Average Validation Loss: 2.4865\n",
      "Epoch: [4/50]          || Step: [140/142]       || Average Validation Loss: 2.4871\n",
      "****************************************************************************************************\n",
      "Epoch: [4/50] || Training Loss = 2.43 || Validation Loss: 2.48 || Time: 21.478240\n",
      "****************************************************************************************************\n",
      "Epoch: [5/50]          || Step: [0/1579]        || Average Training Loss: 2.7058\n",
      "Epoch: [5/50]          || Step: [20/1579]       || Average Training Loss: 2.4992\n",
      "Epoch: [5/50]          || Step: [40/1579]       || Average Training Loss: 2.4893\n",
      "Epoch: [5/50]          || Step: [60/1579]       || Average Training Loss: 2.4728\n",
      "Epoch: [5/50]          || Step: [80/1579]       || Average Training Loss: 2.4607\n",
      "Epoch: [5/50]          || Step: [100/1579]      || Average Training Loss: 2.4420\n",
      "Epoch: [5/50]          || Step: [120/1579]      || Average Training Loss: 2.4378\n",
      "Epoch: [5/50]          || Step: [140/1579]      || Average Training Loss: 2.4371\n",
      "Epoch: [5/50]          || Step: [160/1579]      || Average Training Loss: 2.4351\n",
      "Epoch: [5/50]          || Step: [180/1579]      || Average Training Loss: 2.4277\n",
      "Epoch: [5/50]          || Step: [200/1579]      || Average Training Loss: 2.4266\n",
      "Epoch: [5/50]          || Step: [220/1579]      || Average Training Loss: 2.4229\n",
      "Epoch: [5/50]          || Step: [240/1579]      || Average Training Loss: 2.4195\n",
      "Epoch: [5/50]          || Step: [260/1579]      || Average Training Loss: 2.4200\n",
      "Epoch: [5/50]          || Step: [280/1579]      || Average Training Loss: 2.4175\n",
      "Epoch: [5/50]          || Step: [300/1579]      || Average Training Loss: 2.4178\n",
      "Epoch: [5/50]          || Step: [320/1579]      || Average Training Loss: 2.4191\n",
      "Epoch: [5/50]          || Step: [340/1579]      || Average Training Loss: 2.4174\n",
      "Epoch: [5/50]          || Step: [360/1579]      || Average Training Loss: 2.4169\n",
      "Epoch: [5/50]          || Step: [380/1579]      || Average Training Loss: 2.4168\n",
      "Epoch: [5/50]          || Step: [400/1579]      || Average Training Loss: 2.4171\n",
      "Epoch: [5/50]          || Step: [420/1579]      || Average Training Loss: 2.4173\n",
      "Epoch: [5/50]          || Step: [440/1579]      || Average Training Loss: 2.4186\n",
      "Epoch: [5/50]          || Step: [460/1579]      || Average Training Loss: 2.4191\n",
      "Epoch: [5/50]          || Step: [480/1579]      || Average Training Loss: 2.4187\n",
      "Epoch: [5/50]          || Step: [500/1579]      || Average Training Loss: 2.4182\n",
      "Epoch: [5/50]          || Step: [520/1579]      || Average Training Loss: 2.4193\n",
      "Epoch: [5/50]          || Step: [540/1579]      || Average Training Loss: 2.4184\n",
      "Epoch: [5/50]          || Step: [560/1579]      || Average Training Loss: 2.4193\n",
      "Epoch: [5/50]          || Step: [580/1579]      || Average Training Loss: 2.4180\n",
      "Epoch: [5/50]          || Step: [600/1579]      || Average Training Loss: 2.4179\n",
      "Epoch: [5/50]          || Step: [620/1579]      || Average Training Loss: 2.4174\n",
      "Epoch: [5/50]          || Step: [640/1579]      || Average Training Loss: 2.4180\n",
      "Epoch: [5/50]          || Step: [660/1579]      || Average Training Loss: 2.4192\n",
      "Epoch: [5/50]          || Step: [680/1579]      || Average Training Loss: 2.4202\n",
      "Epoch: [5/50]          || Step: [700/1579]      || Average Training Loss: 2.4188\n",
      "Epoch: [5/50]          || Step: [720/1579]      || Average Training Loss: 2.4197\n",
      "Epoch: [5/50]          || Step: [740/1579]      || Average Training Loss: 2.4182\n",
      "Epoch: [5/50]          || Step: [760/1579]      || Average Training Loss: 2.4175\n",
      "Epoch: [5/50]          || Step: [780/1579]      || Average Training Loss: 2.4180\n",
      "Epoch: [5/50]          || Step: [800/1579]      || Average Training Loss: 2.4192\n",
      "Epoch: [5/50]          || Step: [820/1579]      || Average Training Loss: 2.4172\n",
      "Epoch: [5/50]          || Step: [840/1579]      || Average Training Loss: 2.4165\n",
      "Epoch: [5/50]          || Step: [860/1579]      || Average Training Loss: 2.4179\n",
      "Epoch: [5/50]          || Step: [880/1579]      || Average Training Loss: 2.4175\n",
      "Epoch: [5/50]          || Step: [900/1579]      || Average Training Loss: 2.4179\n",
      "Epoch: [5/50]          || Step: [920/1579]      || Average Training Loss: 2.4188\n",
      "Epoch: [5/50]          || Step: [940/1579]      || Average Training Loss: 2.4181\n",
      "Epoch: [5/50]          || Step: [960/1579]      || Average Training Loss: 2.4187\n",
      "Epoch: [5/50]          || Step: [980/1579]      || Average Training Loss: 2.4187\n",
      "Epoch: [5/50]          || Step: [1000/1579]     || Average Training Loss: 2.4186\n",
      "Epoch: [5/50]          || Step: [1020/1579]     || Average Training Loss: 2.4184\n",
      "Epoch: [5/50]          || Step: [1040/1579]     || Average Training Loss: 2.4183\n",
      "Epoch: [5/50]          || Step: [1060/1579]     || Average Training Loss: 2.4182\n",
      "Epoch: [5/50]          || Step: [1080/1579]     || Average Training Loss: 2.4180\n",
      "Epoch: [5/50]          || Step: [1100/1579]     || Average Training Loss: 2.4189\n",
      "Epoch: [5/50]          || Step: [1120/1579]     || Average Training Loss: 2.4188\n",
      "Epoch: [5/50]          || Step: [1140/1579]     || Average Training Loss: 2.4194\n",
      "Epoch: [5/50]          || Step: [1160/1579]     || Average Training Loss: 2.4199\n",
      "Epoch: [5/50]          || Step: [1180/1579]     || Average Training Loss: 2.4197\n",
      "Epoch: [5/50]          || Step: [1200/1579]     || Average Training Loss: 2.4199\n",
      "Epoch: [5/50]          || Step: [1220/1579]     || Average Training Loss: 2.4196\n",
      "Epoch: [5/50]          || Step: [1240/1579]     || Average Training Loss: 2.4187\n",
      "Epoch: [5/50]          || Step: [1260/1579]     || Average Training Loss: 2.4189\n",
      "Epoch: [5/50]          || Step: [1280/1579]     || Average Training Loss: 2.4194\n",
      "Epoch: [5/50]          || Step: [1300/1579]     || Average Training Loss: 2.4197\n",
      "Epoch: [5/50]          || Step: [1320/1579]     || Average Training Loss: 2.4193\n",
      "Epoch: [5/50]          || Step: [1340/1579]     || Average Training Loss: 2.4199\n",
      "Epoch: [5/50]          || Step: [1360/1579]     || Average Training Loss: 2.4198\n",
      "Epoch: [5/50]          || Step: [1380/1579]     || Average Training Loss: 2.4194\n",
      "Epoch: [5/50]          || Step: [1400/1579]     || Average Training Loss: 2.4195\n",
      "Epoch: [5/50]          || Step: [1420/1579]     || Average Training Loss: 2.4198\n",
      "Epoch: [5/50]          || Step: [1440/1579]     || Average Training Loss: 2.4196\n",
      "Epoch: [5/50]          || Step: [1460/1579]     || Average Training Loss: 2.4195\n",
      "Epoch: [5/50]          || Step: [1480/1579]     || Average Training Loss: 2.4197\n",
      "Epoch: [5/50]          || Step: [1500/1579]     || Average Training Loss: 2.4195\n",
      "Epoch: [5/50]          || Step: [1520/1579]     || Average Training Loss: 2.4190\n",
      "Epoch: [5/50]          || Step: [1540/1579]     || Average Training Loss: 2.4186\n",
      "Epoch: [5/50]          || Step: [1560/1579]     || Average Training Loss: 2.4188\n",
      "Epoch: [5/50]          || Step: [0/142]         || Average Validation Loss: 2.5467\n",
      "Epoch: [5/50]          || Step: [20/142]        || Average Validation Loss: 2.4375\n",
      "Epoch: [5/50]          || Step: [40/142]        || Average Validation Loss: 2.4617\n",
      "Epoch: [5/50]          || Step: [60/142]        || Average Validation Loss: 2.4641\n",
      "Epoch: [5/50]          || Step: [80/142]        || Average Validation Loss: 2.4576\n",
      "Epoch: [5/50]          || Step: [100/142]       || Average Validation Loss: 2.4609\n",
      "Epoch: [5/50]          || Step: [120/142]       || Average Validation Loss: 2.4607\n",
      "Epoch: [5/50]          || Step: [140/142]       || Average Validation Loss: 2.4607\n",
      "****************************************************************************************************\n",
      "Epoch: [5/50] || Training Loss = 2.42 || Validation Loss: 2.46 || Time: 22.108039\n",
      "****************************************************************************************************\n",
      "Epoch: [6/50]          || Step: [0/1579]        || Average Training Loss: 2.2674\n",
      "Epoch: [6/50]          || Step: [20/1579]       || Average Training Loss: 2.4716\n",
      "Epoch: [6/50]          || Step: [40/1579]       || Average Training Loss: 2.4589\n",
      "Epoch: [6/50]          || Step: [60/1579]       || Average Training Loss: 2.4421\n",
      "Epoch: [6/50]          || Step: [80/1579]       || Average Training Loss: 2.4330\n",
      "Epoch: [6/50]          || Step: [100/1579]      || Average Training Loss: 2.4339\n",
      "Epoch: [6/50]          || Step: [120/1579]      || Average Training Loss: 2.4392\n",
      "Epoch: [6/50]          || Step: [140/1579]      || Average Training Loss: 2.4381\n",
      "Epoch: [6/50]          || Step: [160/1579]      || Average Training Loss: 2.4347\n",
      "Epoch: [6/50]          || Step: [180/1579]      || Average Training Loss: 2.4318\n",
      "Epoch: [6/50]          || Step: [200/1579]      || Average Training Loss: 2.4268\n",
      "Epoch: [6/50]          || Step: [220/1579]      || Average Training Loss: 2.4229\n",
      "Epoch: [6/50]          || Step: [240/1579]      || Average Training Loss: 2.4249\n",
      "Epoch: [6/50]          || Step: [260/1579]      || Average Training Loss: 2.4223\n",
      "Epoch: [6/50]          || Step: [280/1579]      || Average Training Loss: 2.4229\n",
      "Epoch: [6/50]          || Step: [300/1579]      || Average Training Loss: 2.4233\n",
      "Epoch: [6/50]          || Step: [320/1579]      || Average Training Loss: 2.4208\n",
      "Epoch: [6/50]          || Step: [340/1579]      || Average Training Loss: 2.4195\n",
      "Epoch: [6/50]          || Step: [360/1579]      || Average Training Loss: 2.4205\n",
      "Epoch: [6/50]          || Step: [380/1579]      || Average Training Loss: 2.4212\n",
      "Epoch: [6/50]          || Step: [400/1579]      || Average Training Loss: 2.4191\n",
      "Epoch: [6/50]          || Step: [420/1579]      || Average Training Loss: 2.4189\n",
      "Epoch: [6/50]          || Step: [440/1579]      || Average Training Loss: 2.4182\n",
      "Epoch: [6/50]          || Step: [460/1579]      || Average Training Loss: 2.4156\n",
      "Epoch: [6/50]          || Step: [480/1579]      || Average Training Loss: 2.4160\n",
      "Epoch: [6/50]          || Step: [500/1579]      || Average Training Loss: 2.4166\n",
      "Epoch: [6/50]          || Step: [520/1579]      || Average Training Loss: 2.4173\n",
      "Epoch: [6/50]          || Step: [540/1579]      || Average Training Loss: 2.4176\n",
      "Epoch: [6/50]          || Step: [560/1579]      || Average Training Loss: 2.4169\n",
      "Epoch: [6/50]          || Step: [580/1579]      || Average Training Loss: 2.4175\n",
      "Epoch: [6/50]          || Step: [600/1579]      || Average Training Loss: 2.4171\n",
      "Epoch: [6/50]          || Step: [620/1579]      || Average Training Loss: 2.4170\n",
      "Epoch: [6/50]          || Step: [640/1579]      || Average Training Loss: 2.4169\n",
      "Epoch: [6/50]          || Step: [660/1579]      || Average Training Loss: 2.4192\n",
      "Epoch: [6/50]          || Step: [680/1579]      || Average Training Loss: 2.4191\n",
      "Epoch: [6/50]          || Step: [700/1579]      || Average Training Loss: 2.4190\n",
      "Epoch: [6/50]          || Step: [720/1579]      || Average Training Loss: 2.4177\n",
      "Epoch: [6/50]          || Step: [740/1579]      || Average Training Loss: 2.4159\n",
      "Epoch: [6/50]          || Step: [760/1579]      || Average Training Loss: 2.4163\n",
      "Epoch: [6/50]          || Step: [780/1579]      || Average Training Loss: 2.4163\n",
      "Epoch: [6/50]          || Step: [800/1579]      || Average Training Loss: 2.4164\n",
      "Epoch: [6/50]          || Step: [820/1579]      || Average Training Loss: 2.4158\n",
      "Epoch: [6/50]          || Step: [840/1579]      || Average Training Loss: 2.4153\n",
      "Epoch: [6/50]          || Step: [860/1579]      || Average Training Loss: 2.4164\n",
      "Epoch: [6/50]          || Step: [880/1579]      || Average Training Loss: 2.4161\n",
      "Epoch: [6/50]          || Step: [900/1579]      || Average Training Loss: 2.4153\n",
      "Epoch: [6/50]          || Step: [920/1579]      || Average Training Loss: 2.4154\n",
      "Epoch: [6/50]          || Step: [940/1579]      || Average Training Loss: 2.4144\n",
      "Epoch: [6/50]          || Step: [960/1579]      || Average Training Loss: 2.4149\n",
      "Epoch: [6/50]          || Step: [980/1579]      || Average Training Loss: 2.4148\n",
      "Epoch: [6/50]          || Step: [1000/1579]     || Average Training Loss: 2.4133\n",
      "Epoch: [6/50]          || Step: [1020/1579]     || Average Training Loss: 2.4133\n",
      "Epoch: [6/50]          || Step: [1040/1579]     || Average Training Loss: 2.4120\n",
      "Epoch: [6/50]          || Step: [1060/1579]     || Average Training Loss: 2.4112\n",
      "Epoch: [6/50]          || Step: [1080/1579]     || Average Training Loss: 2.4120\n",
      "Epoch: [6/50]          || Step: [1100/1579]     || Average Training Loss: 2.4113\n",
      "Epoch: [6/50]          || Step: [1120/1579]     || Average Training Loss: 2.4108\n",
      "Epoch: [6/50]          || Step: [1140/1579]     || Average Training Loss: 2.4111\n",
      "Epoch: [6/50]          || Step: [1160/1579]     || Average Training Loss: 2.4115\n",
      "Epoch: [6/50]          || Step: [1180/1579]     || Average Training Loss: 2.4117\n",
      "Epoch: [6/50]          || Step: [1200/1579]     || Average Training Loss: 2.4122\n",
      "Epoch: [6/50]          || Step: [1220/1579]     || Average Training Loss: 2.4118\n",
      "Epoch: [6/50]          || Step: [1240/1579]     || Average Training Loss: 2.4115\n",
      "Epoch: [6/50]          || Step: [1260/1579]     || Average Training Loss: 2.4116\n",
      "Epoch: [6/50]          || Step: [1280/1579]     || Average Training Loss: 2.4111\n",
      "Epoch: [6/50]          || Step: [1300/1579]     || Average Training Loss: 2.4109\n",
      "Epoch: [6/50]          || Step: [1320/1579]     || Average Training Loss: 2.4112\n",
      "Epoch: [6/50]          || Step: [1340/1579]     || Average Training Loss: 2.4114\n",
      "Epoch: [6/50]          || Step: [1360/1579]     || Average Training Loss: 2.4109\n",
      "Epoch: [6/50]          || Step: [1380/1579]     || Average Training Loss: 2.4112\n",
      "Epoch: [6/50]          || Step: [1400/1579]     || Average Training Loss: 2.4107\n",
      "Epoch: [6/50]          || Step: [1420/1579]     || Average Training Loss: 2.4108\n",
      "Epoch: [6/50]          || Step: [1440/1579]     || Average Training Loss: 2.4105\n",
      "Epoch: [6/50]          || Step: [1460/1579]     || Average Training Loss: 2.4101\n",
      "Epoch: [6/50]          || Step: [1480/1579]     || Average Training Loss: 2.4104\n",
      "Epoch: [6/50]          || Step: [1500/1579]     || Average Training Loss: 2.4102\n",
      "Epoch: [6/50]          || Step: [1520/1579]     || Average Training Loss: 2.4094\n",
      "Epoch: [6/50]          || Step: [1540/1579]     || Average Training Loss: 2.4095\n",
      "Epoch: [6/50]          || Step: [1560/1579]     || Average Training Loss: 2.4090\n",
      "Epoch: [6/50]          || Step: [0/142]         || Average Validation Loss: 2.5582\n",
      "Epoch: [6/50]          || Step: [20/142]        || Average Validation Loss: 2.3987\n",
      "Epoch: [6/50]          || Step: [40/142]        || Average Validation Loss: 2.4013\n",
      "Epoch: [6/50]          || Step: [60/142]        || Average Validation Loss: 2.3979\n",
      "Epoch: [6/50]          || Step: [80/142]        || Average Validation Loss: 2.4039\n",
      "Epoch: [6/50]          || Step: [100/142]       || Average Validation Loss: 2.4108\n",
      "Epoch: [6/50]          || Step: [120/142]       || Average Validation Loss: 2.4168\n",
      "Epoch: [6/50]          || Step: [140/142]       || Average Validation Loss: 2.4151\n",
      "****************************************************************************************************\n",
      "Epoch: [6/50] || Training Loss = 2.41 || Validation Loss: 2.42 || Time: 21.724512\n",
      "****************************************************************************************************\n",
      "Epoch: [7/50]          || Step: [0/1579]        || Average Training Loss: 2.3939\n",
      "Epoch: [7/50]          || Step: [20/1579]       || Average Training Loss: 2.4609\n",
      "Epoch: [7/50]          || Step: [40/1579]       || Average Training Loss: 2.4480\n",
      "Epoch: [7/50]          || Step: [60/1579]       || Average Training Loss: 2.4234\n",
      "Epoch: [7/50]          || Step: [80/1579]       || Average Training Loss: 2.4164\n",
      "Epoch: [7/50]          || Step: [100/1579]      || Average Training Loss: 2.4168\n",
      "Epoch: [7/50]          || Step: [120/1579]      || Average Training Loss: 2.4092\n",
      "Epoch: [7/50]          || Step: [140/1579]      || Average Training Loss: 2.4158\n",
      "Epoch: [7/50]          || Step: [160/1579]      || Average Training Loss: 2.4105\n",
      "Epoch: [7/50]          || Step: [180/1579]      || Average Training Loss: 2.4082\n",
      "Epoch: [7/50]          || Step: [200/1579]      || Average Training Loss: 2.4099\n",
      "Epoch: [7/50]          || Step: [220/1579]      || Average Training Loss: 2.4107\n",
      "Epoch: [7/50]          || Step: [240/1579]      || Average Training Loss: 2.4112\n",
      "Epoch: [7/50]          || Step: [260/1579]      || Average Training Loss: 2.4094\n",
      "Epoch: [7/50]          || Step: [280/1579]      || Average Training Loss: 2.4100\n",
      "Epoch: [7/50]          || Step: [300/1579]      || Average Training Loss: 2.4090\n",
      "Epoch: [7/50]          || Step: [320/1579]      || Average Training Loss: 2.4097\n",
      "Epoch: [7/50]          || Step: [340/1579]      || Average Training Loss: 2.4071\n",
      "Epoch: [7/50]          || Step: [360/1579]      || Average Training Loss: 2.4092\n",
      "Epoch: [7/50]          || Step: [380/1579]      || Average Training Loss: 2.4089\n",
      "Epoch: [7/50]          || Step: [400/1579]      || Average Training Loss: 2.4067\n",
      "Epoch: [7/50]          || Step: [420/1579]      || Average Training Loss: 2.4061\n",
      "Epoch: [7/50]          || Step: [440/1579]      || Average Training Loss: 2.4061\n",
      "Epoch: [7/50]          || Step: [460/1579]      || Average Training Loss: 2.4059\n",
      "Epoch: [7/50]          || Step: [480/1579]      || Average Training Loss: 2.4056\n",
      "Epoch: [7/50]          || Step: [500/1579]      || Average Training Loss: 2.4048\n",
      "Epoch: [7/50]          || Step: [520/1579]      || Average Training Loss: 2.4053\n",
      "Epoch: [7/50]          || Step: [540/1579]      || Average Training Loss: 2.4055\n",
      "Epoch: [7/50]          || Step: [560/1579]      || Average Training Loss: 2.4052\n",
      "Epoch: [7/50]          || Step: [580/1579]      || Average Training Loss: 2.4055\n",
      "Epoch: [7/50]          || Step: [600/1579]      || Average Training Loss: 2.4053\n",
      "Epoch: [7/50]          || Step: [620/1579]      || Average Training Loss: 2.4075\n",
      "Epoch: [7/50]          || Step: [640/1579]      || Average Training Loss: 2.4070\n",
      "Epoch: [7/50]          || Step: [660/1579]      || Average Training Loss: 2.4077\n",
      "Epoch: [7/50]          || Step: [680/1579]      || Average Training Loss: 2.4070\n",
      "Epoch: [7/50]          || Step: [700/1579]      || Average Training Loss: 2.4069\n",
      "Epoch: [7/50]          || Step: [720/1579]      || Average Training Loss: 2.4074\n",
      "Epoch: [7/50]          || Step: [740/1579]      || Average Training Loss: 2.4065\n",
      "Epoch: [7/50]          || Step: [760/1579]      || Average Training Loss: 2.4067\n",
      "Epoch: [7/50]          || Step: [780/1579]      || Average Training Loss: 2.4064\n",
      "Epoch: [7/50]          || Step: [800/1579]      || Average Training Loss: 2.4057\n",
      "Epoch: [7/50]          || Step: [820/1579]      || Average Training Loss: 2.4053\n",
      "Epoch: [7/50]          || Step: [840/1579]      || Average Training Loss: 2.4046\n",
      "Epoch: [7/50]          || Step: [860/1579]      || Average Training Loss: 2.4058\n",
      "Epoch: [7/50]          || Step: [880/1579]      || Average Training Loss: 2.4050\n",
      "Epoch: [7/50]          || Step: [900/1579]      || Average Training Loss: 2.4057\n",
      "Epoch: [7/50]          || Step: [920/1579]      || Average Training Loss: 2.4055\n",
      "Epoch: [7/50]          || Step: [940/1579]      || Average Training Loss: 2.4062\n",
      "Epoch: [7/50]          || Step: [960/1579]      || Average Training Loss: 2.4057\n",
      "Epoch: [7/50]          || Step: [980/1579]      || Average Training Loss: 2.4050\n",
      "Epoch: [7/50]          || Step: [1000/1579]     || Average Training Loss: 2.4047\n",
      "Epoch: [7/50]          || Step: [1020/1579]     || Average Training Loss: 2.4051\n",
      "Epoch: [7/50]          || Step: [1040/1579]     || Average Training Loss: 2.4056\n",
      "Epoch: [7/50]          || Step: [1060/1579]     || Average Training Loss: 2.4064\n",
      "Epoch: [7/50]          || Step: [1080/1579]     || Average Training Loss: 2.4072\n",
      "Epoch: [7/50]          || Step: [1100/1579]     || Average Training Loss: 2.4064\n",
      "Epoch: [7/50]          || Step: [1120/1579]     || Average Training Loss: 2.4060\n",
      "Epoch: [7/50]          || Step: [1140/1579]     || Average Training Loss: 2.4061\n",
      "Epoch: [7/50]          || Step: [1160/1579]     || Average Training Loss: 2.4059\n",
      "Epoch: [7/50]          || Step: [1180/1579]     || Average Training Loss: 2.4059\n",
      "Epoch: [7/50]          || Step: [1200/1579]     || Average Training Loss: 2.4063\n",
      "Epoch: [7/50]          || Step: [1220/1579]     || Average Training Loss: 2.4060\n",
      "Epoch: [7/50]          || Step: [1240/1579]     || Average Training Loss: 2.4054\n",
      "Epoch: [7/50]          || Step: [1260/1579]     || Average Training Loss: 2.4058\n",
      "Epoch: [7/50]          || Step: [1280/1579]     || Average Training Loss: 2.4053\n",
      "Epoch: [7/50]          || Step: [1300/1579]     || Average Training Loss: 2.4053\n",
      "Epoch: [7/50]          || Step: [1320/1579]     || Average Training Loss: 2.4052\n",
      "Epoch: [7/50]          || Step: [1340/1579]     || Average Training Loss: 2.4046\n",
      "Epoch: [7/50]          || Step: [1360/1579]     || Average Training Loss: 2.4045\n",
      "Epoch: [7/50]          || Step: [1380/1579]     || Average Training Loss: 2.4043\n",
      "Epoch: [7/50]          || Step: [1400/1579]     || Average Training Loss: 2.4042\n",
      "Epoch: [7/50]          || Step: [1420/1579]     || Average Training Loss: 2.4034\n",
      "Epoch: [7/50]          || Step: [1440/1579]     || Average Training Loss: 2.4031\n",
      "Epoch: [7/50]          || Step: [1460/1579]     || Average Training Loss: 2.4023\n",
      "Epoch: [7/50]          || Step: [1480/1579]     || Average Training Loss: 2.4016\n",
      "Epoch: [7/50]          || Step: [1500/1579]     || Average Training Loss: 2.4017\n",
      "Epoch: [7/50]          || Step: [1520/1579]     || Average Training Loss: 2.4016\n",
      "Epoch: [7/50]          || Step: [1540/1579]     || Average Training Loss: 2.4011\n",
      "Epoch: [7/50]          || Step: [1560/1579]     || Average Training Loss: 2.4011\n",
      "Epoch: [7/50]          || Step: [0/142]         || Average Validation Loss: 2.4227\n",
      "Epoch: [7/50]          || Step: [20/142]        || Average Validation Loss: 2.4282\n",
      "Epoch: [7/50]          || Step: [40/142]        || Average Validation Loss: 2.4041\n",
      "Epoch: [7/50]          || Step: [60/142]        || Average Validation Loss: 2.4148\n",
      "Epoch: [7/50]          || Step: [80/142]        || Average Validation Loss: 2.4190\n",
      "Epoch: [7/50]          || Step: [100/142]       || Average Validation Loss: 2.4120\n",
      "Epoch: [7/50]          || Step: [120/142]       || Average Validation Loss: 2.4164\n",
      "Epoch: [7/50]          || Step: [140/142]       || Average Validation Loss: 2.4225\n",
      "****************************************************************************************************\n",
      "Epoch: [7/50] || Training Loss = 2.40 || Validation Loss: 2.42 || Time: 21.882370\n",
      "****************************************************************************************************\n",
      "Epoch: [8/50]          || Step: [0/1579]        || Average Training Loss: 2.4411\n",
      "Epoch: [8/50]          || Step: [20/1579]       || Average Training Loss: 2.4661\n",
      "Epoch: [8/50]          || Step: [40/1579]       || Average Training Loss: 2.4522\n",
      "Epoch: [8/50]          || Step: [60/1579]       || Average Training Loss: 2.4345\n",
      "Epoch: [8/50]          || Step: [80/1579]       || Average Training Loss: 2.4257\n",
      "Epoch: [8/50]          || Step: [100/1579]      || Average Training Loss: 2.4193\n",
      "Epoch: [8/50]          || Step: [120/1579]      || Average Training Loss: 2.4173\n",
      "Epoch: [8/50]          || Step: [140/1579]      || Average Training Loss: 2.4084\n",
      "Epoch: [8/50]          || Step: [160/1579]      || Average Training Loss: 2.4138\n",
      "Epoch: [8/50]          || Step: [180/1579]      || Average Training Loss: 2.4156\n",
      "Epoch: [8/50]          || Step: [200/1579]      || Average Training Loss: 2.4150\n",
      "Epoch: [8/50]          || Step: [220/1579]      || Average Training Loss: 2.4130\n",
      "Epoch: [8/50]          || Step: [240/1579]      || Average Training Loss: 2.4096\n",
      "Epoch: [8/50]          || Step: [260/1579]      || Average Training Loss: 2.4109\n",
      "Epoch: [8/50]          || Step: [280/1579]      || Average Training Loss: 2.4116\n",
      "Epoch: [8/50]          || Step: [300/1579]      || Average Training Loss: 2.4140\n",
      "Epoch: [8/50]          || Step: [320/1579]      || Average Training Loss: 2.4105\n",
      "Epoch: [8/50]          || Step: [340/1579]      || Average Training Loss: 2.4104\n",
      "Epoch: [8/50]          || Step: [360/1579]      || Average Training Loss: 2.4093\n",
      "Epoch: [8/50]          || Step: [380/1579]      || Average Training Loss: 2.4091\n",
      "Epoch: [8/50]          || Step: [400/1579]      || Average Training Loss: 2.4081\n",
      "Epoch: [8/50]          || Step: [420/1579]      || Average Training Loss: 2.4051\n",
      "Epoch: [8/50]          || Step: [440/1579]      || Average Training Loss: 2.4060\n",
      "Epoch: [8/50]          || Step: [460/1579]      || Average Training Loss: 2.4045\n",
      "Epoch: [8/50]          || Step: [480/1579]      || Average Training Loss: 2.4037\n",
      "Epoch: [8/50]          || Step: [500/1579]      || Average Training Loss: 2.4037\n",
      "Epoch: [8/50]          || Step: [520/1579]      || Average Training Loss: 2.4048\n",
      "Epoch: [8/50]          || Step: [540/1579]      || Average Training Loss: 2.4045\n",
      "Epoch: [8/50]          || Step: [560/1579]      || Average Training Loss: 2.4023\n",
      "Epoch: [8/50]          || Step: [580/1579]      || Average Training Loss: 2.4016\n",
      "Epoch: [8/50]          || Step: [600/1579]      || Average Training Loss: 2.4020\n",
      "Epoch: [8/50]          || Step: [620/1579]      || Average Training Loss: 2.4015\n",
      "Epoch: [8/50]          || Step: [640/1579]      || Average Training Loss: 2.3987\n",
      "Epoch: [8/50]          || Step: [660/1579]      || Average Training Loss: 2.3974\n",
      "Epoch: [8/50]          || Step: [680/1579]      || Average Training Loss: 2.3982\n",
      "Epoch: [8/50]          || Step: [700/1579]      || Average Training Loss: 2.3985\n",
      "Epoch: [8/50]          || Step: [720/1579]      || Average Training Loss: 2.3980\n",
      "Epoch: [8/50]          || Step: [740/1579]      || Average Training Loss: 2.3988\n",
      "Epoch: [8/50]          || Step: [760/1579]      || Average Training Loss: 2.3996\n",
      "Epoch: [8/50]          || Step: [780/1579]      || Average Training Loss: 2.3995\n",
      "Epoch: [8/50]          || Step: [800/1579]      || Average Training Loss: 2.4006\n",
      "Epoch: [8/50]          || Step: [820/1579]      || Average Training Loss: 2.4006\n",
      "Epoch: [8/50]          || Step: [840/1579]      || Average Training Loss: 2.4000\n",
      "Epoch: [8/50]          || Step: [860/1579]      || Average Training Loss: 2.4001\n",
      "Epoch: [8/50]          || Step: [880/1579]      || Average Training Loss: 2.3994\n",
      "Epoch: [8/50]          || Step: [900/1579]      || Average Training Loss: 2.3994\n",
      "Epoch: [8/50]          || Step: [920/1579]      || Average Training Loss: 2.3986\n",
      "Epoch: [8/50]          || Step: [940/1579]      || Average Training Loss: 2.3974\n",
      "Epoch: [8/50]          || Step: [960/1579]      || Average Training Loss: 2.3973\n",
      "Epoch: [8/50]          || Step: [980/1579]      || Average Training Loss: 2.3965\n",
      "Epoch: [8/50]          || Step: [1000/1579]     || Average Training Loss: 2.3971\n",
      "Epoch: [8/50]          || Step: [1020/1579]     || Average Training Loss: 2.3964\n",
      "Epoch: [8/50]          || Step: [1040/1579]     || Average Training Loss: 2.3969\n",
      "Epoch: [8/50]          || Step: [1060/1579]     || Average Training Loss: 2.3966\n",
      "Epoch: [8/50]          || Step: [1080/1579]     || Average Training Loss: 2.3966\n",
      "Epoch: [8/50]          || Step: [1100/1579]     || Average Training Loss: 2.3975\n",
      "Epoch: [8/50]          || Step: [1120/1579]     || Average Training Loss: 2.3975\n",
      "Epoch: [8/50]          || Step: [1140/1579]     || Average Training Loss: 2.3977\n",
      "Epoch: [8/50]          || Step: [1160/1579]     || Average Training Loss: 2.3977\n",
      "Epoch: [8/50]          || Step: [1180/1579]     || Average Training Loss: 2.3973\n",
      "Epoch: [8/50]          || Step: [1200/1579]     || Average Training Loss: 2.3966\n",
      "Epoch: [8/50]          || Step: [1220/1579]     || Average Training Loss: 2.3971\n",
      "Epoch: [8/50]          || Step: [1240/1579]     || Average Training Loss: 2.3977\n",
      "Epoch: [8/50]          || Step: [1260/1579]     || Average Training Loss: 2.3975\n",
      "Epoch: [8/50]          || Step: [1280/1579]     || Average Training Loss: 2.3971\n",
      "Epoch: [8/50]          || Step: [1300/1579]     || Average Training Loss: 2.3976\n",
      "Epoch: [8/50]          || Step: [1320/1579]     || Average Training Loss: 2.3981\n",
      "Epoch: [8/50]          || Step: [1340/1579]     || Average Training Loss: 2.3983\n",
      "Epoch: [8/50]          || Step: [1360/1579]     || Average Training Loss: 2.3982\n",
      "Epoch: [8/50]          || Step: [1380/1579]     || Average Training Loss: 2.3982\n",
      "Epoch: [8/50]          || Step: [1400/1579]     || Average Training Loss: 2.3986\n",
      "Epoch: [8/50]          || Step: [1420/1579]     || Average Training Loss: 2.3982\n",
      "Epoch: [8/50]          || Step: [1440/1579]     || Average Training Loss: 2.3975\n",
      "Epoch: [8/50]          || Step: [1460/1579]     || Average Training Loss: 2.3971\n",
      "Epoch: [8/50]          || Step: [1480/1579]     || Average Training Loss: 2.3973\n",
      "Epoch: [8/50]          || Step: [1500/1579]     || Average Training Loss: 2.3966\n",
      "Epoch: [8/50]          || Step: [1520/1579]     || Average Training Loss: 2.3963\n",
      "Epoch: [8/50]          || Step: [1540/1579]     || Average Training Loss: 2.3961\n",
      "Epoch: [8/50]          || Step: [1560/1579]     || Average Training Loss: 2.3964\n",
      "Epoch: [8/50]          || Step: [0/142]         || Average Validation Loss: 2.4357\n",
      "Epoch: [8/50]          || Step: [20/142]        || Average Validation Loss: 2.4906\n",
      "Epoch: [8/50]          || Step: [40/142]        || Average Validation Loss: 2.5189\n",
      "Epoch: [8/50]          || Step: [60/142]        || Average Validation Loss: 2.5022\n",
      "Epoch: [8/50]          || Step: [80/142]        || Average Validation Loss: 2.5077\n",
      "Epoch: [8/50]          || Step: [100/142]       || Average Validation Loss: 2.5040\n",
      "Epoch: [8/50]          || Step: [120/142]       || Average Validation Loss: 2.5052\n",
      "Epoch: [8/50]          || Step: [140/142]       || Average Validation Loss: 2.5047\n",
      "****************************************************************************************************\n",
      "Epoch: [8/50] || Training Loss = 2.40 || Validation Loss: 2.50 || Time: 21.649547\n",
      "****************************************************************************************************\n",
      "Epoch: [9/50]          || Step: [0/1579]        || Average Training Loss: 2.6482\n",
      "Epoch: [9/50]          || Step: [20/1579]       || Average Training Loss: 2.4803\n",
      "Epoch: [9/50]          || Step: [40/1579]       || Average Training Loss: 2.4613\n",
      "Epoch: [9/50]          || Step: [60/1579]       || Average Training Loss: 2.4530\n",
      "Epoch: [9/50]          || Step: [80/1579]       || Average Training Loss: 2.4409\n",
      "Epoch: [9/50]          || Step: [100/1579]      || Average Training Loss: 2.4359\n",
      "Epoch: [9/50]          || Step: [120/1579]      || Average Training Loss: 2.4267\n",
      "Epoch: [9/50]          || Step: [140/1579]      || Average Training Loss: 2.4196\n",
      "Epoch: [9/50]          || Step: [160/1579]      || Average Training Loss: 2.4217\n",
      "Epoch: [9/50]          || Step: [180/1579]      || Average Training Loss: 2.4168\n",
      "Epoch: [9/50]          || Step: [200/1579]      || Average Training Loss: 2.4112\n",
      "Epoch: [9/50]          || Step: [220/1579]      || Average Training Loss: 2.4059\n",
      "Epoch: [9/50]          || Step: [240/1579]      || Average Training Loss: 2.4039\n",
      "Epoch: [9/50]          || Step: [260/1579]      || Average Training Loss: 2.4037\n",
      "Epoch: [9/50]          || Step: [280/1579]      || Average Training Loss: 2.4020\n",
      "Epoch: [9/50]          || Step: [300/1579]      || Average Training Loss: 2.4025\n",
      "Epoch: [9/50]          || Step: [320/1579]      || Average Training Loss: 2.4012\n",
      "Epoch: [9/50]          || Step: [340/1579]      || Average Training Loss: 2.4010\n",
      "Epoch: [9/50]          || Step: [360/1579]      || Average Training Loss: 2.4017\n",
      "Epoch: [9/50]          || Step: [380/1579]      || Average Training Loss: 2.4006\n",
      "Epoch: [9/50]          || Step: [400/1579]      || Average Training Loss: 2.3995\n",
      "Epoch: [9/50]          || Step: [420/1579]      || Average Training Loss: 2.4003\n",
      "Epoch: [9/50]          || Step: [440/1579]      || Average Training Loss: 2.3999\n",
      "Epoch: [9/50]          || Step: [460/1579]      || Average Training Loss: 2.3990\n",
      "Epoch: [9/50]          || Step: [480/1579]      || Average Training Loss: 2.4016\n",
      "Epoch: [9/50]          || Step: [500/1579]      || Average Training Loss: 2.4017\n",
      "Epoch: [9/50]          || Step: [520/1579]      || Average Training Loss: 2.4005\n",
      "Epoch: [9/50]          || Step: [540/1579]      || Average Training Loss: 2.4005\n",
      "Epoch: [9/50]          || Step: [560/1579]      || Average Training Loss: 2.4000\n",
      "Epoch: [9/50]          || Step: [580/1579]      || Average Training Loss: 2.4019\n",
      "Epoch: [9/50]          || Step: [600/1579]      || Average Training Loss: 2.4021\n",
      "Epoch: [9/50]          || Step: [620/1579]      || Average Training Loss: 2.4012\n",
      "Epoch: [9/50]          || Step: [640/1579]      || Average Training Loss: 2.4003\n",
      "Epoch: [9/50]          || Step: [660/1579]      || Average Training Loss: 2.4003\n",
      "Epoch: [9/50]          || Step: [680/1579]      || Average Training Loss: 2.4006\n",
      "Epoch: [9/50]          || Step: [700/1579]      || Average Training Loss: 2.4010\n",
      "Epoch: [9/50]          || Step: [720/1579]      || Average Training Loss: 2.4006\n",
      "Epoch: [9/50]          || Step: [740/1579]      || Average Training Loss: 2.4003\n",
      "Epoch: [9/50]          || Step: [760/1579]      || Average Training Loss: 2.4005\n",
      "Epoch: [9/50]          || Step: [780/1579]      || Average Training Loss: 2.4013\n",
      "Epoch: [9/50]          || Step: [800/1579]      || Average Training Loss: 2.4004\n",
      "Epoch: [9/50]          || Step: [820/1579]      || Average Training Loss: 2.4006\n",
      "Epoch: [9/50]          || Step: [840/1579]      || Average Training Loss: 2.4003\n",
      "Epoch: [9/50]          || Step: [860/1579]      || Average Training Loss: 2.4000\n",
      "Epoch: [9/50]          || Step: [880/1579]      || Average Training Loss: 2.3997\n",
      "Epoch: [9/50]          || Step: [900/1579]      || Average Training Loss: 2.4000\n",
      "Epoch: [9/50]          || Step: [920/1579]      || Average Training Loss: 2.4003\n",
      "Epoch: [9/50]          || Step: [940/1579]      || Average Training Loss: 2.4001\n",
      "Epoch: [9/50]          || Step: [960/1579]      || Average Training Loss: 2.3995\n",
      "Epoch: [9/50]          || Step: [980/1579]      || Average Training Loss: 2.4003\n",
      "Epoch: [9/50]          || Step: [1000/1579]     || Average Training Loss: 2.3994\n",
      "Epoch: [9/50]          || Step: [1020/1579]     || Average Training Loss: 2.3986\n",
      "Epoch: [9/50]          || Step: [1040/1579]     || Average Training Loss: 2.3984\n",
      "Epoch: [9/50]          || Step: [1060/1579]     || Average Training Loss: 2.3984\n",
      "Epoch: [9/50]          || Step: [1080/1579]     || Average Training Loss: 2.3994\n",
      "Epoch: [9/50]          || Step: [1100/1579]     || Average Training Loss: 2.3988\n",
      "Epoch: [9/50]          || Step: [1120/1579]     || Average Training Loss: 2.3992\n",
      "Epoch: [9/50]          || Step: [1140/1579]     || Average Training Loss: 2.3998\n",
      "Epoch: [9/50]          || Step: [1160/1579]     || Average Training Loss: 2.4002\n",
      "Epoch: [9/50]          || Step: [1180/1579]     || Average Training Loss: 2.3999\n",
      "Epoch: [9/50]          || Step: [1200/1579]     || Average Training Loss: 2.3987\n",
      "Epoch: [9/50]          || Step: [1220/1579]     || Average Training Loss: 2.3993\n",
      "Epoch: [9/50]          || Step: [1240/1579]     || Average Training Loss: 2.3999\n",
      "Epoch: [9/50]          || Step: [1260/1579]     || Average Training Loss: 2.3998\n",
      "Epoch: [9/50]          || Step: [1280/1579]     || Average Training Loss: 2.3996\n",
      "Epoch: [9/50]          || Step: [1300/1579]     || Average Training Loss: 2.3984\n",
      "Epoch: [9/50]          || Step: [1320/1579]     || Average Training Loss: 2.3989\n",
      "Epoch: [9/50]          || Step: [1340/1579]     || Average Training Loss: 2.3991\n",
      "Epoch: [9/50]          || Step: [1360/1579]     || Average Training Loss: 2.3990\n",
      "Epoch: [9/50]          || Step: [1380/1579]     || Average Training Loss: 2.3988\n",
      "Epoch: [9/50]          || Step: [1400/1579]     || Average Training Loss: 2.3983\n",
      "Epoch: [9/50]          || Step: [1420/1579]     || Average Training Loss: 2.3990\n",
      "Epoch: [9/50]          || Step: [1440/1579]     || Average Training Loss: 2.3982\n",
      "Epoch: [9/50]          || Step: [1460/1579]     || Average Training Loss: 2.3982\n",
      "Epoch: [9/50]          || Step: [1480/1579]     || Average Training Loss: 2.3986\n",
      "Epoch: [9/50]          || Step: [1500/1579]     || Average Training Loss: 2.3984\n",
      "Epoch: [9/50]          || Step: [1520/1579]     || Average Training Loss: 2.3982\n",
      "Epoch: [9/50]          || Step: [1540/1579]     || Average Training Loss: 2.3982\n",
      "Epoch: [9/50]          || Step: [1560/1579]     || Average Training Loss: 2.3967\n",
      "Epoch: [9/50]          || Step: [0/142]         || Average Validation Loss: 2.3718\n",
      "Epoch: [9/50]          || Step: [20/142]        || Average Validation Loss: 2.4710\n",
      "Epoch: [9/50]          || Step: [40/142]        || Average Validation Loss: 2.4870\n",
      "Epoch: [9/50]          || Step: [60/142]        || Average Validation Loss: 2.4773\n",
      "Epoch: [9/50]          || Step: [80/142]        || Average Validation Loss: 2.4810\n",
      "Epoch: [9/50]          || Step: [100/142]       || Average Validation Loss: 2.4717\n",
      "Epoch: [9/50]          || Step: [120/142]       || Average Validation Loss: 2.4628\n",
      "Epoch: [9/50]          || Step: [140/142]       || Average Validation Loss: 2.4564\n",
      "****************************************************************************************************\n",
      "Epoch: [9/50] || Training Loss = 2.40 || Validation Loss: 2.46 || Time: 21.983140\n",
      "****************************************************************************************************\n",
      "Epoch: [10/50]         || Step: [0/1579]        || Average Training Loss: 2.4878\n",
      "Epoch: [10/50]         || Step: [20/1579]       || Average Training Loss: 2.4251\n",
      "Epoch: [10/50]         || Step: [40/1579]       || Average Training Loss: 2.3966\n",
      "Epoch: [10/50]         || Step: [60/1579]       || Average Training Loss: 2.3780\n",
      "Epoch: [10/50]         || Step: [80/1579]       || Average Training Loss: 2.3716\n",
      "Epoch: [10/50]         || Step: [100/1579]      || Average Training Loss: 2.3872\n",
      "Epoch: [10/50]         || Step: [120/1579]      || Average Training Loss: 2.3884\n",
      "Epoch: [10/50]         || Step: [140/1579]      || Average Training Loss: 2.3830\n",
      "Epoch: [10/50]         || Step: [160/1579]      || Average Training Loss: 2.3881\n",
      "Epoch: [10/50]         || Step: [180/1579]      || Average Training Loss: 2.3854\n",
      "Epoch: [10/50]         || Step: [200/1579]      || Average Training Loss: 2.3862\n",
      "Epoch: [10/50]         || Step: [220/1579]      || Average Training Loss: 2.3873\n",
      "Epoch: [10/50]         || Step: [240/1579]      || Average Training Loss: 2.3851\n",
      "Epoch: [10/50]         || Step: [260/1579]      || Average Training Loss: 2.3830\n",
      "Epoch: [10/50]         || Step: [280/1579]      || Average Training Loss: 2.3842\n",
      "Epoch: [10/50]         || Step: [300/1579]      || Average Training Loss: 2.3810\n",
      "Epoch: [10/50]         || Step: [320/1579]      || Average Training Loss: 2.3806\n",
      "Epoch: [10/50]         || Step: [340/1579]      || Average Training Loss: 2.3832\n",
      "Epoch: [10/50]         || Step: [360/1579]      || Average Training Loss: 2.3831\n",
      "Epoch: [10/50]         || Step: [380/1579]      || Average Training Loss: 2.3825\n",
      "Epoch: [10/50]         || Step: [400/1579]      || Average Training Loss: 2.3825\n",
      "Epoch: [10/50]         || Step: [420/1579]      || Average Training Loss: 2.3828\n",
      "Epoch: [10/50]         || Step: [440/1579]      || Average Training Loss: 2.3820\n",
      "Epoch: [10/50]         || Step: [460/1579]      || Average Training Loss: 2.3823\n",
      "Epoch: [10/50]         || Step: [480/1579]      || Average Training Loss: 2.3843\n",
      "Epoch: [10/50]         || Step: [500/1579]      || Average Training Loss: 2.3847\n",
      "Epoch: [10/50]         || Step: [520/1579]      || Average Training Loss: 2.3869\n",
      "Epoch: [10/50]         || Step: [540/1579]      || Average Training Loss: 2.3859\n",
      "Epoch: [10/50]         || Step: [560/1579]      || Average Training Loss: 2.3850\n",
      "Epoch: [10/50]         || Step: [580/1579]      || Average Training Loss: 2.3866\n",
      "Epoch: [10/50]         || Step: [600/1579]      || Average Training Loss: 2.3871\n",
      "Epoch: [10/50]         || Step: [620/1579]      || Average Training Loss: 2.3873\n",
      "Epoch: [10/50]         || Step: [640/1579]      || Average Training Loss: 2.3873\n",
      "Epoch: [10/50]         || Step: [660/1579]      || Average Training Loss: 2.3867\n",
      "Epoch: [10/50]         || Step: [680/1579]      || Average Training Loss: 2.3859\n",
      "Epoch: [10/50]         || Step: [700/1579]      || Average Training Loss: 2.3852\n",
      "Epoch: [10/50]         || Step: [720/1579]      || Average Training Loss: 2.3862\n",
      "Epoch: [10/50]         || Step: [740/1579]      || Average Training Loss: 2.3857\n",
      "Epoch: [10/50]         || Step: [760/1579]      || Average Training Loss: 2.3861\n",
      "Epoch: [10/50]         || Step: [780/1579]      || Average Training Loss: 2.3869\n",
      "Epoch: [10/50]         || Step: [800/1579]      || Average Training Loss: 2.3867\n",
      "Epoch: [10/50]         || Step: [820/1579]      || Average Training Loss: 2.3877\n",
      "Epoch: [10/50]         || Step: [840/1579]      || Average Training Loss: 2.3874\n",
      "Epoch: [10/50]         || Step: [860/1579]      || Average Training Loss: 2.3870\n",
      "Epoch: [10/50]         || Step: [880/1579]      || Average Training Loss: 2.3870\n",
      "Epoch: [10/50]         || Step: [900/1579]      || Average Training Loss: 2.3872\n",
      "Epoch: [10/50]         || Step: [920/1579]      || Average Training Loss: 2.3866\n",
      "Epoch: [10/50]         || Step: [940/1579]      || Average Training Loss: 2.3872\n",
      "Epoch: [10/50]         || Step: [960/1579]      || Average Training Loss: 2.3875\n",
      "Epoch: [10/50]         || Step: [980/1579]      || Average Training Loss: 2.3877\n",
      "Epoch: [10/50]         || Step: [1000/1579]     || Average Training Loss: 2.3868\n",
      "Epoch: [10/50]         || Step: [1020/1579]     || Average Training Loss: 2.3878\n",
      "Epoch: [10/50]         || Step: [1040/1579]     || Average Training Loss: 2.3878\n",
      "Epoch: [10/50]         || Step: [1060/1579]     || Average Training Loss: 2.3882\n",
      "Epoch: [10/50]         || Step: [1080/1579]     || Average Training Loss: 2.3879\n",
      "Epoch: [10/50]         || Step: [1100/1579]     || Average Training Loss: 2.3882\n",
      "Epoch: [10/50]         || Step: [1120/1579]     || Average Training Loss: 2.3883\n",
      "Epoch: [10/50]         || Step: [1140/1579]     || Average Training Loss: 2.3886\n",
      "Epoch: [10/50]         || Step: [1160/1579]     || Average Training Loss: 2.3894\n",
      "Epoch: [10/50]         || Step: [1180/1579]     || Average Training Loss: 2.3895\n",
      "Epoch: [10/50]         || Step: [1200/1579]     || Average Training Loss: 2.3888\n",
      "Epoch: [10/50]         || Step: [1220/1579]     || Average Training Loss: 2.3892\n",
      "Epoch: [10/50]         || Step: [1240/1579]     || Average Training Loss: 2.3892\n",
      "Epoch: [10/50]         || Step: [1260/1579]     || Average Training Loss: 2.3882\n",
      "Epoch: [10/50]         || Step: [1280/1579]     || Average Training Loss: 2.3887\n",
      "Epoch: [10/50]         || Step: [1300/1579]     || Average Training Loss: 2.3891\n",
      "Epoch: [10/50]         || Step: [1320/1579]     || Average Training Loss: 2.3898\n",
      "Epoch: [10/50]         || Step: [1340/1579]     || Average Training Loss: 2.3895\n",
      "Epoch: [10/50]         || Step: [1360/1579]     || Average Training Loss: 2.3886\n",
      "Epoch: [10/50]         || Step: [1380/1579]     || Average Training Loss: 2.3884\n",
      "Epoch: [10/50]         || Step: [1400/1579]     || Average Training Loss: 2.3889\n",
      "Epoch: [10/50]         || Step: [1420/1579]     || Average Training Loss: 2.3892\n",
      "Epoch: [10/50]         || Step: [1440/1579]     || Average Training Loss: 2.3898\n",
      "Epoch: [10/50]         || Step: [1460/1579]     || Average Training Loss: 2.3906\n",
      "Epoch: [10/50]         || Step: [1480/1579]     || Average Training Loss: 2.3905\n",
      "Epoch: [10/50]         || Step: [1500/1579]     || Average Training Loss: 2.3905\n",
      "Epoch: [10/50]         || Step: [1520/1579]     || Average Training Loss: 2.3904\n",
      "Epoch: [10/50]         || Step: [1540/1579]     || Average Training Loss: 2.3912\n",
      "Epoch: [10/50]         || Step: [1560/1579]     || Average Training Loss: 2.3905\n",
      "Epoch: [10/50]         || Step: [0/142]         || Average Validation Loss: 2.0992\n",
      "Epoch: [10/50]         || Step: [20/142]        || Average Validation Loss: 2.3775\n",
      "Epoch: [10/50]         || Step: [40/142]        || Average Validation Loss: 2.3896\n",
      "Epoch: [10/50]         || Step: [60/142]        || Average Validation Loss: 2.3883\n",
      "Epoch: [10/50]         || Step: [80/142]        || Average Validation Loss: 2.3898\n",
      "Epoch: [10/50]         || Step: [100/142]       || Average Validation Loss: 2.3858\n",
      "Epoch: [10/50]         || Step: [120/142]       || Average Validation Loss: 2.3805\n",
      "Epoch: [10/50]         || Step: [140/142]       || Average Validation Loss: 2.3748\n",
      "****************************************************************************************************\n",
      "Epoch: [10/50] || Training Loss = 2.39 || Validation Loss: 2.38 || Time: 21.983850\n",
      "****************************************************************************************************\n",
      "Epoch: [11/50]         || Step: [0/1579]        || Average Training Loss: 2.3746\n",
      "Epoch: [11/50]         || Step: [20/1579]       || Average Training Loss: 2.4040\n",
      "Epoch: [11/50]         || Step: [40/1579]       || Average Training Loss: 2.4051\n",
      "Epoch: [11/50]         || Step: [60/1579]       || Average Training Loss: 2.3955\n",
      "Epoch: [11/50]         || Step: [80/1579]       || Average Training Loss: 2.3880\n",
      "Epoch: [11/50]         || Step: [100/1579]      || Average Training Loss: 2.3784\n",
      "Epoch: [11/50]         || Step: [120/1579]      || Average Training Loss: 2.3856\n",
      "Epoch: [11/50]         || Step: [140/1579]      || Average Training Loss: 2.3890\n",
      "Epoch: [11/50]         || Step: [160/1579]      || Average Training Loss: 2.3818\n",
      "Epoch: [11/50]         || Step: [180/1579]      || Average Training Loss: 2.3817\n",
      "Epoch: [11/50]         || Step: [200/1579]      || Average Training Loss: 2.3823\n",
      "Epoch: [11/50]         || Step: [220/1579]      || Average Training Loss: 2.3863\n",
      "Epoch: [11/50]         || Step: [240/1579]      || Average Training Loss: 2.3844\n",
      "Epoch: [11/50]         || Step: [260/1579]      || Average Training Loss: 2.3871\n",
      "Epoch: [11/50]         || Step: [280/1579]      || Average Training Loss: 2.3846\n",
      "Epoch: [11/50]         || Step: [300/1579]      || Average Training Loss: 2.3870\n",
      "Epoch: [11/50]         || Step: [320/1579]      || Average Training Loss: 2.3866\n",
      "Epoch: [11/50]         || Step: [340/1579]      || Average Training Loss: 2.3873\n",
      "Epoch: [11/50]         || Step: [360/1579]      || Average Training Loss: 2.3857\n",
      "Epoch: [11/50]         || Step: [380/1579]      || Average Training Loss: 2.3860\n",
      "Epoch: [11/50]         || Step: [400/1579]      || Average Training Loss: 2.3853\n",
      "Epoch: [11/50]         || Step: [420/1579]      || Average Training Loss: 2.3835\n",
      "Epoch: [11/50]         || Step: [440/1579]      || Average Training Loss: 2.3849\n",
      "Epoch: [11/50]         || Step: [460/1579]      || Average Training Loss: 2.3842\n",
      "Epoch: [11/50]         || Step: [480/1579]      || Average Training Loss: 2.3838\n",
      "Epoch: [11/50]         || Step: [500/1579]      || Average Training Loss: 2.3831\n",
      "Epoch: [11/50]         || Step: [520/1579]      || Average Training Loss: 2.3847\n",
      "Epoch: [11/50]         || Step: [540/1579]      || Average Training Loss: 2.3844\n",
      "Epoch: [11/50]         || Step: [560/1579]      || Average Training Loss: 2.3840\n",
      "Epoch: [11/50]         || Step: [580/1579]      || Average Training Loss: 2.3836\n",
      "Epoch: [11/50]         || Step: [600/1579]      || Average Training Loss: 2.3821\n",
      "Epoch: [11/50]         || Step: [620/1579]      || Average Training Loss: 2.3854\n",
      "Epoch: [11/50]         || Step: [640/1579]      || Average Training Loss: 2.3852\n",
      "Epoch: [11/50]         || Step: [660/1579]      || Average Training Loss: 2.3848\n",
      "Epoch: [11/50]         || Step: [680/1579]      || Average Training Loss: 2.3843\n",
      "Epoch: [11/50]         || Step: [700/1579]      || Average Training Loss: 2.3842\n",
      "Epoch: [11/50]         || Step: [720/1579]      || Average Training Loss: 2.3837\n",
      "Epoch: [11/50]         || Step: [740/1579]      || Average Training Loss: 2.3850\n",
      "Epoch: [11/50]         || Step: [760/1579]      || Average Training Loss: 2.3866\n",
      "Epoch: [11/50]         || Step: [780/1579]      || Average Training Loss: 2.3867\n",
      "Epoch: [11/50]         || Step: [800/1579]      || Average Training Loss: 2.3873\n",
      "Epoch: [11/50]         || Step: [820/1579]      || Average Training Loss: 2.3871\n",
      "Epoch: [11/50]         || Step: [840/1579]      || Average Training Loss: 2.3859\n",
      "Epoch: [11/50]         || Step: [860/1579]      || Average Training Loss: 2.3854\n",
      "Epoch: [11/50]         || Step: [880/1579]      || Average Training Loss: 2.3854\n",
      "Epoch: [11/50]         || Step: [900/1579]      || Average Training Loss: 2.3843\n",
      "Epoch: [11/50]         || Step: [920/1579]      || Average Training Loss: 2.3833\n",
      "Epoch: [11/50]         || Step: [940/1579]      || Average Training Loss: 2.3838\n",
      "Epoch: [11/50]         || Step: [960/1579]      || Average Training Loss: 2.3845\n",
      "Epoch: [11/50]         || Step: [980/1579]      || Average Training Loss: 2.3843\n",
      "Epoch: [11/50]         || Step: [1000/1579]     || Average Training Loss: 2.3848\n",
      "Epoch: [11/50]         || Step: [1020/1579]     || Average Training Loss: 2.3848\n",
      "Epoch: [11/50]         || Step: [1040/1579]     || Average Training Loss: 2.3839\n",
      "Epoch: [11/50]         || Step: [1060/1579]     || Average Training Loss: 2.3842\n",
      "Epoch: [11/50]         || Step: [1080/1579]     || Average Training Loss: 2.3842\n",
      "Epoch: [11/50]         || Step: [1100/1579]     || Average Training Loss: 2.3845\n",
      "Epoch: [11/50]         || Step: [1120/1579]     || Average Training Loss: 2.3842\n",
      "Epoch: [11/50]         || Step: [1140/1579]     || Average Training Loss: 2.3839\n",
      "Epoch: [11/50]         || Step: [1160/1579]     || Average Training Loss: 2.3840\n",
      "Epoch: [11/50]         || Step: [1180/1579]     || Average Training Loss: 2.3842\n",
      "Epoch: [11/50]         || Step: [1200/1579]     || Average Training Loss: 2.3845\n",
      "Epoch: [11/50]         || Step: [1220/1579]     || Average Training Loss: 2.3854\n",
      "Epoch: [11/50]         || Step: [1240/1579]     || Average Training Loss: 2.3862\n",
      "Epoch: [11/50]         || Step: [1260/1579]     || Average Training Loss: 2.3855\n",
      "Epoch: [11/50]         || Step: [1280/1579]     || Average Training Loss: 2.3846\n",
      "Epoch: [11/50]         || Step: [1300/1579]     || Average Training Loss: 2.3843\n",
      "Epoch: [11/50]         || Step: [1320/1579]     || Average Training Loss: 2.3838\n",
      "Epoch: [11/50]         || Step: [1340/1579]     || Average Training Loss: 2.3834\n",
      "Epoch: [11/50]         || Step: [1360/1579]     || Average Training Loss: 2.3835\n",
      "Epoch: [11/50]         || Step: [1380/1579]     || Average Training Loss: 2.3829\n",
      "Epoch: [11/50]         || Step: [1400/1579]     || Average Training Loss: 2.3830\n",
      "Epoch: [11/50]         || Step: [1420/1579]     || Average Training Loss: 2.3828\n",
      "Epoch: [11/50]         || Step: [1440/1579]     || Average Training Loss: 2.3831\n",
      "Epoch: [11/50]         || Step: [1460/1579]     || Average Training Loss: 2.3837\n",
      "Epoch: [11/50]         || Step: [1480/1579]     || Average Training Loss: 2.3832\n",
      "Epoch: [11/50]         || Step: [1500/1579]     || Average Training Loss: 2.3831\n",
      "Epoch: [11/50]         || Step: [1520/1579]     || Average Training Loss: 2.3825\n",
      "Epoch: [11/50]         || Step: [1540/1579]     || Average Training Loss: 2.3831\n",
      "Epoch: [11/50]         || Step: [1560/1579]     || Average Training Loss: 2.3841\n",
      "Epoch: [11/50]         || Step: [0/142]         || Average Validation Loss: 2.4994\n",
      "Epoch: [11/50]         || Step: [20/142]        || Average Validation Loss: 2.4123\n",
      "Epoch: [11/50]         || Step: [40/142]        || Average Validation Loss: 2.3866\n",
      "Epoch: [11/50]         || Step: [60/142]        || Average Validation Loss: 2.3779\n",
      "Epoch: [11/50]         || Step: [80/142]        || Average Validation Loss: 2.3732\n",
      "Epoch: [11/50]         || Step: [100/142]       || Average Validation Loss: 2.3654\n",
      "Epoch: [11/50]         || Step: [120/142]       || Average Validation Loss: 2.3656\n",
      "Epoch: [11/50]         || Step: [140/142]       || Average Validation Loss: 2.3632\n",
      "****************************************************************************************************\n",
      "Epoch: [11/50] || Training Loss = 2.38 || Validation Loss: 2.36 || Time: 21.794364\n",
      "****************************************************************************************************\n",
      "Epoch: [12/50]         || Step: [0/1579]        || Average Training Loss: 2.2130\n",
      "Epoch: [12/50]         || Step: [20/1579]       || Average Training Loss: 2.3910\n",
      "Epoch: [12/50]         || Step: [40/1579]       || Average Training Loss: 2.4062\n",
      "Epoch: [12/50]         || Step: [60/1579]       || Average Training Loss: 2.3946\n",
      "Epoch: [12/50]         || Step: [80/1579]       || Average Training Loss: 2.4006\n",
      "Epoch: [12/50]         || Step: [100/1579]      || Average Training Loss: 2.4003\n",
      "Epoch: [12/50]         || Step: [120/1579]      || Average Training Loss: 2.3933\n",
      "Epoch: [12/50]         || Step: [140/1579]      || Average Training Loss: 2.3825\n",
      "Epoch: [12/50]         || Step: [160/1579]      || Average Training Loss: 2.3784\n",
      "Epoch: [12/50]         || Step: [180/1579]      || Average Training Loss: 2.3790\n",
      "Epoch: [12/50]         || Step: [200/1579]      || Average Training Loss: 2.3820\n",
      "Epoch: [12/50]         || Step: [220/1579]      || Average Training Loss: 2.3771\n",
      "Epoch: [12/50]         || Step: [240/1579]      || Average Training Loss: 2.3741\n",
      "Epoch: [12/50]         || Step: [260/1579]      || Average Training Loss: 2.3766\n",
      "Epoch: [12/50]         || Step: [280/1579]      || Average Training Loss: 2.3757\n",
      "Epoch: [12/50]         || Step: [300/1579]      || Average Training Loss: 2.3764\n",
      "Epoch: [12/50]         || Step: [320/1579]      || Average Training Loss: 2.3736\n",
      "Epoch: [12/50]         || Step: [340/1579]      || Average Training Loss: 2.3716\n",
      "Epoch: [12/50]         || Step: [360/1579]      || Average Training Loss: 2.3727\n",
      "Epoch: [12/50]         || Step: [380/1579]      || Average Training Loss: 2.3718\n",
      "Epoch: [12/50]         || Step: [400/1579]      || Average Training Loss: 2.3719\n",
      "Epoch: [12/50]         || Step: [420/1579]      || Average Training Loss: 2.3721\n",
      "Epoch: [12/50]         || Step: [440/1579]      || Average Training Loss: 2.3735\n",
      "Epoch: [12/50]         || Step: [460/1579]      || Average Training Loss: 2.3745\n",
      "Epoch: [12/50]         || Step: [480/1579]      || Average Training Loss: 2.3733\n",
      "Epoch: [12/50]         || Step: [500/1579]      || Average Training Loss: 2.3767\n",
      "Epoch: [12/50]         || Step: [520/1579]      || Average Training Loss: 2.3770\n",
      "Epoch: [12/50]         || Step: [540/1579]      || Average Training Loss: 2.3782\n",
      "Epoch: [12/50]         || Step: [560/1579]      || Average Training Loss: 2.3781\n",
      "Epoch: [12/50]         || Step: [580/1579]      || Average Training Loss: 2.3785\n",
      "Epoch: [12/50]         || Step: [600/1579]      || Average Training Loss: 2.3786\n",
      "Epoch: [12/50]         || Step: [620/1579]      || Average Training Loss: 2.3775\n",
      "Epoch: [12/50]         || Step: [640/1579]      || Average Training Loss: 2.3784\n",
      "Epoch: [12/50]         || Step: [660/1579]      || Average Training Loss: 2.3781\n",
      "Epoch: [12/50]         || Step: [680/1579]      || Average Training Loss: 2.3779\n",
      "Epoch: [12/50]         || Step: [700/1579]      || Average Training Loss: 2.3784\n",
      "Epoch: [12/50]         || Step: [720/1579]      || Average Training Loss: 2.3788\n",
      "Epoch: [12/50]         || Step: [740/1579]      || Average Training Loss: 2.3798\n",
      "Epoch: [12/50]         || Step: [760/1579]      || Average Training Loss: 2.3803\n",
      "Epoch: [12/50]         || Step: [780/1579]      || Average Training Loss: 2.3803\n",
      "Epoch: [12/50]         || Step: [800/1579]      || Average Training Loss: 2.3804\n",
      "Epoch: [12/50]         || Step: [820/1579]      || Average Training Loss: 2.3797\n",
      "Epoch: [12/50]         || Step: [840/1579]      || Average Training Loss: 2.3802\n",
      "Epoch: [12/50]         || Step: [860/1579]      || Average Training Loss: 2.3815\n",
      "Epoch: [12/50]         || Step: [880/1579]      || Average Training Loss: 2.3824\n",
      "Epoch: [12/50]         || Step: [900/1579]      || Average Training Loss: 2.3825\n",
      "Epoch: [12/50]         || Step: [920/1579]      || Average Training Loss: 2.3818\n",
      "Epoch: [12/50]         || Step: [940/1579]      || Average Training Loss: 2.3823\n",
      "Epoch: [12/50]         || Step: [960/1579]      || Average Training Loss: 2.3823\n",
      "Epoch: [12/50]         || Step: [980/1579]      || Average Training Loss: 2.3823\n",
      "Epoch: [12/50]         || Step: [1000/1579]     || Average Training Loss: 2.3823\n",
      "Epoch: [12/50]         || Step: [1020/1579]     || Average Training Loss: 2.3825\n",
      "Epoch: [12/50]         || Step: [1040/1579]     || Average Training Loss: 2.3826\n",
      "Epoch: [12/50]         || Step: [1060/1579]     || Average Training Loss: 2.3827\n",
      "Epoch: [12/50]         || Step: [1080/1579]     || Average Training Loss: 2.3835\n",
      "Epoch: [12/50]         || Step: [1100/1579]     || Average Training Loss: 2.3831\n",
      "Epoch: [12/50]         || Step: [1120/1579]     || Average Training Loss: 2.3833\n",
      "Epoch: [12/50]         || Step: [1140/1579]     || Average Training Loss: 2.3832\n",
      "Epoch: [12/50]         || Step: [1160/1579]     || Average Training Loss: 2.3830\n",
      "Epoch: [12/50]         || Step: [1180/1579]     || Average Training Loss: 2.3836\n",
      "Epoch: [12/50]         || Step: [1200/1579]     || Average Training Loss: 2.3831\n",
      "Epoch: [12/50]         || Step: [1220/1579]     || Average Training Loss: 2.3826\n",
      "Epoch: [12/50]         || Step: [1240/1579]     || Average Training Loss: 2.3828\n",
      "Epoch: [12/50]         || Step: [1260/1579]     || Average Training Loss: 2.3828\n",
      "Epoch: [12/50]         || Step: [1280/1579]     || Average Training Loss: 2.3830\n",
      "Epoch: [12/50]         || Step: [1300/1579]     || Average Training Loss: 2.3828\n",
      "Epoch: [12/50]         || Step: [1320/1579]     || Average Training Loss: 2.3831\n",
      "Epoch: [12/50]         || Step: [1340/1579]     || Average Training Loss: 2.3835\n",
      "Epoch: [12/50]         || Step: [1360/1579]     || Average Training Loss: 2.3841\n",
      "Epoch: [12/50]         || Step: [1380/1579]     || Average Training Loss: 2.3837\n",
      "Epoch: [12/50]         || Step: [1400/1579]     || Average Training Loss: 2.3842\n",
      "Epoch: [12/50]         || Step: [1420/1579]     || Average Training Loss: 2.3842\n",
      "Epoch: [12/50]         || Step: [1440/1579]     || Average Training Loss: 2.3843\n",
      "Epoch: [12/50]         || Step: [1460/1579]     || Average Training Loss: 2.3841\n",
      "Epoch: [12/50]         || Step: [1480/1579]     || Average Training Loss: 2.3842\n",
      "Epoch: [12/50]         || Step: [1500/1579]     || Average Training Loss: 2.3837\n",
      "Epoch: [12/50]         || Step: [1520/1579]     || Average Training Loss: 2.3837\n",
      "Epoch: [12/50]         || Step: [1540/1579]     || Average Training Loss: 2.3838\n",
      "Epoch: [12/50]         || Step: [1560/1579]     || Average Training Loss: 2.3835\n",
      "Epoch: [12/50]         || Step: [0/142]         || Average Validation Loss: 2.3257\n",
      "Epoch: [12/50]         || Step: [20/142]        || Average Validation Loss: 2.3790\n",
      "Epoch: [12/50]         || Step: [40/142]        || Average Validation Loss: 2.3923\n",
      "Epoch: [12/50]         || Step: [60/142]        || Average Validation Loss: 2.3955\n",
      "Epoch: [12/50]         || Step: [80/142]        || Average Validation Loss: 2.4067\n",
      "Epoch: [12/50]         || Step: [100/142]       || Average Validation Loss: 2.3957\n",
      "Epoch: [12/50]         || Step: [120/142]       || Average Validation Loss: 2.3961\n",
      "Epoch: [12/50]         || Step: [140/142]       || Average Validation Loss: 2.3993\n",
      "****************************************************************************************************\n",
      "Epoch: [12/50] || Training Loss = 2.38 || Validation Loss: 2.40 || Time: 22.079647\n",
      "****************************************************************************************************\n",
      "Epoch: [13/50]         || Step: [0/1579]        || Average Training Loss: 2.5254\n",
      "Epoch: [13/50]         || Step: [20/1579]       || Average Training Loss: 2.4498\n",
      "Epoch: [13/50]         || Step: [40/1579]       || Average Training Loss: 2.4214\n",
      "Epoch: [13/50]         || Step: [60/1579]       || Average Training Loss: 2.4085\n",
      "Epoch: [13/50]         || Step: [80/1579]       || Average Training Loss: 2.4005\n",
      "Epoch: [13/50]         || Step: [100/1579]      || Average Training Loss: 2.3977\n",
      "Epoch: [13/50]         || Step: [120/1579]      || Average Training Loss: 2.3972\n",
      "Epoch: [13/50]         || Step: [140/1579]      || Average Training Loss: 2.3953\n",
      "Epoch: [13/50]         || Step: [160/1579]      || Average Training Loss: 2.3943\n",
      "Epoch: [13/50]         || Step: [180/1579]      || Average Training Loss: 2.3878\n",
      "Epoch: [13/50]         || Step: [200/1579]      || Average Training Loss: 2.3867\n",
      "Epoch: [13/50]         || Step: [220/1579]      || Average Training Loss: 2.3857\n",
      "Epoch: [13/50]         || Step: [240/1579]      || Average Training Loss: 2.3837\n",
      "Epoch: [13/50]         || Step: [260/1579]      || Average Training Loss: 2.3778\n",
      "Epoch: [13/50]         || Step: [280/1579]      || Average Training Loss: 2.3767\n",
      "Epoch: [13/50]         || Step: [300/1579]      || Average Training Loss: 2.3783\n",
      "Epoch: [13/50]         || Step: [320/1579]      || Average Training Loss: 2.3800\n",
      "Epoch: [13/50]         || Step: [340/1579]      || Average Training Loss: 2.3784\n",
      "Epoch: [13/50]         || Step: [360/1579]      || Average Training Loss: 2.3793\n",
      "Epoch: [13/50]         || Step: [380/1579]      || Average Training Loss: 2.3789\n",
      "Epoch: [13/50]         || Step: [400/1579]      || Average Training Loss: 2.3782\n",
      "Epoch: [13/50]         || Step: [420/1579]      || Average Training Loss: 2.3797\n",
      "Epoch: [13/50]         || Step: [440/1579]      || Average Training Loss: 2.3806\n",
      "Epoch: [13/50]         || Step: [460/1579]      || Average Training Loss: 2.3807\n",
      "Epoch: [13/50]         || Step: [480/1579]      || Average Training Loss: 2.3803\n",
      "Epoch: [13/50]         || Step: [500/1579]      || Average Training Loss: 2.3801\n",
      "Epoch: [13/50]         || Step: [520/1579]      || Average Training Loss: 2.3817\n",
      "Epoch: [13/50]         || Step: [540/1579]      || Average Training Loss: 2.3825\n",
      "Epoch: [13/50]         || Step: [560/1579]      || Average Training Loss: 2.3826\n",
      "Epoch: [13/50]         || Step: [580/1579]      || Average Training Loss: 2.3817\n",
      "Epoch: [13/50]         || Step: [600/1579]      || Average Training Loss: 2.3817\n",
      "Epoch: [13/50]         || Step: [620/1579]      || Average Training Loss: 2.3827\n",
      "Epoch: [13/50]         || Step: [640/1579]      || Average Training Loss: 2.3827\n",
      "Epoch: [13/50]         || Step: [660/1579]      || Average Training Loss: 2.3814\n",
      "Epoch: [13/50]         || Step: [680/1579]      || Average Training Loss: 2.3805\n",
      "Epoch: [13/50]         || Step: [700/1579]      || Average Training Loss: 2.3823\n",
      "Epoch: [13/50]         || Step: [720/1579]      || Average Training Loss: 2.3822\n",
      "Epoch: [13/50]         || Step: [740/1579]      || Average Training Loss: 2.3826\n",
      "Epoch: [13/50]         || Step: [760/1579]      || Average Training Loss: 2.3824\n",
      "Epoch: [13/50]         || Step: [780/1579]      || Average Training Loss: 2.3826\n",
      "Epoch: [13/50]         || Step: [800/1579]      || Average Training Loss: 2.3830\n",
      "Epoch: [13/50]         || Step: [820/1579]      || Average Training Loss: 2.3827\n",
      "Epoch: [13/50]         || Step: [840/1579]      || Average Training Loss: 2.3829\n",
      "Epoch: [13/50]         || Step: [860/1579]      || Average Training Loss: 2.3823\n",
      "Epoch: [13/50]         || Step: [880/1579]      || Average Training Loss: 2.3814\n",
      "Epoch: [13/50]         || Step: [900/1579]      || Average Training Loss: 2.3801\n",
      "Epoch: [13/50]         || Step: [920/1579]      || Average Training Loss: 2.3792\n",
      "Epoch: [13/50]         || Step: [940/1579]      || Average Training Loss: 2.3787\n",
      "Epoch: [13/50]         || Step: [960/1579]      || Average Training Loss: 2.3781\n",
      "Epoch: [13/50]         || Step: [980/1579]      || Average Training Loss: 2.3788\n",
      "Epoch: [13/50]         || Step: [1000/1579]     || Average Training Loss: 2.3784\n",
      "Epoch: [13/50]         || Step: [1020/1579]     || Average Training Loss: 2.3789\n",
      "Epoch: [13/50]         || Step: [1040/1579]     || Average Training Loss: 2.3789\n",
      "Epoch: [13/50]         || Step: [1060/1579]     || Average Training Loss: 2.3792\n",
      "Epoch: [13/50]         || Step: [1080/1579]     || Average Training Loss: 2.3780\n",
      "Epoch: [13/50]         || Step: [1100/1579]     || Average Training Loss: 2.3779\n",
      "Epoch: [13/50]         || Step: [1120/1579]     || Average Training Loss: 2.3776\n",
      "Epoch: [13/50]         || Step: [1140/1579]     || Average Training Loss: 2.3776\n",
      "Epoch: [13/50]         || Step: [1160/1579]     || Average Training Loss: 2.3786\n",
      "Epoch: [13/50]         || Step: [1180/1579]     || Average Training Loss: 2.3780\n",
      "Epoch: [13/50]         || Step: [1200/1579]     || Average Training Loss: 2.3775\n",
      "Epoch: [13/50]         || Step: [1220/1579]     || Average Training Loss: 2.3775\n",
      "Epoch: [13/50]         || Step: [1240/1579]     || Average Training Loss: 2.3778\n",
      "Epoch: [13/50]         || Step: [1260/1579]     || Average Training Loss: 2.3777\n",
      "Epoch: [13/50]         || Step: [1280/1579]     || Average Training Loss: 2.3782\n",
      "Epoch: [13/50]         || Step: [1300/1579]     || Average Training Loss: 2.3787\n",
      "Epoch: [13/50]         || Step: [1320/1579]     || Average Training Loss: 2.3778\n",
      "Epoch: [13/50]         || Step: [1340/1579]     || Average Training Loss: 2.3783\n",
      "Epoch: [13/50]         || Step: [1360/1579]     || Average Training Loss: 2.3789\n",
      "Epoch: [13/50]         || Step: [1380/1579]     || Average Training Loss: 2.3786\n",
      "Epoch: [13/50]         || Step: [1400/1579]     || Average Training Loss: 2.3785\n",
      "Epoch: [13/50]         || Step: [1420/1579]     || Average Training Loss: 2.3794\n",
      "Epoch: [13/50]         || Step: [1440/1579]     || Average Training Loss: 2.3797\n",
      "Epoch: [13/50]         || Step: [1460/1579]     || Average Training Loss: 2.3800\n",
      "Epoch: [13/50]         || Step: [1480/1579]     || Average Training Loss: 2.3803\n",
      "Epoch: [13/50]         || Step: [1500/1579]     || Average Training Loss: 2.3805\n",
      "Epoch: [13/50]         || Step: [1520/1579]     || Average Training Loss: 2.3803\n",
      "Epoch: [13/50]         || Step: [1540/1579]     || Average Training Loss: 2.3796\n",
      "Epoch: [13/50]         || Step: [1560/1579]     || Average Training Loss: 2.3801\n",
      "Epoch: [13/50]         || Step: [0/142]         || Average Validation Loss: 2.4207\n",
      "Epoch: [13/50]         || Step: [20/142]        || Average Validation Loss: 2.3986\n",
      "Epoch: [13/50]         || Step: [40/142]        || Average Validation Loss: 2.3851\n",
      "Epoch: [13/50]         || Step: [60/142]        || Average Validation Loss: 2.3795\n",
      "Epoch: [13/50]         || Step: [80/142]        || Average Validation Loss: 2.3705\n",
      "Epoch: [13/50]         || Step: [100/142]       || Average Validation Loss: 2.3746\n",
      "Epoch: [13/50]         || Step: [120/142]       || Average Validation Loss: 2.3788\n",
      "Epoch: [13/50]         || Step: [140/142]       || Average Validation Loss: 2.3807\n",
      "****************************************************************************************************\n",
      "Epoch: [13/50] || Training Loss = 2.38 || Validation Loss: 2.38 || Time: 21.959776\n",
      "****************************************************************************************************\n",
      "Epoch: [14/50]         || Step: [0/1579]        || Average Training Loss: 2.2840\n",
      "Epoch: [14/50]         || Step: [20/1579]       || Average Training Loss: 2.4312\n",
      "Epoch: [14/50]         || Step: [40/1579]       || Average Training Loss: 2.3935\n",
      "Epoch: [14/50]         || Step: [60/1579]       || Average Training Loss: 2.4014\n",
      "Epoch: [14/50]         || Step: [80/1579]       || Average Training Loss: 2.3956\n",
      "Epoch: [14/50]         || Step: [100/1579]      || Average Training Loss: 2.3896\n",
      "Epoch: [14/50]         || Step: [120/1579]      || Average Training Loss: 2.3864\n",
      "Epoch: [14/50]         || Step: [140/1579]      || Average Training Loss: 2.3798\n",
      "Epoch: [14/50]         || Step: [160/1579]      || Average Training Loss: 2.3839\n",
      "Epoch: [14/50]         || Step: [180/1579]      || Average Training Loss: 2.3815\n",
      "Epoch: [14/50]         || Step: [200/1579]      || Average Training Loss: 2.3774\n",
      "Epoch: [14/50]         || Step: [220/1579]      || Average Training Loss: 2.3767\n",
      "Epoch: [14/50]         || Step: [240/1579]      || Average Training Loss: 2.3807\n",
      "Epoch: [14/50]         || Step: [260/1579]      || Average Training Loss: 2.3761\n",
      "Epoch: [14/50]         || Step: [280/1579]      || Average Training Loss: 2.3764\n",
      "Epoch: [14/50]         || Step: [300/1579]      || Average Training Loss: 2.3766\n",
      "Epoch: [14/50]         || Step: [320/1579]      || Average Training Loss: 2.3771\n",
      "Epoch: [14/50]         || Step: [340/1579]      || Average Training Loss: 2.3750\n",
      "Epoch: [14/50]         || Step: [360/1579]      || Average Training Loss: 2.3754\n",
      "Epoch: [14/50]         || Step: [380/1579]      || Average Training Loss: 2.3709\n",
      "Epoch: [14/50]         || Step: [400/1579]      || Average Training Loss: 2.3708\n",
      "Epoch: [14/50]         || Step: [420/1579]      || Average Training Loss: 2.3695\n",
      "Epoch: [14/50]         || Step: [440/1579]      || Average Training Loss: 2.3685\n",
      "Epoch: [14/50]         || Step: [460/1579]      || Average Training Loss: 2.3684\n",
      "Epoch: [14/50]         || Step: [480/1579]      || Average Training Loss: 2.3696\n",
      "Epoch: [14/50]         || Step: [500/1579]      || Average Training Loss: 2.3694\n",
      "Epoch: [14/50]         || Step: [520/1579]      || Average Training Loss: 2.3691\n",
      "Epoch: [14/50]         || Step: [540/1579]      || Average Training Loss: 2.3692\n",
      "Epoch: [14/50]         || Step: [560/1579]      || Average Training Loss: 2.3699\n",
      "Epoch: [14/50]         || Step: [580/1579]      || Average Training Loss: 2.3695\n",
      "Epoch: [14/50]         || Step: [600/1579]      || Average Training Loss: 2.3692\n",
      "Epoch: [14/50]         || Step: [620/1579]      || Average Training Loss: 2.3705\n",
      "Epoch: [14/50]         || Step: [640/1579]      || Average Training Loss: 2.3703\n",
      "Epoch: [14/50]         || Step: [660/1579]      || Average Training Loss: 2.3706\n",
      "Epoch: [14/50]         || Step: [680/1579]      || Average Training Loss: 2.3713\n",
      "Epoch: [14/50]         || Step: [700/1579]      || Average Training Loss: 2.3715\n",
      "Epoch: [14/50]         || Step: [720/1579]      || Average Training Loss: 2.3719\n",
      "Epoch: [14/50]         || Step: [740/1579]      || Average Training Loss: 2.3716\n",
      "Epoch: [14/50]         || Step: [760/1579]      || Average Training Loss: 2.3714\n",
      "Epoch: [14/50]         || Step: [780/1579]      || Average Training Loss: 2.3724\n",
      "Epoch: [14/50]         || Step: [800/1579]      || Average Training Loss: 2.3732\n",
      "Epoch: [14/50]         || Step: [820/1579]      || Average Training Loss: 2.3729\n",
      "Epoch: [14/50]         || Step: [840/1579]      || Average Training Loss: 2.3727\n",
      "Epoch: [14/50]         || Step: [860/1579]      || Average Training Loss: 2.3731\n",
      "Epoch: [14/50]         || Step: [880/1579]      || Average Training Loss: 2.3733\n",
      "Epoch: [14/50]         || Step: [900/1579]      || Average Training Loss: 2.3746\n",
      "Epoch: [14/50]         || Step: [920/1579]      || Average Training Loss: 2.3743\n",
      "Epoch: [14/50]         || Step: [940/1579]      || Average Training Loss: 2.3745\n",
      "Epoch: [14/50]         || Step: [960/1579]      || Average Training Loss: 2.3747\n",
      "Epoch: [14/50]         || Step: [980/1579]      || Average Training Loss: 2.3747\n",
      "Epoch: [14/50]         || Step: [1000/1579]     || Average Training Loss: 2.3747\n",
      "Epoch: [14/50]         || Step: [1020/1579]     || Average Training Loss: 2.3748\n",
      "Epoch: [14/50]         || Step: [1040/1579]     || Average Training Loss: 2.3751\n",
      "Epoch: [14/50]         || Step: [1060/1579]     || Average Training Loss: 2.3756\n",
      "Epoch: [14/50]         || Step: [1080/1579]     || Average Training Loss: 2.3756\n",
      "Epoch: [14/50]         || Step: [1100/1579]     || Average Training Loss: 2.3760\n",
      "Epoch: [14/50]         || Step: [1120/1579]     || Average Training Loss: 2.3764\n",
      "Epoch: [14/50]         || Step: [1140/1579]     || Average Training Loss: 2.3771\n",
      "Epoch: [14/50]         || Step: [1160/1579]     || Average Training Loss: 2.3770\n",
      "Epoch: [14/50]         || Step: [1180/1579]     || Average Training Loss: 2.3772\n",
      "Epoch: [14/50]         || Step: [1200/1579]     || Average Training Loss: 2.3772\n",
      "Epoch: [14/50]         || Step: [1220/1579]     || Average Training Loss: 2.3775\n",
      "Epoch: [14/50]         || Step: [1240/1579]     || Average Training Loss: 2.3771\n",
      "Epoch: [14/50]         || Step: [1260/1579]     || Average Training Loss: 2.3766\n",
      "Epoch: [14/50]         || Step: [1280/1579]     || Average Training Loss: 2.3764\n",
      "Epoch: [14/50]         || Step: [1300/1579]     || Average Training Loss: 2.3763\n",
      "Epoch: [14/50]         || Step: [1320/1579]     || Average Training Loss: 2.3759\n",
      "Epoch: [14/50]         || Step: [1340/1579]     || Average Training Loss: 2.3769\n",
      "Epoch: [14/50]         || Step: [1360/1579]     || Average Training Loss: 2.3766\n",
      "Epoch: [14/50]         || Step: [1380/1579]     || Average Training Loss: 2.3772\n",
      "Epoch: [14/50]         || Step: [1400/1579]     || Average Training Loss: 2.3775\n",
      "Epoch: [14/50]         || Step: [1420/1579]     || Average Training Loss: 2.3779\n",
      "Epoch: [14/50]         || Step: [1440/1579]     || Average Training Loss: 2.3787\n",
      "Epoch: [14/50]         || Step: [1460/1579]     || Average Training Loss: 2.3785\n",
      "Epoch: [14/50]         || Step: [1480/1579]     || Average Training Loss: 2.3783\n",
      "Epoch: [14/50]         || Step: [1500/1579]     || Average Training Loss: 2.3785\n",
      "Epoch: [14/50]         || Step: [1520/1579]     || Average Training Loss: 2.3787\n",
      "Epoch: [14/50]         || Step: [1540/1579]     || Average Training Loss: 2.3791\n",
      "Epoch: [14/50]         || Step: [1560/1579]     || Average Training Loss: 2.3786\n",
      "Epoch: [14/50]         || Step: [0/142]         || Average Validation Loss: 2.4828\n",
      "Epoch: [14/50]         || Step: [20/142]        || Average Validation Loss: 2.4139\n",
      "Epoch: [14/50]         || Step: [40/142]        || Average Validation Loss: 2.4220\n",
      "Epoch: [14/50]         || Step: [60/142]        || Average Validation Loss: 2.4280\n",
      "Epoch: [14/50]         || Step: [80/142]        || Average Validation Loss: 2.4365\n",
      "Epoch: [14/50]         || Step: [100/142]       || Average Validation Loss: 2.4290\n",
      "Epoch: [14/50]         || Step: [120/142]       || Average Validation Loss: 2.4313\n",
      "Epoch: [14/50]         || Step: [140/142]       || Average Validation Loss: 2.4334\n",
      "****************************************************************************************************\n",
      "Epoch: [14/50] || Training Loss = 2.38 || Validation Loss: 2.44 || Time: 21.310775\n",
      "****************************************************************************************************\n",
      "Epoch: [15/50]         || Step: [0/1579]        || Average Training Loss: 2.3995\n",
      "Epoch: [15/50]         || Step: [20/1579]       || Average Training Loss: 2.4428\n",
      "Epoch: [15/50]         || Step: [40/1579]       || Average Training Loss: 2.4007\n",
      "Epoch: [15/50]         || Step: [60/1579]       || Average Training Loss: 2.4021\n",
      "Epoch: [15/50]         || Step: [80/1579]       || Average Training Loss: 2.3899\n",
      "Epoch: [15/50]         || Step: [100/1579]      || Average Training Loss: 2.3854\n",
      "Epoch: [15/50]         || Step: [120/1579]      || Average Training Loss: 2.3915\n",
      "Epoch: [15/50]         || Step: [140/1579]      || Average Training Loss: 2.3842\n",
      "Epoch: [15/50]         || Step: [160/1579]      || Average Training Loss: 2.3857\n",
      "Epoch: [15/50]         || Step: [180/1579]      || Average Training Loss: 2.3818\n",
      "Epoch: [15/50]         || Step: [200/1579]      || Average Training Loss: 2.3840\n",
      "Epoch: [15/50]         || Step: [220/1579]      || Average Training Loss: 2.3872\n",
      "Epoch: [15/50]         || Step: [240/1579]      || Average Training Loss: 2.3841\n",
      "Epoch: [15/50]         || Step: [260/1579]      || Average Training Loss: 2.3816\n",
      "Epoch: [15/50]         || Step: [280/1579]      || Average Training Loss: 2.3807\n",
      "Epoch: [15/50]         || Step: [300/1579]      || Average Training Loss: 2.3797\n",
      "Epoch: [15/50]         || Step: [320/1579]      || Average Training Loss: 2.3738\n",
      "Epoch: [15/50]         || Step: [340/1579]      || Average Training Loss: 2.3744\n",
      "Epoch: [15/50]         || Step: [360/1579]      || Average Training Loss: 2.3719\n",
      "Epoch: [15/50]         || Step: [380/1579]      || Average Training Loss: 2.3742\n",
      "Epoch: [15/50]         || Step: [400/1579]      || Average Training Loss: 2.3724\n",
      "Epoch: [15/50]         || Step: [420/1579]      || Average Training Loss: 2.3721\n",
      "Epoch: [15/50]         || Step: [440/1579]      || Average Training Loss: 2.3753\n",
      "Epoch: [15/50]         || Step: [460/1579]      || Average Training Loss: 2.3744\n",
      "Epoch: [15/50]         || Step: [480/1579]      || Average Training Loss: 2.3750\n",
      "Epoch: [15/50]         || Step: [500/1579]      || Average Training Loss: 2.3741\n",
      "Epoch: [15/50]         || Step: [520/1579]      || Average Training Loss: 2.3742\n",
      "Epoch: [15/50]         || Step: [540/1579]      || Average Training Loss: 2.3738\n",
      "Epoch: [15/50]         || Step: [560/1579]      || Average Training Loss: 2.3744\n",
      "Epoch: [15/50]         || Step: [580/1579]      || Average Training Loss: 2.3727\n",
      "Epoch: [15/50]         || Step: [600/1579]      || Average Training Loss: 2.3731\n",
      "Epoch: [15/50]         || Step: [620/1579]      || Average Training Loss: 2.3718\n",
      "Epoch: [15/50]         || Step: [640/1579]      || Average Training Loss: 2.3709\n",
      "Epoch: [15/50]         || Step: [660/1579]      || Average Training Loss: 2.3719\n",
      "Epoch: [15/50]         || Step: [680/1579]      || Average Training Loss: 2.3712\n",
      "Epoch: [15/50]         || Step: [700/1579]      || Average Training Loss: 2.3716\n",
      "Epoch: [15/50]         || Step: [720/1579]      || Average Training Loss: 2.3726\n",
      "Epoch: [15/50]         || Step: [740/1579]      || Average Training Loss: 2.3723\n",
      "Epoch: [15/50]         || Step: [760/1579]      || Average Training Loss: 2.3725\n",
      "Epoch: [15/50]         || Step: [780/1579]      || Average Training Loss: 2.3728\n",
      "Epoch: [15/50]         || Step: [800/1579]      || Average Training Loss: 2.3732\n",
      "Epoch: [15/50]         || Step: [820/1579]      || Average Training Loss: 2.3735\n",
      "Epoch: [15/50]         || Step: [840/1579]      || Average Training Loss: 2.3745\n",
      "Epoch: [15/50]         || Step: [860/1579]      || Average Training Loss: 2.3741\n",
      "Epoch: [15/50]         || Step: [880/1579]      || Average Training Loss: 2.3740\n",
      "Epoch: [15/50]         || Step: [900/1579]      || Average Training Loss: 2.3741\n",
      "Epoch: [15/50]         || Step: [920/1579]      || Average Training Loss: 2.3743\n",
      "Epoch: [15/50]         || Step: [940/1579]      || Average Training Loss: 2.3742\n",
      "Epoch: [15/50]         || Step: [960/1579]      || Average Training Loss: 2.3742\n",
      "Epoch: [15/50]         || Step: [980/1579]      || Average Training Loss: 2.3737\n",
      "Epoch: [15/50]         || Step: [1000/1579]     || Average Training Loss: 2.3739\n",
      "Epoch: [15/50]         || Step: [1020/1579]     || Average Training Loss: 2.3735\n",
      "Epoch: [15/50]         || Step: [1040/1579]     || Average Training Loss: 2.3744\n",
      "Epoch: [15/50]         || Step: [1060/1579]     || Average Training Loss: 2.3739\n",
      "Epoch: [15/50]         || Step: [1080/1579]     || Average Training Loss: 2.3736\n",
      "Epoch: [15/50]         || Step: [1100/1579]     || Average Training Loss: 2.3729\n",
      "Epoch: [15/50]         || Step: [1120/1579]     || Average Training Loss: 2.3732\n",
      "Epoch: [15/50]         || Step: [1140/1579]     || Average Training Loss: 2.3733\n",
      "Epoch: [15/50]         || Step: [1160/1579]     || Average Training Loss: 2.3747\n",
      "Epoch: [15/50]         || Step: [1180/1579]     || Average Training Loss: 2.3748\n",
      "Epoch: [15/50]         || Step: [1200/1579]     || Average Training Loss: 2.3753\n",
      "Epoch: [15/50]         || Step: [1220/1579]     || Average Training Loss: 2.3755\n",
      "Epoch: [15/50]         || Step: [1240/1579]     || Average Training Loss: 2.3750\n",
      "Epoch: [15/50]         || Step: [1260/1579]     || Average Training Loss: 2.3755\n",
      "Epoch: [15/50]         || Step: [1280/1579]     || Average Training Loss: 2.3754\n",
      "Epoch: [15/50]         || Step: [1300/1579]     || Average Training Loss: 2.3757\n",
      "Epoch: [15/50]         || Step: [1320/1579]     || Average Training Loss: 2.3756\n",
      "Epoch: [15/50]         || Step: [1340/1579]     || Average Training Loss: 2.3756\n",
      "Epoch: [15/50]         || Step: [1360/1579]     || Average Training Loss: 2.3763\n",
      "Epoch: [15/50]         || Step: [1380/1579]     || Average Training Loss: 2.3768\n",
      "Epoch: [15/50]         || Step: [1400/1579]     || Average Training Loss: 2.3764\n",
      "Epoch: [15/50]         || Step: [1420/1579]     || Average Training Loss: 2.3770\n",
      "Epoch: [15/50]         || Step: [1440/1579]     || Average Training Loss: 2.3763\n",
      "Epoch: [15/50]         || Step: [1460/1579]     || Average Training Loss: 2.3765\n",
      "Epoch: [15/50]         || Step: [1480/1579]     || Average Training Loss: 2.3765\n",
      "Epoch: [15/50]         || Step: [1500/1579]     || Average Training Loss: 2.3764\n",
      "Epoch: [15/50]         || Step: [1520/1579]     || Average Training Loss: 2.3769\n",
      "Epoch: [15/50]         || Step: [1540/1579]     || Average Training Loss: 2.3769\n",
      "Epoch: [15/50]         || Step: [1560/1579]     || Average Training Loss: 2.3773\n",
      "Epoch: [15/50]         || Step: [0/142]         || Average Validation Loss: 2.3489\n",
      "Epoch: [15/50]         || Step: [20/142]        || Average Validation Loss: 2.3385\n",
      "Epoch: [15/50]         || Step: [40/142]        || Average Validation Loss: 2.3478\n",
      "Epoch: [15/50]         || Step: [60/142]        || Average Validation Loss: 2.3573\n",
      "Epoch: [15/50]         || Step: [80/142]        || Average Validation Loss: 2.3682\n",
      "Epoch: [15/50]         || Step: [100/142]       || Average Validation Loss: 2.3718\n",
      "Epoch: [15/50]         || Step: [120/142]       || Average Validation Loss: 2.3693\n",
      "Epoch: [15/50]         || Step: [140/142]       || Average Validation Loss: 2.3683\n",
      "****************************************************************************************************\n",
      "Epoch: [15/50] || Training Loss = 2.38 || Validation Loss: 2.37 || Time: 22.409010\n",
      "****************************************************************************************************\n",
      "Epoch: [16/50]         || Step: [0/1579]        || Average Training Loss: 2.4663\n",
      "Epoch: [16/50]         || Step: [20/1579]       || Average Training Loss: 2.3502\n",
      "Epoch: [16/50]         || Step: [40/1579]       || Average Training Loss: 2.3368\n",
      "Epoch: [16/50]         || Step: [60/1579]       || Average Training Loss: 2.3662\n",
      "Epoch: [16/50]         || Step: [80/1579]       || Average Training Loss: 2.3674\n",
      "Epoch: [16/50]         || Step: [100/1579]      || Average Training Loss: 2.3749\n",
      "Epoch: [16/50]         || Step: [120/1579]      || Average Training Loss: 2.3806\n",
      "Epoch: [16/50]         || Step: [140/1579]      || Average Training Loss: 2.3781\n",
      "Epoch: [16/50]         || Step: [160/1579]      || Average Training Loss: 2.3759\n",
      "Epoch: [16/50]         || Step: [180/1579]      || Average Training Loss: 2.3778\n",
      "Epoch: [16/50]         || Step: [200/1579]      || Average Training Loss: 2.3786\n",
      "Epoch: [16/50]         || Step: [220/1579]      || Average Training Loss: 2.3755\n",
      "Epoch: [16/50]         || Step: [240/1579]      || Average Training Loss: 2.3748\n",
      "Epoch: [16/50]         || Step: [260/1579]      || Average Training Loss: 2.3814\n",
      "Epoch: [16/50]         || Step: [280/1579]      || Average Training Loss: 2.3826\n",
      "Epoch: [16/50]         || Step: [300/1579]      || Average Training Loss: 2.3814\n",
      "Epoch: [16/50]         || Step: [320/1579]      || Average Training Loss: 2.3807\n",
      "Epoch: [16/50]         || Step: [340/1579]      || Average Training Loss: 2.3799\n",
      "Epoch: [16/50]         || Step: [360/1579]      || Average Training Loss: 2.3797\n",
      "Epoch: [16/50]         || Step: [380/1579]      || Average Training Loss: 2.3810\n",
      "Epoch: [16/50]         || Step: [400/1579]      || Average Training Loss: 2.3800\n",
      "Epoch: [16/50]         || Step: [420/1579]      || Average Training Loss: 2.3804\n",
      "Epoch: [16/50]         || Step: [440/1579]      || Average Training Loss: 2.3825\n",
      "Epoch: [16/50]         || Step: [460/1579]      || Average Training Loss: 2.3820\n",
      "Epoch: [16/50]         || Step: [480/1579]      || Average Training Loss: 2.3834\n",
      "Epoch: [16/50]         || Step: [500/1579]      || Average Training Loss: 2.3828\n",
      "Epoch: [16/50]         || Step: [520/1579]      || Average Training Loss: 2.3821\n",
      "Epoch: [16/50]         || Step: [540/1579]      || Average Training Loss: 2.3803\n",
      "Epoch: [16/50]         || Step: [560/1579]      || Average Training Loss: 2.3795\n",
      "Epoch: [16/50]         || Step: [580/1579]      || Average Training Loss: 2.3803\n",
      "Epoch: [16/50]         || Step: [600/1579]      || Average Training Loss: 2.3801\n",
      "Epoch: [16/50]         || Step: [620/1579]      || Average Training Loss: 2.3789\n",
      "Epoch: [16/50]         || Step: [640/1579]      || Average Training Loss: 2.3802\n",
      "Epoch: [16/50]         || Step: [660/1579]      || Average Training Loss: 2.3795\n",
      "Epoch: [16/50]         || Step: [680/1579]      || Average Training Loss: 2.3800\n",
      "Epoch: [16/50]         || Step: [700/1579]      || Average Training Loss: 2.3793\n",
      "Epoch: [16/50]         || Step: [720/1579]      || Average Training Loss: 2.3779\n",
      "Epoch: [16/50]         || Step: [740/1579]      || Average Training Loss: 2.3783\n",
      "Epoch: [16/50]         || Step: [760/1579]      || Average Training Loss: 2.3770\n",
      "Epoch: [16/50]         || Step: [780/1579]      || Average Training Loss: 2.3769\n",
      "Epoch: [16/50]         || Step: [800/1579]      || Average Training Loss: 2.3769\n",
      "Epoch: [16/50]         || Step: [820/1579]      || Average Training Loss: 2.3758\n",
      "Epoch: [16/50]         || Step: [840/1579]      || Average Training Loss: 2.3749\n",
      "Epoch: [16/50]         || Step: [860/1579]      || Average Training Loss: 2.3739\n",
      "Epoch: [16/50]         || Step: [880/1579]      || Average Training Loss: 2.3745\n",
      "Epoch: [16/50]         || Step: [900/1579]      || Average Training Loss: 2.3743\n",
      "Epoch: [16/50]         || Step: [920/1579]      || Average Training Loss: 2.3752\n",
      "Epoch: [16/50]         || Step: [940/1579]      || Average Training Loss: 2.3761\n",
      "Epoch: [16/50]         || Step: [960/1579]      || Average Training Loss: 2.3763\n",
      "Epoch: [16/50]         || Step: [980/1579]      || Average Training Loss: 2.3764\n",
      "Epoch: [16/50]         || Step: [1000/1579]     || Average Training Loss: 2.3769\n",
      "Epoch: [16/50]         || Step: [1020/1579]     || Average Training Loss: 2.3762\n",
      "Epoch: [16/50]         || Step: [1040/1579]     || Average Training Loss: 2.3761\n",
      "Epoch: [16/50]         || Step: [1060/1579]     || Average Training Loss: 2.3753\n",
      "Epoch: [16/50]         || Step: [1080/1579]     || Average Training Loss: 2.3755\n",
      "Epoch: [16/50]         || Step: [1100/1579]     || Average Training Loss: 2.3749\n",
      "Epoch: [16/50]         || Step: [1120/1579]     || Average Training Loss: 2.3739\n",
      "Epoch: [16/50]         || Step: [1140/1579]     || Average Training Loss: 2.3748\n",
      "Epoch: [16/50]         || Step: [1160/1579]     || Average Training Loss: 2.3752\n",
      "Epoch: [16/50]         || Step: [1180/1579]     || Average Training Loss: 2.3757\n",
      "Epoch: [16/50]         || Step: [1200/1579]     || Average Training Loss: 2.3761\n",
      "Epoch: [16/50]         || Step: [1220/1579]     || Average Training Loss: 2.3761\n",
      "Epoch: [16/50]         || Step: [1240/1579]     || Average Training Loss: 2.3758\n",
      "Epoch: [16/50]         || Step: [1260/1579]     || Average Training Loss: 2.3760\n",
      "Epoch: [16/50]         || Step: [1280/1579]     || Average Training Loss: 2.3756\n",
      "Epoch: [16/50]         || Step: [1300/1579]     || Average Training Loss: 2.3756\n",
      "Epoch: [16/50]         || Step: [1320/1579]     || Average Training Loss: 2.3758\n",
      "Epoch: [16/50]         || Step: [1340/1579]     || Average Training Loss: 2.3762\n",
      "Epoch: [16/50]         || Step: [1360/1579]     || Average Training Loss: 2.3758\n",
      "Epoch: [16/50]         || Step: [1380/1579]     || Average Training Loss: 2.3755\n",
      "Epoch: [16/50]         || Step: [1400/1579]     || Average Training Loss: 2.3760\n",
      "Epoch: [16/50]         || Step: [1420/1579]     || Average Training Loss: 2.3756\n",
      "Epoch: [16/50]         || Step: [1440/1579]     || Average Training Loss: 2.3754\n",
      "Epoch: [16/50]         || Step: [1460/1579]     || Average Training Loss: 2.3754\n",
      "Epoch: [16/50]         || Step: [1480/1579]     || Average Training Loss: 2.3760\n",
      "Epoch: [16/50]         || Step: [1500/1579]     || Average Training Loss: 2.3759\n",
      "Epoch: [16/50]         || Step: [1520/1579]     || Average Training Loss: 2.3759\n",
      "Epoch: [16/50]         || Step: [1540/1579]     || Average Training Loss: 2.3763\n",
      "Epoch: [16/50]         || Step: [1560/1579]     || Average Training Loss: 2.3760\n",
      "Epoch: [16/50]         || Step: [0/142]         || Average Validation Loss: 2.6350\n",
      "Epoch: [16/50]         || Step: [20/142]        || Average Validation Loss: 2.5370\n",
      "Epoch: [16/50]         || Step: [40/142]        || Average Validation Loss: 2.5100\n",
      "Epoch: [16/50]         || Step: [60/142]        || Average Validation Loss: 2.5055\n",
      "Epoch: [16/50]         || Step: [80/142]        || Average Validation Loss: 2.5081\n",
      "Epoch: [16/50]         || Step: [100/142]       || Average Validation Loss: 2.5018\n",
      "Epoch: [16/50]         || Step: [120/142]       || Average Validation Loss: 2.5049\n",
      "Epoch: [16/50]         || Step: [140/142]       || Average Validation Loss: 2.4987\n",
      "****************************************************************************************************\n",
      "Epoch: [16/50] || Training Loss = 2.38 || Validation Loss: 2.50 || Time: 22.242280\n",
      "****************************************************************************************************\n",
      "Epoch: [17/50]         || Step: [0/1579]        || Average Training Loss: 2.3892\n",
      "Epoch: [17/50]         || Step: [20/1579]       || Average Training Loss: 2.4294\n",
      "Epoch: [17/50]         || Step: [40/1579]       || Average Training Loss: 2.4166\n",
      "Epoch: [17/50]         || Step: [60/1579]       || Average Training Loss: 2.4077\n",
      "Epoch: [17/50]         || Step: [80/1579]       || Average Training Loss: 2.4082\n",
      "Epoch: [17/50]         || Step: [100/1579]      || Average Training Loss: 2.3989\n",
      "Epoch: [17/50]         || Step: [120/1579]      || Average Training Loss: 2.3888\n",
      "Epoch: [17/50]         || Step: [140/1579]      || Average Training Loss: 2.3867\n",
      "Epoch: [17/50]         || Step: [160/1579]      || Average Training Loss: 2.3875\n",
      "Epoch: [17/50]         || Step: [180/1579]      || Average Training Loss: 2.3808\n",
      "Epoch: [17/50]         || Step: [200/1579]      || Average Training Loss: 2.3794\n",
      "Epoch: [17/50]         || Step: [220/1579]      || Average Training Loss: 2.3784\n",
      "Epoch: [17/50]         || Step: [240/1579]      || Average Training Loss: 2.3815\n",
      "Epoch: [17/50]         || Step: [260/1579]      || Average Training Loss: 2.3805\n",
      "Epoch: [17/50]         || Step: [280/1579]      || Average Training Loss: 2.3798\n",
      "Epoch: [17/50]         || Step: [300/1579]      || Average Training Loss: 2.3776\n",
      "Epoch: [17/50]         || Step: [320/1579]      || Average Training Loss: 2.3755\n",
      "Epoch: [17/50]         || Step: [340/1579]      || Average Training Loss: 2.3767\n",
      "Epoch: [17/50]         || Step: [360/1579]      || Average Training Loss: 2.3782\n",
      "Epoch: [17/50]         || Step: [380/1579]      || Average Training Loss: 2.3809\n",
      "Epoch: [17/50]         || Step: [400/1579]      || Average Training Loss: 2.3818\n",
      "Epoch: [17/50]         || Step: [420/1579]      || Average Training Loss: 2.3830\n",
      "Epoch: [17/50]         || Step: [440/1579]      || Average Training Loss: 2.3825\n",
      "Epoch: [17/50]         || Step: [460/1579]      || Average Training Loss: 2.3810\n",
      "Epoch: [17/50]         || Step: [480/1579]      || Average Training Loss: 2.3804\n",
      "Epoch: [17/50]         || Step: [500/1579]      || Average Training Loss: 2.3811\n",
      "Epoch: [17/50]         || Step: [520/1579]      || Average Training Loss: 2.3791\n",
      "Epoch: [17/50]         || Step: [540/1579]      || Average Training Loss: 2.3780\n",
      "Epoch: [17/50]         || Step: [560/1579]      || Average Training Loss: 2.3800\n",
      "Epoch: [17/50]         || Step: [580/1579]      || Average Training Loss: 2.3796\n",
      "Epoch: [17/50]         || Step: [600/1579]      || Average Training Loss: 2.3787\n",
      "Epoch: [17/50]         || Step: [620/1579]      || Average Training Loss: 2.3782\n",
      "Epoch: [17/50]         || Step: [640/1579]      || Average Training Loss: 2.3796\n",
      "Epoch: [17/50]         || Step: [660/1579]      || Average Training Loss: 2.3801\n",
      "Epoch: [17/50]         || Step: [680/1579]      || Average Training Loss: 2.3800\n",
      "Epoch: [17/50]         || Step: [700/1579]      || Average Training Loss: 2.3807\n",
      "Epoch: [17/50]         || Step: [720/1579]      || Average Training Loss: 2.3797\n",
      "Epoch: [17/50]         || Step: [740/1579]      || Average Training Loss: 2.3790\n",
      "Epoch: [17/50]         || Step: [760/1579]      || Average Training Loss: 2.3791\n",
      "Epoch: [17/50]         || Step: [780/1579]      || Average Training Loss: 2.3783\n",
      "Epoch: [17/50]         || Step: [800/1579]      || Average Training Loss: 2.3778\n",
      "Epoch: [17/50]         || Step: [820/1579]      || Average Training Loss: 2.3780\n",
      "Epoch: [17/50]         || Step: [840/1579]      || Average Training Loss: 2.3778\n",
      "Epoch: [17/50]         || Step: [860/1579]      || Average Training Loss: 2.3775\n",
      "Epoch: [17/50]         || Step: [880/1579]      || Average Training Loss: 2.3762\n",
      "Epoch: [17/50]         || Step: [900/1579]      || Average Training Loss: 2.3757\n",
      "Epoch: [17/50]         || Step: [920/1579]      || Average Training Loss: 2.3757\n",
      "Epoch: [17/50]         || Step: [940/1579]      || Average Training Loss: 2.3773\n",
      "Epoch: [17/50]         || Step: [960/1579]      || Average Training Loss: 2.3774\n",
      "Epoch: [17/50]         || Step: [980/1579]      || Average Training Loss: 2.3781\n",
      "Epoch: [17/50]         || Step: [1000/1579]     || Average Training Loss: 2.3780\n",
      "Epoch: [17/50]         || Step: [1020/1579]     || Average Training Loss: 2.3786\n",
      "Epoch: [17/50]         || Step: [1040/1579]     || Average Training Loss: 2.3785\n",
      "Epoch: [17/50]         || Step: [1060/1579]     || Average Training Loss: 2.3787\n",
      "Epoch: [17/50]         || Step: [1080/1579]     || Average Training Loss: 2.3783\n",
      "Epoch: [17/50]         || Step: [1100/1579]     || Average Training Loss: 2.3783\n",
      "Epoch: [17/50]         || Step: [1120/1579]     || Average Training Loss: 2.3781\n",
      "Epoch: [17/50]         || Step: [1140/1579]     || Average Training Loss: 2.3782\n",
      "Epoch: [17/50]         || Step: [1160/1579]     || Average Training Loss: 2.3776\n",
      "Epoch: [17/50]         || Step: [1180/1579]     || Average Training Loss: 2.3776\n",
      "Epoch: [17/50]         || Step: [1200/1579]     || Average Training Loss: 2.3778\n",
      "Epoch: [17/50]         || Step: [1220/1579]     || Average Training Loss: 2.3778\n",
      "Epoch: [17/50]         || Step: [1240/1579]     || Average Training Loss: 2.3775\n",
      "Epoch: [17/50]         || Step: [1260/1579]     || Average Training Loss: 2.3780\n",
      "Epoch: [17/50]         || Step: [1280/1579]     || Average Training Loss: 2.3772\n",
      "Epoch: [17/50]         || Step: [1300/1579]     || Average Training Loss: 2.3776\n",
      "Epoch: [17/50]         || Step: [1320/1579]     || Average Training Loss: 2.3775\n",
      "Epoch: [17/50]         || Step: [1340/1579]     || Average Training Loss: 2.3776\n",
      "Epoch: [17/50]         || Step: [1360/1579]     || Average Training Loss: 2.3769\n",
      "Epoch: [17/50]         || Step: [1380/1579]     || Average Training Loss: 2.3766\n",
      "Epoch: [17/50]         || Step: [1400/1579]     || Average Training Loss: 2.3769\n",
      "Epoch: [17/50]         || Step: [1420/1579]     || Average Training Loss: 2.3764\n",
      "Epoch: [17/50]         || Step: [1440/1579]     || Average Training Loss: 2.3760\n",
      "Epoch: [17/50]         || Step: [1460/1579]     || Average Training Loss: 2.3757\n",
      "Epoch: [17/50]         || Step: [1480/1579]     || Average Training Loss: 2.3764\n",
      "Epoch: [17/50]         || Step: [1500/1579]     || Average Training Loss: 2.3764\n",
      "Epoch: [17/50]         || Step: [1520/1579]     || Average Training Loss: 2.3757\n",
      "Epoch: [17/50]         || Step: [1540/1579]     || Average Training Loss: 2.3749\n",
      "Epoch: [17/50]         || Step: [1560/1579]     || Average Training Loss: 2.3753\n",
      "Epoch: [17/50]         || Step: [0/142]         || Average Validation Loss: 2.1104\n",
      "Epoch: [17/50]         || Step: [20/142]        || Average Validation Loss: 2.3992\n",
      "Epoch: [17/50]         || Step: [40/142]        || Average Validation Loss: 2.4219\n",
      "Epoch: [17/50]         || Step: [60/142]        || Average Validation Loss: 2.4155\n",
      "Epoch: [17/50]         || Step: [80/142]        || Average Validation Loss: 2.4086\n",
      "Epoch: [17/50]         || Step: [100/142]       || Average Validation Loss: 2.4031\n",
      "Epoch: [17/50]         || Step: [120/142]       || Average Validation Loss: 2.3974\n",
      "Epoch: [17/50]         || Step: [140/142]       || Average Validation Loss: 2.3951\n",
      "****************************************************************************************************\n",
      "Epoch: [17/50] || Training Loss = 2.38 || Validation Loss: 2.40 || Time: 21.520947\n",
      "****************************************************************************************************\n",
      "Epoch: [18/50]         || Step: [0/1579]        || Average Training Loss: 2.3451\n",
      "Epoch: [18/50]         || Step: [20/1579]       || Average Training Loss: 2.3606\n",
      "Epoch: [18/50]         || Step: [40/1579]       || Average Training Loss: 2.3572\n",
      "Epoch: [18/50]         || Step: [60/1579]       || Average Training Loss: 2.3429\n",
      "Epoch: [18/50]         || Step: [80/1579]       || Average Training Loss: 2.3480\n",
      "Epoch: [18/50]         || Step: [100/1579]      || Average Training Loss: 2.3466\n",
      "Epoch: [18/50]         || Step: [120/1579]      || Average Training Loss: 2.3606\n",
      "Epoch: [18/50]         || Step: [140/1579]      || Average Training Loss: 2.3656\n",
      "Epoch: [18/50]         || Step: [160/1579]      || Average Training Loss: 2.3627\n",
      "Epoch: [18/50]         || Step: [180/1579]      || Average Training Loss: 2.3624\n",
      "Epoch: [18/50]         || Step: [200/1579]      || Average Training Loss: 2.3599\n",
      "Epoch: [18/50]         || Step: [220/1579]      || Average Training Loss: 2.3628\n",
      "Epoch: [18/50]         || Step: [240/1579]      || Average Training Loss: 2.3635\n",
      "Epoch: [18/50]         || Step: [260/1579]      || Average Training Loss: 2.3624\n",
      "Epoch: [18/50]         || Step: [280/1579]      || Average Training Loss: 2.3628\n",
      "Epoch: [18/50]         || Step: [300/1579]      || Average Training Loss: 2.3639\n",
      "Epoch: [18/50]         || Step: [320/1579]      || Average Training Loss: 2.3611\n",
      "Epoch: [18/50]         || Step: [340/1579]      || Average Training Loss: 2.3613\n",
      "Epoch: [18/50]         || Step: [360/1579]      || Average Training Loss: 2.3634\n",
      "Epoch: [18/50]         || Step: [380/1579]      || Average Training Loss: 2.3628\n",
      "Epoch: [18/50]         || Step: [400/1579]      || Average Training Loss: 2.3620\n",
      "Epoch: [18/50]         || Step: [420/1579]      || Average Training Loss: 2.3630\n",
      "Epoch: [18/50]         || Step: [440/1579]      || Average Training Loss: 2.3630\n",
      "Epoch: [18/50]         || Step: [460/1579]      || Average Training Loss: 2.3646\n",
      "Epoch: [18/50]         || Step: [480/1579]      || Average Training Loss: 2.3621\n",
      "Epoch: [18/50]         || Step: [500/1579]      || Average Training Loss: 2.3621\n",
      "Epoch: [18/50]         || Step: [520/1579]      || Average Training Loss: 2.3606\n",
      "Epoch: [18/50]         || Step: [540/1579]      || Average Training Loss: 2.3602\n",
      "Epoch: [18/50]         || Step: [560/1579]      || Average Training Loss: 2.3605\n",
      "Epoch: [18/50]         || Step: [580/1579]      || Average Training Loss: 2.3605\n",
      "Epoch: [18/50]         || Step: [600/1579]      || Average Training Loss: 2.3621\n",
      "Epoch: [18/50]         || Step: [620/1579]      || Average Training Loss: 2.3640\n",
      "Epoch: [18/50]         || Step: [640/1579]      || Average Training Loss: 2.3633\n",
      "Epoch: [18/50]         || Step: [660/1579]      || Average Training Loss: 2.3624\n",
      "Epoch: [18/50]         || Step: [680/1579]      || Average Training Loss: 2.3626\n",
      "Epoch: [18/50]         || Step: [700/1579]      || Average Training Loss: 2.3622\n",
      "Epoch: [18/50]         || Step: [720/1579]      || Average Training Loss: 2.3618\n",
      "Epoch: [18/50]         || Step: [740/1579]      || Average Training Loss: 2.3619\n",
      "Epoch: [18/50]         || Step: [760/1579]      || Average Training Loss: 2.3628\n",
      "Epoch: [18/50]         || Step: [780/1579]      || Average Training Loss: 2.3635\n",
      "Epoch: [18/50]         || Step: [800/1579]      || Average Training Loss: 2.3630\n",
      "Epoch: [18/50]         || Step: [820/1579]      || Average Training Loss: 2.3629\n",
      "Epoch: [18/50]         || Step: [840/1579]      || Average Training Loss: 2.3633\n",
      "Epoch: [18/50]         || Step: [860/1579]      || Average Training Loss: 2.3638\n",
      "Epoch: [18/50]         || Step: [880/1579]      || Average Training Loss: 2.3641\n",
      "Epoch: [18/50]         || Step: [900/1579]      || Average Training Loss: 2.3655\n",
      "Epoch: [18/50]         || Step: [920/1579]      || Average Training Loss: 2.3666\n",
      "Epoch: [18/50]         || Step: [940/1579]      || Average Training Loss: 2.3657\n",
      "Epoch: [18/50]         || Step: [960/1579]      || Average Training Loss: 2.3653\n",
      "Epoch: [18/50]         || Step: [980/1579]      || Average Training Loss: 2.3656\n",
      "Epoch: [18/50]         || Step: [1000/1579]     || Average Training Loss: 2.3659\n",
      "Epoch: [18/50]         || Step: [1020/1579]     || Average Training Loss: 2.3662\n",
      "Epoch: [18/50]         || Step: [1040/1579]     || Average Training Loss: 2.3660\n",
      "Epoch: [18/50]         || Step: [1060/1579]     || Average Training Loss: 2.3665\n",
      "Epoch: [18/50]         || Step: [1080/1579]     || Average Training Loss: 2.3664\n",
      "Epoch: [18/50]         || Step: [1100/1579]     || Average Training Loss: 2.3663\n",
      "Epoch: [18/50]         || Step: [1120/1579]     || Average Training Loss: 2.3671\n",
      "Epoch: [18/50]         || Step: [1140/1579]     || Average Training Loss: 2.3682\n",
      "Epoch: [18/50]         || Step: [1160/1579]     || Average Training Loss: 2.3690\n",
      "Epoch: [18/50]         || Step: [1180/1579]     || Average Training Loss: 2.3690\n",
      "Epoch: [18/50]         || Step: [1200/1579]     || Average Training Loss: 2.3686\n",
      "Epoch: [18/50]         || Step: [1220/1579]     || Average Training Loss: 2.3683\n",
      "Epoch: [18/50]         || Step: [1240/1579]     || Average Training Loss: 2.3685\n",
      "Epoch: [18/50]         || Step: [1260/1579]     || Average Training Loss: 2.3683\n",
      "Epoch: [18/50]         || Step: [1280/1579]     || Average Training Loss: 2.3687\n",
      "Epoch: [18/50]         || Step: [1300/1579]     || Average Training Loss: 2.3690\n",
      "Epoch: [18/50]         || Step: [1320/1579]     || Average Training Loss: 2.3685\n",
      "Epoch: [18/50]         || Step: [1340/1579]     || Average Training Loss: 2.3687\n",
      "Epoch: [18/50]         || Step: [1360/1579]     || Average Training Loss: 2.3694\n",
      "Epoch: [18/50]         || Step: [1380/1579]     || Average Training Loss: 2.3695\n",
      "Epoch: [18/50]         || Step: [1400/1579]     || Average Training Loss: 2.3693\n",
      "Epoch: [18/50]         || Step: [1420/1579]     || Average Training Loss: 2.3691\n",
      "Epoch: [18/50]         || Step: [1440/1579]     || Average Training Loss: 2.3693\n",
      "Epoch: [18/50]         || Step: [1460/1579]     || Average Training Loss: 2.3699\n",
      "Epoch: [18/50]         || Step: [1480/1579]     || Average Training Loss: 2.3700\n",
      "Epoch: [18/50]         || Step: [1500/1579]     || Average Training Loss: 2.3708\n",
      "Epoch: [18/50]         || Step: [1520/1579]     || Average Training Loss: 2.3710\n",
      "Epoch: [18/50]         || Step: [1540/1579]     || Average Training Loss: 2.3719\n",
      "Epoch: [18/50]         || Step: [1560/1579]     || Average Training Loss: 2.3717\n",
      "Epoch: [18/50]         || Step: [0/142]         || Average Validation Loss: 2.3844\n",
      "Epoch: [18/50]         || Step: [20/142]        || Average Validation Loss: 2.5293\n",
      "Epoch: [18/50]         || Step: [40/142]        || Average Validation Loss: 2.5635\n",
      "Epoch: [18/50]         || Step: [60/142]        || Average Validation Loss: 2.5631\n",
      "Epoch: [18/50]         || Step: [80/142]        || Average Validation Loss: 2.5526\n",
      "Epoch: [18/50]         || Step: [100/142]       || Average Validation Loss: 2.5636\n",
      "Epoch: [18/50]         || Step: [120/142]       || Average Validation Loss: 2.5652\n",
      "Epoch: [18/50]         || Step: [140/142]       || Average Validation Loss: 2.5575\n",
      "****************************************************************************************************\n",
      "Epoch: [18/50] || Training Loss = 2.37 || Validation Loss: 2.56 || Time: 22.069320\n",
      "****************************************************************************************************\n",
      "Epoch: [19/50]         || Step: [0/1579]        || Average Training Loss: 2.4748\n",
      "Epoch: [19/50]         || Step: [20/1579]       || Average Training Loss: 2.4890\n",
      "Epoch: [19/50]         || Step: [40/1579]       || Average Training Loss: 2.4499\n",
      "Epoch: [19/50]         || Step: [60/1579]       || Average Training Loss: 2.4453\n",
      "Epoch: [19/50]         || Step: [80/1579]       || Average Training Loss: 2.4279\n",
      "Epoch: [19/50]         || Step: [100/1579]      || Average Training Loss: 2.4141\n",
      "Epoch: [19/50]         || Step: [120/1579]      || Average Training Loss: 2.4106\n",
      "Epoch: [19/50]         || Step: [140/1579]      || Average Training Loss: 2.4007\n",
      "Epoch: [19/50]         || Step: [160/1579]      || Average Training Loss: 2.4021\n",
      "Epoch: [19/50]         || Step: [180/1579]      || Average Training Loss: 2.4002\n",
      "Epoch: [19/50]         || Step: [200/1579]      || Average Training Loss: 2.3948\n",
      "Epoch: [19/50]         || Step: [220/1579]      || Average Training Loss: 2.3900\n",
      "Epoch: [19/50]         || Step: [240/1579]      || Average Training Loss: 2.3873\n",
      "Epoch: [19/50]         || Step: [260/1579]      || Average Training Loss: 2.3843\n",
      "Epoch: [19/50]         || Step: [280/1579]      || Average Training Loss: 2.3830\n",
      "Epoch: [19/50]         || Step: [300/1579]      || Average Training Loss: 2.3812\n",
      "Epoch: [19/50]         || Step: [320/1579]      || Average Training Loss: 2.3815\n",
      "Epoch: [19/50]         || Step: [340/1579]      || Average Training Loss: 2.3811\n",
      "Epoch: [19/50]         || Step: [360/1579]      || Average Training Loss: 2.3825\n",
      "Epoch: [19/50]         || Step: [380/1579]      || Average Training Loss: 2.3813\n",
      "Epoch: [19/50]         || Step: [400/1579]      || Average Training Loss: 2.3810\n",
      "Epoch: [19/50]         || Step: [420/1579]      || Average Training Loss: 2.3787\n",
      "Epoch: [19/50]         || Step: [440/1579]      || Average Training Loss: 2.3795\n",
      "Epoch: [19/50]         || Step: [460/1579]      || Average Training Loss: 2.3797\n",
      "Epoch: [19/50]         || Step: [480/1579]      || Average Training Loss: 2.3768\n",
      "Epoch: [19/50]         || Step: [500/1579]      || Average Training Loss: 2.3769\n",
      "Epoch: [19/50]         || Step: [520/1579]      || Average Training Loss: 2.3783\n",
      "Epoch: [19/50]         || Step: [540/1579]      || Average Training Loss: 2.3789\n",
      "Epoch: [19/50]         || Step: [560/1579]      || Average Training Loss: 2.3778\n",
      "Epoch: [19/50]         || Step: [580/1579]      || Average Training Loss: 2.3772\n",
      "Epoch: [19/50]         || Step: [600/1579]      || Average Training Loss: 2.3766\n",
      "Epoch: [19/50]         || Step: [620/1579]      || Average Training Loss: 2.3771\n",
      "Epoch: [19/50]         || Step: [640/1579]      || Average Training Loss: 2.3770\n",
      "Epoch: [19/50]         || Step: [660/1579]      || Average Training Loss: 2.3769\n",
      "Epoch: [19/50]         || Step: [680/1579]      || Average Training Loss: 2.3767\n",
      "Epoch: [19/50]         || Step: [700/1579]      || Average Training Loss: 2.3768\n",
      "Epoch: [19/50]         || Step: [720/1579]      || Average Training Loss: 2.3764\n",
      "Epoch: [19/50]         || Step: [740/1579]      || Average Training Loss: 2.3757\n",
      "Epoch: [19/50]         || Step: [760/1579]      || Average Training Loss: 2.3755\n",
      "Epoch: [19/50]         || Step: [780/1579]      || Average Training Loss: 2.3756\n",
      "Epoch: [19/50]         || Step: [800/1579]      || Average Training Loss: 2.3749\n",
      "Epoch: [19/50]         || Step: [820/1579]      || Average Training Loss: 2.3755\n",
      "Epoch: [19/50]         || Step: [840/1579]      || Average Training Loss: 2.3757\n",
      "Epoch: [19/50]         || Step: [860/1579]      || Average Training Loss: 2.3762\n",
      "Epoch: [19/50]         || Step: [880/1579]      || Average Training Loss: 2.3762\n",
      "Epoch: [19/50]         || Step: [900/1579]      || Average Training Loss: 2.3764\n",
      "Epoch: [19/50]         || Step: [920/1579]      || Average Training Loss: 2.3767\n",
      "Epoch: [19/50]         || Step: [940/1579]      || Average Training Loss: 2.3757\n",
      "Epoch: [19/50]         || Step: [960/1579]      || Average Training Loss: 2.3763\n",
      "Epoch: [19/50]         || Step: [980/1579]      || Average Training Loss: 2.3768\n",
      "Epoch: [19/50]         || Step: [1000/1579]     || Average Training Loss: 2.3758\n",
      "Epoch: [19/50]         || Step: [1020/1579]     || Average Training Loss: 2.3747\n",
      "Epoch: [19/50]         || Step: [1040/1579]     || Average Training Loss: 2.3740\n",
      "Epoch: [19/50]         || Step: [1060/1579]     || Average Training Loss: 2.3746\n",
      "Epoch: [19/50]         || Step: [1080/1579]     || Average Training Loss: 2.3742\n",
      "Epoch: [19/50]         || Step: [1100/1579]     || Average Training Loss: 2.3740\n",
      "Epoch: [19/50]         || Step: [1120/1579]     || Average Training Loss: 2.3724\n",
      "Epoch: [19/50]         || Step: [1140/1579]     || Average Training Loss: 2.3712\n",
      "Epoch: [19/50]         || Step: [1160/1579]     || Average Training Loss: 2.3710\n",
      "Epoch: [19/50]         || Step: [1180/1579]     || Average Training Loss: 2.3708\n",
      "Epoch: [19/50]         || Step: [1200/1579]     || Average Training Loss: 2.3709\n",
      "Epoch: [19/50]         || Step: [1220/1579]     || Average Training Loss: 2.3708\n",
      "Epoch: [19/50]         || Step: [1240/1579]     || Average Training Loss: 2.3705\n",
      "Epoch: [19/50]         || Step: [1260/1579]     || Average Training Loss: 2.3703\n",
      "Epoch: [19/50]         || Step: [1280/1579]     || Average Training Loss: 2.3695\n",
      "Epoch: [19/50]         || Step: [1300/1579]     || Average Training Loss: 2.3698\n",
      "Epoch: [19/50]         || Step: [1320/1579]     || Average Training Loss: 2.3696\n",
      "Epoch: [19/50]         || Step: [1340/1579]     || Average Training Loss: 2.3699\n",
      "Epoch: [19/50]         || Step: [1360/1579]     || Average Training Loss: 2.3698\n",
      "Epoch: [19/50]         || Step: [1380/1579]     || Average Training Loss: 2.3695\n",
      "Epoch: [19/50]         || Step: [1400/1579]     || Average Training Loss: 2.3697\n",
      "Epoch: [19/50]         || Step: [1420/1579]     || Average Training Loss: 2.3706\n",
      "Epoch: [19/50]         || Step: [1440/1579]     || Average Training Loss: 2.3707\n",
      "Epoch: [19/50]         || Step: [1460/1579]     || Average Training Loss: 2.3709\n",
      "Epoch: [19/50]         || Step: [1480/1579]     || Average Training Loss: 2.3708\n",
      "Epoch: [19/50]         || Step: [1500/1579]     || Average Training Loss: 2.3715\n",
      "Epoch: [19/50]         || Step: [1520/1579]     || Average Training Loss: 2.3722\n",
      "Epoch: [19/50]         || Step: [1540/1579]     || Average Training Loss: 2.3726\n",
      "Epoch: [19/50]         || Step: [1560/1579]     || Average Training Loss: 2.3728\n",
      "Epoch: [19/50]         || Step: [0/142]         || Average Validation Loss: 2.5646\n",
      "Epoch: [19/50]         || Step: [20/142]        || Average Validation Loss: 2.4911\n",
      "Epoch: [19/50]         || Step: [40/142]        || Average Validation Loss: 2.5003\n",
      "Epoch: [19/50]         || Step: [60/142]        || Average Validation Loss: 2.4993\n",
      "Epoch: [19/50]         || Step: [80/142]        || Average Validation Loss: 2.4844\n",
      "Epoch: [19/50]         || Step: [100/142]       || Average Validation Loss: 2.4850\n",
      "Epoch: [19/50]         || Step: [120/142]       || Average Validation Loss: 2.4780\n",
      "Epoch: [19/50]         || Step: [140/142]       || Average Validation Loss: 2.4815\n",
      "****************************************************************************************************\n",
      "Epoch: [19/50] || Training Loss = 2.37 || Validation Loss: 2.48 || Time: 22.213060\n",
      "****************************************************************************************************\n",
      "Epoch: [20/50]         || Step: [0/1579]        || Average Training Loss: 2.5040\n",
      "Epoch: [20/50]         || Step: [20/1579]       || Average Training Loss: 2.4354\n",
      "Epoch: [20/50]         || Step: [40/1579]       || Average Training Loss: 2.4034\n",
      "Epoch: [20/50]         || Step: [60/1579]       || Average Training Loss: 2.3980\n",
      "Epoch: [20/50]         || Step: [80/1579]       || Average Training Loss: 2.3850\n",
      "Epoch: [20/50]         || Step: [100/1579]      || Average Training Loss: 2.3866\n",
      "Epoch: [20/50]         || Step: [120/1579]      || Average Training Loss: 2.3800\n",
      "Epoch: [20/50]         || Step: [140/1579]      || Average Training Loss: 2.3738\n",
      "Epoch: [20/50]         || Step: [160/1579]      || Average Training Loss: 2.3692\n",
      "Epoch: [20/50]         || Step: [180/1579]      || Average Training Loss: 2.3680\n",
      "Epoch: [20/50]         || Step: [200/1579]      || Average Training Loss: 2.3684\n",
      "Epoch: [20/50]         || Step: [220/1579]      || Average Training Loss: 2.3708\n",
      "Epoch: [20/50]         || Step: [240/1579]      || Average Training Loss: 2.3670\n",
      "Epoch: [20/50]         || Step: [260/1579]      || Average Training Loss: 2.3635\n",
      "Epoch: [20/50]         || Step: [280/1579]      || Average Training Loss: 2.3619\n",
      "Epoch: [20/50]         || Step: [300/1579]      || Average Training Loss: 2.3602\n",
      "Epoch: [20/50]         || Step: [320/1579]      || Average Training Loss: 2.3603\n",
      "Epoch: [20/50]         || Step: [340/1579]      || Average Training Loss: 2.3593\n",
      "Epoch: [20/50]         || Step: [360/1579]      || Average Training Loss: 2.3593\n",
      "Epoch: [20/50]         || Step: [380/1579]      || Average Training Loss: 2.3595\n",
      "Epoch: [20/50]         || Step: [400/1579]      || Average Training Loss: 2.3618\n",
      "Epoch: [20/50]         || Step: [420/1579]      || Average Training Loss: 2.3606\n",
      "Epoch: [20/50]         || Step: [440/1579]      || Average Training Loss: 2.3604\n",
      "Epoch: [20/50]         || Step: [460/1579]      || Average Training Loss: 2.3600\n",
      "Epoch: [20/50]         || Step: [480/1579]      || Average Training Loss: 2.3601\n",
      "Epoch: [20/50]         || Step: [500/1579]      || Average Training Loss: 2.3590\n",
      "Epoch: [20/50]         || Step: [520/1579]      || Average Training Loss: 2.3592\n",
      "Epoch: [20/50]         || Step: [540/1579]      || Average Training Loss: 2.3592\n",
      "Epoch: [20/50]         || Step: [560/1579]      || Average Training Loss: 2.3594\n",
      "Epoch: [20/50]         || Step: [580/1579]      || Average Training Loss: 2.3614\n",
      "Epoch: [20/50]         || Step: [600/1579]      || Average Training Loss: 2.3607\n",
      "Epoch: [20/50]         || Step: [620/1579]      || Average Training Loss: 2.3602\n",
      "Epoch: [20/50]         || Step: [640/1579]      || Average Training Loss: 2.3610\n",
      "Epoch: [20/50]         || Step: [660/1579]      || Average Training Loss: 2.3603\n",
      "Epoch: [20/50]         || Step: [680/1579]      || Average Training Loss: 2.3587\n",
      "Epoch: [20/50]         || Step: [700/1579]      || Average Training Loss: 2.3593\n",
      "Epoch: [20/50]         || Step: [720/1579]      || Average Training Loss: 2.3595\n",
      "Epoch: [20/50]         || Step: [740/1579]      || Average Training Loss: 2.3590\n",
      "Epoch: [20/50]         || Step: [760/1579]      || Average Training Loss: 2.3603\n",
      "Epoch: [20/50]         || Step: [780/1579]      || Average Training Loss: 2.3595\n",
      "Epoch: [20/50]         || Step: [800/1579]      || Average Training Loss: 2.3590\n",
      "Epoch: [20/50]         || Step: [820/1579]      || Average Training Loss: 2.3600\n",
      "Epoch: [20/50]         || Step: [840/1579]      || Average Training Loss: 2.3611\n",
      "Epoch: [20/50]         || Step: [860/1579]      || Average Training Loss: 2.3615\n",
      "Epoch: [20/50]         || Step: [880/1579]      || Average Training Loss: 2.3625\n",
      "Epoch: [20/50]         || Step: [900/1579]      || Average Training Loss: 2.3617\n",
      "Epoch: [20/50]         || Step: [920/1579]      || Average Training Loss: 2.3605\n",
      "Epoch: [20/50]         || Step: [940/1579]      || Average Training Loss: 2.3603\n",
      "Epoch: [20/50]         || Step: [960/1579]      || Average Training Loss: 2.3609\n",
      "Epoch: [20/50]         || Step: [980/1579]      || Average Training Loss: 2.3623\n",
      "Epoch: [20/50]         || Step: [1000/1579]     || Average Training Loss: 2.3625\n",
      "Epoch: [20/50]         || Step: [1020/1579]     || Average Training Loss: 2.3635\n",
      "Epoch: [20/50]         || Step: [1040/1579]     || Average Training Loss: 2.3644\n",
      "Epoch: [20/50]         || Step: [1060/1579]     || Average Training Loss: 2.3644\n",
      "Epoch: [20/50]         || Step: [1080/1579]     || Average Training Loss: 2.3650\n",
      "Epoch: [20/50]         || Step: [1100/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [20/50]         || Step: [1120/1579]     || Average Training Loss: 2.3657\n",
      "Epoch: [20/50]         || Step: [1140/1579]     || Average Training Loss: 2.3660\n",
      "Epoch: [20/50]         || Step: [1160/1579]     || Average Training Loss: 2.3671\n",
      "Epoch: [20/50]         || Step: [1180/1579]     || Average Training Loss: 2.3678\n",
      "Epoch: [20/50]         || Step: [1200/1579]     || Average Training Loss: 2.3678\n",
      "Epoch: [20/50]         || Step: [1220/1579]     || Average Training Loss: 2.3683\n",
      "Epoch: [20/50]         || Step: [1240/1579]     || Average Training Loss: 2.3680\n",
      "Epoch: [20/50]         || Step: [1260/1579]     || Average Training Loss: 2.3680\n",
      "Epoch: [20/50]         || Step: [1280/1579]     || Average Training Loss: 2.3673\n",
      "Epoch: [20/50]         || Step: [1300/1579]     || Average Training Loss: 2.3681\n",
      "Epoch: [20/50]         || Step: [1320/1579]     || Average Training Loss: 2.3677\n",
      "Epoch: [20/50]         || Step: [1340/1579]     || Average Training Loss: 2.3666\n",
      "Epoch: [20/50]         || Step: [1360/1579]     || Average Training Loss: 2.3667\n",
      "Epoch: [20/50]         || Step: [1380/1579]     || Average Training Loss: 2.3673\n",
      "Epoch: [20/50]         || Step: [1400/1579]     || Average Training Loss: 2.3670\n",
      "Epoch: [20/50]         || Step: [1420/1579]     || Average Training Loss: 2.3675\n",
      "Epoch: [20/50]         || Step: [1440/1579]     || Average Training Loss: 2.3682\n",
      "Epoch: [20/50]         || Step: [1460/1579]     || Average Training Loss: 2.3679\n",
      "Epoch: [20/50]         || Step: [1480/1579]     || Average Training Loss: 2.3681\n",
      "Epoch: [20/50]         || Step: [1500/1579]     || Average Training Loss: 2.3675\n",
      "Epoch: [20/50]         || Step: [1520/1579]     || Average Training Loss: 2.3674\n",
      "Epoch: [20/50]         || Step: [1540/1579]     || Average Training Loss: 2.3678\n",
      "Epoch: [20/50]         || Step: [1560/1579]     || Average Training Loss: 2.3686\n",
      "Epoch: [20/50]         || Step: [0/142]         || Average Validation Loss: 2.4470\n",
      "Epoch: [20/50]         || Step: [20/142]        || Average Validation Loss: 2.3498\n",
      "Epoch: [20/50]         || Step: [40/142]        || Average Validation Loss: 2.3678\n",
      "Epoch: [20/50]         || Step: [60/142]        || Average Validation Loss: 2.3694\n",
      "Epoch: [20/50]         || Step: [80/142]        || Average Validation Loss: 2.3685\n",
      "Epoch: [20/50]         || Step: [100/142]       || Average Validation Loss: 2.3740\n",
      "Epoch: [20/50]         || Step: [120/142]       || Average Validation Loss: 2.3603\n",
      "Epoch: [20/50]         || Step: [140/142]       || Average Validation Loss: 2.3611\n",
      "****************************************************************************************************\n",
      "Epoch: [20/50] || Training Loss = 2.37 || Validation Loss: 2.36 || Time: 21.466330\n",
      "****************************************************************************************************\n",
      "Epoch: [21/50]         || Step: [0/1579]        || Average Training Loss: 2.3487\n",
      "Epoch: [21/50]         || Step: [20/1579]       || Average Training Loss: 2.3986\n",
      "Epoch: [21/50]         || Step: [40/1579]       || Average Training Loss: 2.3918\n",
      "Epoch: [21/50]         || Step: [60/1579]       || Average Training Loss: 2.3836\n",
      "Epoch: [21/50]         || Step: [80/1579]       || Average Training Loss: 2.3910\n",
      "Epoch: [21/50]         || Step: [100/1579]      || Average Training Loss: 2.3870\n",
      "Epoch: [21/50]         || Step: [120/1579]      || Average Training Loss: 2.3912\n",
      "Epoch: [21/50]         || Step: [140/1579]      || Average Training Loss: 2.3816\n",
      "Epoch: [21/50]         || Step: [160/1579]      || Average Training Loss: 2.3793\n",
      "Epoch: [21/50]         || Step: [180/1579]      || Average Training Loss: 2.3747\n",
      "Epoch: [21/50]         || Step: [200/1579]      || Average Training Loss: 2.3725\n",
      "Epoch: [21/50]         || Step: [220/1579]      || Average Training Loss: 2.3709\n",
      "Epoch: [21/50]         || Step: [240/1579]      || Average Training Loss: 2.3735\n",
      "Epoch: [21/50]         || Step: [260/1579]      || Average Training Loss: 2.3733\n",
      "Epoch: [21/50]         || Step: [280/1579]      || Average Training Loss: 2.3733\n",
      "Epoch: [21/50]         || Step: [300/1579]      || Average Training Loss: 2.3703\n",
      "Epoch: [21/50]         || Step: [320/1579]      || Average Training Loss: 2.3705\n",
      "Epoch: [21/50]         || Step: [340/1579]      || Average Training Loss: 2.3679\n",
      "Epoch: [21/50]         || Step: [360/1579]      || Average Training Loss: 2.3682\n",
      "Epoch: [21/50]         || Step: [380/1579]      || Average Training Loss: 2.3694\n",
      "Epoch: [21/50]         || Step: [400/1579]      || Average Training Loss: 2.3700\n",
      "Epoch: [21/50]         || Step: [420/1579]      || Average Training Loss: 2.3706\n",
      "Epoch: [21/50]         || Step: [440/1579]      || Average Training Loss: 2.3699\n",
      "Epoch: [21/50]         || Step: [460/1579]      || Average Training Loss: 2.3691\n",
      "Epoch: [21/50]         || Step: [480/1579]      || Average Training Loss: 2.3687\n",
      "Epoch: [21/50]         || Step: [500/1579]      || Average Training Loss: 2.3687\n",
      "Epoch: [21/50]         || Step: [520/1579]      || Average Training Loss: 2.3685\n",
      "Epoch: [21/50]         || Step: [540/1579]      || Average Training Loss: 2.3695\n",
      "Epoch: [21/50]         || Step: [560/1579]      || Average Training Loss: 2.3700\n",
      "Epoch: [21/50]         || Step: [580/1579]      || Average Training Loss: 2.3711\n",
      "Epoch: [21/50]         || Step: [600/1579]      || Average Training Loss: 2.3702\n",
      "Epoch: [21/50]         || Step: [620/1579]      || Average Training Loss: 2.3701\n",
      "Epoch: [21/50]         || Step: [640/1579]      || Average Training Loss: 2.3693\n",
      "Epoch: [21/50]         || Step: [660/1579]      || Average Training Loss: 2.3700\n",
      "Epoch: [21/50]         || Step: [680/1579]      || Average Training Loss: 2.3682\n",
      "Epoch: [21/50]         || Step: [700/1579]      || Average Training Loss: 2.3688\n",
      "Epoch: [21/50]         || Step: [720/1579]      || Average Training Loss: 2.3684\n",
      "Epoch: [21/50]         || Step: [740/1579]      || Average Training Loss: 2.3681\n",
      "Epoch: [21/50]         || Step: [760/1579]      || Average Training Loss: 2.3675\n",
      "Epoch: [21/50]         || Step: [780/1579]      || Average Training Loss: 2.3666\n",
      "Epoch: [21/50]         || Step: [800/1579]      || Average Training Loss: 2.3671\n",
      "Epoch: [21/50]         || Step: [820/1579]      || Average Training Loss: 2.3680\n",
      "Epoch: [21/50]         || Step: [840/1579]      || Average Training Loss: 2.3671\n",
      "Epoch: [21/50]         || Step: [860/1579]      || Average Training Loss: 2.3660\n",
      "Epoch: [21/50]         || Step: [880/1579]      || Average Training Loss: 2.3662\n",
      "Epoch: [21/50]         || Step: [900/1579]      || Average Training Loss: 2.3663\n",
      "Epoch: [21/50]         || Step: [920/1579]      || Average Training Loss: 2.3656\n",
      "Epoch: [21/50]         || Step: [940/1579]      || Average Training Loss: 2.3661\n",
      "Epoch: [21/50]         || Step: [960/1579]      || Average Training Loss: 2.3658\n",
      "Epoch: [21/50]         || Step: [980/1579]      || Average Training Loss: 2.3661\n",
      "Epoch: [21/50]         || Step: [1000/1579]     || Average Training Loss: 2.3657\n",
      "Epoch: [21/50]         || Step: [1020/1579]     || Average Training Loss: 2.3659\n",
      "Epoch: [21/50]         || Step: [1040/1579]     || Average Training Loss: 2.3660\n",
      "Epoch: [21/50]         || Step: [1060/1579]     || Average Training Loss: 2.3657\n",
      "Epoch: [21/50]         || Step: [1080/1579]     || Average Training Loss: 2.3665\n",
      "Epoch: [21/50]         || Step: [1100/1579]     || Average Training Loss: 2.3672\n",
      "Epoch: [21/50]         || Step: [1120/1579]     || Average Training Loss: 2.3678\n",
      "Epoch: [21/50]         || Step: [1140/1579]     || Average Training Loss: 2.3670\n",
      "Epoch: [21/50]         || Step: [1160/1579]     || Average Training Loss: 2.3687\n",
      "Epoch: [21/50]         || Step: [1180/1579]     || Average Training Loss: 2.3692\n",
      "Epoch: [21/50]         || Step: [1200/1579]     || Average Training Loss: 2.3691\n",
      "Epoch: [21/50]         || Step: [1220/1579]     || Average Training Loss: 2.3695\n",
      "Epoch: [21/50]         || Step: [1240/1579]     || Average Training Loss: 2.3702\n",
      "Epoch: [21/50]         || Step: [1260/1579]     || Average Training Loss: 2.3701\n",
      "Epoch: [21/50]         || Step: [1280/1579]     || Average Training Loss: 2.3702\n",
      "Epoch: [21/50]         || Step: [1300/1579]     || Average Training Loss: 2.3698\n",
      "Epoch: [21/50]         || Step: [1320/1579]     || Average Training Loss: 2.3697\n",
      "Epoch: [21/50]         || Step: [1340/1579]     || Average Training Loss: 2.3691\n",
      "Epoch: [21/50]         || Step: [1360/1579]     || Average Training Loss: 2.3690\n",
      "Epoch: [21/50]         || Step: [1380/1579]     || Average Training Loss: 2.3687\n",
      "Epoch: [21/50]         || Step: [1400/1579]     || Average Training Loss: 2.3683\n",
      "Epoch: [21/50]         || Step: [1420/1579]     || Average Training Loss: 2.3678\n",
      "Epoch: [21/50]         || Step: [1440/1579]     || Average Training Loss: 2.3679\n",
      "Epoch: [21/50]         || Step: [1460/1579]     || Average Training Loss: 2.3675\n",
      "Epoch: [21/50]         || Step: [1480/1579]     || Average Training Loss: 2.3677\n",
      "Epoch: [21/50]         || Step: [1500/1579]     || Average Training Loss: 2.3673\n",
      "Epoch: [21/50]         || Step: [1520/1579]     || Average Training Loss: 2.3678\n",
      "Epoch: [21/50]         || Step: [1540/1579]     || Average Training Loss: 2.3678\n",
      "Epoch: [21/50]         || Step: [1560/1579]     || Average Training Loss: 2.3682\n",
      "Epoch: [21/50]         || Step: [0/142]         || Average Validation Loss: 2.8638\n",
      "Epoch: [21/50]         || Step: [20/142]        || Average Validation Loss: 2.7977\n",
      "Epoch: [21/50]         || Step: [40/142]        || Average Validation Loss: 2.8143\n",
      "Epoch: [21/50]         || Step: [60/142]        || Average Validation Loss: 2.8109\n",
      "Epoch: [21/50]         || Step: [80/142]        || Average Validation Loss: 2.8151\n",
      "Epoch: [21/50]         || Step: [100/142]       || Average Validation Loss: 2.8013\n",
      "Epoch: [21/50]         || Step: [120/142]       || Average Validation Loss: 2.8015\n",
      "Epoch: [21/50]         || Step: [140/142]       || Average Validation Loss: 2.8016\n",
      "****************************************************************************************************\n",
      "Epoch: [21/50] || Training Loss = 2.37 || Validation Loss: 2.80 || Time: 21.735431\n",
      "****************************************************************************************************\n",
      "Epoch: [22/50]         || Step: [0/1579]        || Average Training Loss: 2.6981\n",
      "Epoch: [22/50]         || Step: [20/1579]       || Average Training Loss: 2.5375\n",
      "Epoch: [22/50]         || Step: [40/1579]       || Average Training Loss: 2.4763\n",
      "Epoch: [22/50]         || Step: [60/1579]       || Average Training Loss: 2.4356\n",
      "Epoch: [22/50]         || Step: [80/1579]       || Average Training Loss: 2.4101\n",
      "Epoch: [22/50]         || Step: [100/1579]      || Average Training Loss: 2.4102\n",
      "Epoch: [22/50]         || Step: [120/1579]      || Average Training Loss: 2.4062\n",
      "Epoch: [22/50]         || Step: [140/1579]      || Average Training Loss: 2.4035\n",
      "Epoch: [22/50]         || Step: [160/1579]      || Average Training Loss: 2.4016\n",
      "Epoch: [22/50]         || Step: [180/1579]      || Average Training Loss: 2.3948\n",
      "Epoch: [22/50]         || Step: [200/1579]      || Average Training Loss: 2.3928\n",
      "Epoch: [22/50]         || Step: [220/1579]      || Average Training Loss: 2.3922\n",
      "Epoch: [22/50]         || Step: [240/1579]      || Average Training Loss: 2.3925\n",
      "Epoch: [22/50]         || Step: [260/1579]      || Average Training Loss: 2.3879\n",
      "Epoch: [22/50]         || Step: [280/1579]      || Average Training Loss: 2.3865\n",
      "Epoch: [22/50]         || Step: [300/1579]      || Average Training Loss: 2.3857\n",
      "Epoch: [22/50]         || Step: [320/1579]      || Average Training Loss: 2.3851\n",
      "Epoch: [22/50]         || Step: [340/1579]      || Average Training Loss: 2.3815\n",
      "Epoch: [22/50]         || Step: [360/1579]      || Average Training Loss: 2.3810\n",
      "Epoch: [22/50]         || Step: [380/1579]      || Average Training Loss: 2.3800\n",
      "Epoch: [22/50]         || Step: [400/1579]      || Average Training Loss: 2.3794\n",
      "Epoch: [22/50]         || Step: [420/1579]      || Average Training Loss: 2.3765\n",
      "Epoch: [22/50]         || Step: [440/1579]      || Average Training Loss: 2.3753\n",
      "Epoch: [22/50]         || Step: [460/1579]      || Average Training Loss: 2.3752\n",
      "Epoch: [22/50]         || Step: [480/1579]      || Average Training Loss: 2.3738\n",
      "Epoch: [22/50]         || Step: [500/1579]      || Average Training Loss: 2.3743\n",
      "Epoch: [22/50]         || Step: [520/1579]      || Average Training Loss: 2.3752\n",
      "Epoch: [22/50]         || Step: [540/1579]      || Average Training Loss: 2.3744\n",
      "Epoch: [22/50]         || Step: [560/1579]      || Average Training Loss: 2.3732\n",
      "Epoch: [22/50]         || Step: [580/1579]      || Average Training Loss: 2.3749\n",
      "Epoch: [22/50]         || Step: [600/1579]      || Average Training Loss: 2.3749\n",
      "Epoch: [22/50]         || Step: [620/1579]      || Average Training Loss: 2.3749\n",
      "Epoch: [22/50]         || Step: [640/1579]      || Average Training Loss: 2.3742\n",
      "Epoch: [22/50]         || Step: [660/1579]      || Average Training Loss: 2.3722\n",
      "Epoch: [22/50]         || Step: [680/1579]      || Average Training Loss: 2.3718\n",
      "Epoch: [22/50]         || Step: [700/1579]      || Average Training Loss: 2.3711\n",
      "Epoch: [22/50]         || Step: [720/1579]      || Average Training Loss: 2.3688\n",
      "Epoch: [22/50]         || Step: [740/1579]      || Average Training Loss: 2.3694\n",
      "Epoch: [22/50]         || Step: [760/1579]      || Average Training Loss: 2.3695\n",
      "Epoch: [22/50]         || Step: [780/1579]      || Average Training Loss: 2.3688\n",
      "Epoch: [22/50]         || Step: [800/1579]      || Average Training Loss: 2.3691\n",
      "Epoch: [22/50]         || Step: [820/1579]      || Average Training Loss: 2.3688\n",
      "Epoch: [22/50]         || Step: [840/1579]      || Average Training Loss: 2.3700\n",
      "Epoch: [22/50]         || Step: [860/1579]      || Average Training Loss: 2.3705\n",
      "Epoch: [22/50]         || Step: [880/1579]      || Average Training Loss: 2.3707\n",
      "Epoch: [22/50]         || Step: [900/1579]      || Average Training Loss: 2.3730\n",
      "Epoch: [22/50]         || Step: [920/1579]      || Average Training Loss: 2.3720\n",
      "Epoch: [22/50]         || Step: [940/1579]      || Average Training Loss: 2.3714\n",
      "Epoch: [22/50]         || Step: [960/1579]      || Average Training Loss: 2.3710\n",
      "Epoch: [22/50]         || Step: [980/1579]      || Average Training Loss: 2.3705\n",
      "Epoch: [22/50]         || Step: [1000/1579]     || Average Training Loss: 2.3692\n",
      "Epoch: [22/50]         || Step: [1020/1579]     || Average Training Loss: 2.3693\n",
      "Epoch: [22/50]         || Step: [1040/1579]     || Average Training Loss: 2.3698\n",
      "Epoch: [22/50]         || Step: [1060/1579]     || Average Training Loss: 2.3694\n",
      "Epoch: [22/50]         || Step: [1080/1579]     || Average Training Loss: 2.3694\n",
      "Epoch: [22/50]         || Step: [1100/1579]     || Average Training Loss: 2.3694\n",
      "Epoch: [22/50]         || Step: [1120/1579]     || Average Training Loss: 2.3687\n",
      "Epoch: [22/50]         || Step: [1140/1579]     || Average Training Loss: 2.3694\n",
      "Epoch: [22/50]         || Step: [1160/1579]     || Average Training Loss: 2.3700\n",
      "Epoch: [22/50]         || Step: [1180/1579]     || Average Training Loss: 2.3695\n",
      "Epoch: [22/50]         || Step: [1200/1579]     || Average Training Loss: 2.3699\n",
      "Epoch: [22/50]         || Step: [1220/1579]     || Average Training Loss: 2.3708\n",
      "Epoch: [22/50]         || Step: [1240/1579]     || Average Training Loss: 2.3712\n",
      "Epoch: [22/50]         || Step: [1260/1579]     || Average Training Loss: 2.3716\n",
      "Epoch: [22/50]         || Step: [1280/1579]     || Average Training Loss: 2.3719\n",
      "Epoch: [22/50]         || Step: [1300/1579]     || Average Training Loss: 2.3724\n",
      "Epoch: [22/50]         || Step: [1320/1579]     || Average Training Loss: 2.3722\n",
      "Epoch: [22/50]         || Step: [1340/1579]     || Average Training Loss: 2.3721\n",
      "Epoch: [22/50]         || Step: [1360/1579]     || Average Training Loss: 2.3717\n",
      "Epoch: [22/50]         || Step: [1380/1579]     || Average Training Loss: 2.3714\n",
      "Epoch: [22/50]         || Step: [1400/1579]     || Average Training Loss: 2.3707\n",
      "Epoch: [22/50]         || Step: [1420/1579]     || Average Training Loss: 2.3704\n",
      "Epoch: [22/50]         || Step: [1440/1579]     || Average Training Loss: 2.3697\n",
      "Epoch: [22/50]         || Step: [1460/1579]     || Average Training Loss: 2.3691\n",
      "Epoch: [22/50]         || Step: [1480/1579]     || Average Training Loss: 2.3693\n",
      "Epoch: [22/50]         || Step: [1500/1579]     || Average Training Loss: 2.3694\n",
      "Epoch: [22/50]         || Step: [1520/1579]     || Average Training Loss: 2.3692\n",
      "Epoch: [22/50]         || Step: [1540/1579]     || Average Training Loss: 2.3693\n",
      "Epoch: [22/50]         || Step: [1560/1579]     || Average Training Loss: 2.3696\n",
      "Epoch: [22/50]         || Step: [0/142]         || Average Validation Loss: 2.4138\n",
      "Epoch: [22/50]         || Step: [20/142]        || Average Validation Loss: 2.3566\n",
      "Epoch: [22/50]         || Step: [40/142]        || Average Validation Loss: 2.3494\n",
      "Epoch: [22/50]         || Step: [60/142]        || Average Validation Loss: 2.3323\n",
      "Epoch: [22/50]         || Step: [80/142]        || Average Validation Loss: 2.3314\n",
      "Epoch: [22/50]         || Step: [100/142]       || Average Validation Loss: 2.3269\n",
      "Epoch: [22/50]         || Step: [120/142]       || Average Validation Loss: 2.3278\n",
      "Epoch: [22/50]         || Step: [140/142]       || Average Validation Loss: 2.3397\n",
      "****************************************************************************************************\n",
      "Epoch: [22/50] || Training Loss = 2.37 || Validation Loss: 2.34 || Time: 23.081449\n",
      "****************************************************************************************************\n",
      "Epoch: [23/50]         || Step: [0/1579]        || Average Training Loss: 2.4982\n",
      "Epoch: [23/50]         || Step: [20/1579]       || Average Training Loss: 2.3608\n",
      "Epoch: [23/50]         || Step: [40/1579]       || Average Training Loss: 2.3550\n",
      "Epoch: [23/50]         || Step: [60/1579]       || Average Training Loss: 2.3480\n",
      "Epoch: [23/50]         || Step: [80/1579]       || Average Training Loss: 2.3562\n",
      "Epoch: [23/50]         || Step: [100/1579]      || Average Training Loss: 2.3570\n",
      "Epoch: [23/50]         || Step: [120/1579]      || Average Training Loss: 2.3606\n",
      "Epoch: [23/50]         || Step: [140/1579]      || Average Training Loss: 2.3679\n",
      "Epoch: [23/50]         || Step: [160/1579]      || Average Training Loss: 2.3664\n",
      "Epoch: [23/50]         || Step: [180/1579]      || Average Training Loss: 2.3656\n",
      "Epoch: [23/50]         || Step: [200/1579]      || Average Training Loss: 2.3659\n",
      "Epoch: [23/50]         || Step: [220/1579]      || Average Training Loss: 2.3642\n",
      "Epoch: [23/50]         || Step: [240/1579]      || Average Training Loss: 2.3652\n",
      "Epoch: [23/50]         || Step: [260/1579]      || Average Training Loss: 2.3662\n",
      "Epoch: [23/50]         || Step: [280/1579]      || Average Training Loss: 2.3686\n",
      "Epoch: [23/50]         || Step: [300/1579]      || Average Training Loss: 2.3668\n",
      "Epoch: [23/50]         || Step: [320/1579]      || Average Training Loss: 2.3686\n",
      "Epoch: [23/50]         || Step: [340/1579]      || Average Training Loss: 2.3703\n",
      "Epoch: [23/50]         || Step: [360/1579]      || Average Training Loss: 2.3706\n",
      "Epoch: [23/50]         || Step: [380/1579]      || Average Training Loss: 2.3697\n",
      "Epoch: [23/50]         || Step: [400/1579]      || Average Training Loss: 2.3686\n",
      "Epoch: [23/50]         || Step: [420/1579]      || Average Training Loss: 2.3687\n",
      "Epoch: [23/50]         || Step: [440/1579]      || Average Training Loss: 2.3684\n",
      "Epoch: [23/50]         || Step: [460/1579]      || Average Training Loss: 2.3684\n",
      "Epoch: [23/50]         || Step: [480/1579]      || Average Training Loss: 2.3670\n",
      "Epoch: [23/50]         || Step: [500/1579]      || Average Training Loss: 2.3686\n",
      "Epoch: [23/50]         || Step: [520/1579]      || Average Training Loss: 2.3710\n",
      "Epoch: [23/50]         || Step: [540/1579]      || Average Training Loss: 2.3719\n",
      "Epoch: [23/50]         || Step: [560/1579]      || Average Training Loss: 2.3721\n",
      "Epoch: [23/50]         || Step: [580/1579]      || Average Training Loss: 2.3729\n",
      "Epoch: [23/50]         || Step: [600/1579]      || Average Training Loss: 2.3727\n",
      "Epoch: [23/50]         || Step: [620/1579]      || Average Training Loss: 2.3711\n",
      "Epoch: [23/50]         || Step: [640/1579]      || Average Training Loss: 2.3719\n",
      "Epoch: [23/50]         || Step: [660/1579]      || Average Training Loss: 2.3719\n",
      "Epoch: [23/50]         || Step: [680/1579]      || Average Training Loss: 2.3713\n",
      "Epoch: [23/50]         || Step: [700/1579]      || Average Training Loss: 2.3697\n",
      "Epoch: [23/50]         || Step: [720/1579]      || Average Training Loss: 2.3692\n",
      "Epoch: [23/50]         || Step: [740/1579]      || Average Training Loss: 2.3692\n",
      "Epoch: [23/50]         || Step: [760/1579]      || Average Training Loss: 2.3702\n",
      "Epoch: [23/50]         || Step: [780/1579]      || Average Training Loss: 2.3697\n",
      "Epoch: [23/50]         || Step: [800/1579]      || Average Training Loss: 2.3698\n",
      "Epoch: [23/50]         || Step: [820/1579]      || Average Training Loss: 2.3699\n",
      "Epoch: [23/50]         || Step: [840/1579]      || Average Training Loss: 2.3703\n",
      "Epoch: [23/50]         || Step: [860/1579]      || Average Training Loss: 2.3702\n",
      "Epoch: [23/50]         || Step: [880/1579]      || Average Training Loss: 2.3704\n",
      "Epoch: [23/50]         || Step: [900/1579]      || Average Training Loss: 2.3699\n",
      "Epoch: [23/50]         || Step: [920/1579]      || Average Training Loss: 2.3704\n",
      "Epoch: [23/50]         || Step: [940/1579]      || Average Training Loss: 2.3706\n",
      "Epoch: [23/50]         || Step: [960/1579]      || Average Training Loss: 2.3702\n",
      "Epoch: [23/50]         || Step: [980/1579]      || Average Training Loss: 2.3697\n",
      "Epoch: [23/50]         || Step: [1000/1579]     || Average Training Loss: 2.3694\n",
      "Epoch: [23/50]         || Step: [1020/1579]     || Average Training Loss: 2.3695\n",
      "Epoch: [23/50]         || Step: [1040/1579]     || Average Training Loss: 2.3687\n",
      "Epoch: [23/50]         || Step: [1060/1579]     || Average Training Loss: 2.3695\n",
      "Epoch: [23/50]         || Step: [1080/1579]     || Average Training Loss: 2.3696\n",
      "Epoch: [23/50]         || Step: [1100/1579]     || Average Training Loss: 2.3695\n",
      "Epoch: [23/50]         || Step: [1120/1579]     || Average Training Loss: 2.3692\n",
      "Epoch: [23/50]         || Step: [1140/1579]     || Average Training Loss: 2.3691\n",
      "Epoch: [23/50]         || Step: [1160/1579]     || Average Training Loss: 2.3690\n",
      "Epoch: [23/50]         || Step: [1180/1579]     || Average Training Loss: 2.3690\n",
      "Epoch: [23/50]         || Step: [1200/1579]     || Average Training Loss: 2.3688\n",
      "Epoch: [23/50]         || Step: [1220/1579]     || Average Training Loss: 2.3691\n",
      "Epoch: [23/50]         || Step: [1240/1579]     || Average Training Loss: 2.3693\n",
      "Epoch: [23/50]         || Step: [1260/1579]     || Average Training Loss: 2.3694\n",
      "Epoch: [23/50]         || Step: [1280/1579]     || Average Training Loss: 2.3679\n",
      "Epoch: [23/50]         || Step: [1300/1579]     || Average Training Loss: 2.3675\n",
      "Epoch: [23/50]         || Step: [1320/1579]     || Average Training Loss: 2.3678\n",
      "Epoch: [23/50]         || Step: [1340/1579]     || Average Training Loss: 2.3681\n",
      "Epoch: [23/50]         || Step: [1360/1579]     || Average Training Loss: 2.3684\n",
      "Epoch: [23/50]         || Step: [1380/1579]     || Average Training Loss: 2.3682\n",
      "Epoch: [23/50]         || Step: [1400/1579]     || Average Training Loss: 2.3684\n",
      "Epoch: [23/50]         || Step: [1420/1579]     || Average Training Loss: 2.3682\n",
      "Epoch: [23/50]         || Step: [1440/1579]     || Average Training Loss: 2.3683\n",
      "Epoch: [23/50]         || Step: [1460/1579]     || Average Training Loss: 2.3689\n",
      "Epoch: [23/50]         || Step: [1480/1579]     || Average Training Loss: 2.3687\n",
      "Epoch: [23/50]         || Step: [1500/1579]     || Average Training Loss: 2.3680\n",
      "Epoch: [23/50]         || Step: [1520/1579]     || Average Training Loss: 2.3681\n",
      "Epoch: [23/50]         || Step: [1540/1579]     || Average Training Loss: 2.3678\n",
      "Epoch: [23/50]         || Step: [1560/1579]     || Average Training Loss: 2.3683\n",
      "Epoch: [23/50]         || Step: [0/142]         || Average Validation Loss: 2.3469\n",
      "Epoch: [23/50]         || Step: [20/142]        || Average Validation Loss: 2.3679\n",
      "Epoch: [23/50]         || Step: [40/142]        || Average Validation Loss: 2.3624\n",
      "Epoch: [23/50]         || Step: [60/142]        || Average Validation Loss: 2.3673\n",
      "Epoch: [23/50]         || Step: [80/142]        || Average Validation Loss: 2.3745\n",
      "Epoch: [23/50]         || Step: [100/142]       || Average Validation Loss: 2.3672\n",
      "Epoch: [23/50]         || Step: [120/142]       || Average Validation Loss: 2.3600\n",
      "Epoch: [23/50]         || Step: [140/142]       || Average Validation Loss: 2.3617\n",
      "****************************************************************************************************\n",
      "Epoch: [23/50] || Training Loss = 2.37 || Validation Loss: 2.36 || Time: 26.982546\n",
      "****************************************************************************************************\n",
      "Epoch: [24/50]         || Step: [0/1579]        || Average Training Loss: 2.5313\n",
      "Epoch: [24/50]         || Step: [20/1579]       || Average Training Loss: 2.3933\n",
      "Epoch: [24/50]         || Step: [40/1579]       || Average Training Loss: 2.3754\n",
      "Epoch: [24/50]         || Step: [60/1579]       || Average Training Loss: 2.3532\n",
      "Epoch: [24/50]         || Step: [80/1579]       || Average Training Loss: 2.3566\n",
      "Epoch: [24/50]         || Step: [100/1579]      || Average Training Loss: 2.3494\n",
      "Epoch: [24/50]         || Step: [120/1579]      || Average Training Loss: 2.3559\n",
      "Epoch: [24/50]         || Step: [140/1579]      || Average Training Loss: 2.3551\n",
      "Epoch: [24/50]         || Step: [160/1579]      || Average Training Loss: 2.3589\n",
      "Epoch: [24/50]         || Step: [180/1579]      || Average Training Loss: 2.3599\n",
      "Epoch: [24/50]         || Step: [200/1579]      || Average Training Loss: 2.3575\n",
      "Epoch: [24/50]         || Step: [220/1579]      || Average Training Loss: 2.3580\n",
      "Epoch: [24/50]         || Step: [240/1579]      || Average Training Loss: 2.3611\n",
      "Epoch: [24/50]         || Step: [260/1579]      || Average Training Loss: 2.3602\n",
      "Epoch: [24/50]         || Step: [280/1579]      || Average Training Loss: 2.3631\n",
      "Epoch: [24/50]         || Step: [300/1579]      || Average Training Loss: 2.3608\n",
      "Epoch: [24/50]         || Step: [320/1579]      || Average Training Loss: 2.3593\n",
      "Epoch: [24/50]         || Step: [340/1579]      || Average Training Loss: 2.3597\n",
      "Epoch: [24/50]         || Step: [360/1579]      || Average Training Loss: 2.3606\n",
      "Epoch: [24/50]         || Step: [380/1579]      || Average Training Loss: 2.3591\n",
      "Epoch: [24/50]         || Step: [400/1579]      || Average Training Loss: 2.3622\n",
      "Epoch: [24/50]         || Step: [420/1579]      || Average Training Loss: 2.3603\n",
      "Epoch: [24/50]         || Step: [440/1579]      || Average Training Loss: 2.3610\n",
      "Epoch: [24/50]         || Step: [460/1579]      || Average Training Loss: 2.3625\n",
      "Epoch: [24/50]         || Step: [480/1579]      || Average Training Loss: 2.3636\n",
      "Epoch: [24/50]         || Step: [500/1579]      || Average Training Loss: 2.3657\n",
      "Epoch: [24/50]         || Step: [520/1579]      || Average Training Loss: 2.3658\n",
      "Epoch: [24/50]         || Step: [540/1579]      || Average Training Loss: 2.3668\n",
      "Epoch: [24/50]         || Step: [560/1579]      || Average Training Loss: 2.3681\n",
      "Epoch: [24/50]         || Step: [580/1579]      || Average Training Loss: 2.3671\n",
      "Epoch: [24/50]         || Step: [600/1579]      || Average Training Loss: 2.3665\n",
      "Epoch: [24/50]         || Step: [620/1579]      || Average Training Loss: 2.3673\n",
      "Epoch: [24/50]         || Step: [640/1579]      || Average Training Loss: 2.3664\n",
      "Epoch: [24/50]         || Step: [660/1579]      || Average Training Loss: 2.3666\n",
      "Epoch: [24/50]         || Step: [680/1579]      || Average Training Loss: 2.3660\n",
      "Epoch: [24/50]         || Step: [700/1579]      || Average Training Loss: 2.3666\n",
      "Epoch: [24/50]         || Step: [720/1579]      || Average Training Loss: 2.3667\n",
      "Epoch: [24/50]         || Step: [740/1579]      || Average Training Loss: 2.3661\n",
      "Epoch: [24/50]         || Step: [760/1579]      || Average Training Loss: 2.3663\n",
      "Epoch: [24/50]         || Step: [780/1579]      || Average Training Loss: 2.3654\n",
      "Epoch: [24/50]         || Step: [800/1579]      || Average Training Loss: 2.3645\n",
      "Epoch: [24/50]         || Step: [820/1579]      || Average Training Loss: 2.3640\n",
      "Epoch: [24/50]         || Step: [840/1579]      || Average Training Loss: 2.3645\n",
      "Epoch: [24/50]         || Step: [860/1579]      || Average Training Loss: 2.3646\n",
      "Epoch: [24/50]         || Step: [880/1579]      || Average Training Loss: 2.3642\n",
      "Epoch: [24/50]         || Step: [900/1579]      || Average Training Loss: 2.3636\n",
      "Epoch: [24/50]         || Step: [920/1579]      || Average Training Loss: 2.3634\n",
      "Epoch: [24/50]         || Step: [940/1579]      || Average Training Loss: 2.3629\n",
      "Epoch: [24/50]         || Step: [960/1579]      || Average Training Loss: 2.3645\n",
      "Epoch: [24/50]         || Step: [980/1579]      || Average Training Loss: 2.3650\n",
      "Epoch: [24/50]         || Step: [1000/1579]     || Average Training Loss: 2.3650\n",
      "Epoch: [24/50]         || Step: [1020/1579]     || Average Training Loss: 2.3654\n",
      "Epoch: [24/50]         || Step: [1040/1579]     || Average Training Loss: 2.3654\n",
      "Epoch: [24/50]         || Step: [1060/1579]     || Average Training Loss: 2.3660\n",
      "Epoch: [24/50]         || Step: [1080/1579]     || Average Training Loss: 2.3654\n",
      "Epoch: [24/50]         || Step: [1100/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [24/50]         || Step: [1120/1579]     || Average Training Loss: 2.3651\n",
      "Epoch: [24/50]         || Step: [1140/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [24/50]         || Step: [1160/1579]     || Average Training Loss: 2.3647\n",
      "Epoch: [24/50]         || Step: [1180/1579]     || Average Training Loss: 2.3652\n",
      "Epoch: [24/50]         || Step: [1200/1579]     || Average Training Loss: 2.3658\n",
      "Epoch: [24/50]         || Step: [1220/1579]     || Average Training Loss: 2.3665\n",
      "Epoch: [24/50]         || Step: [1240/1579]     || Average Training Loss: 2.3662\n",
      "Epoch: [24/50]         || Step: [1260/1579]     || Average Training Loss: 2.3659\n",
      "Epoch: [24/50]         || Step: [1280/1579]     || Average Training Loss: 2.3660\n",
      "Epoch: [24/50]         || Step: [1300/1579]     || Average Training Loss: 2.3657\n",
      "Epoch: [24/50]         || Step: [1320/1579]     || Average Training Loss: 2.3652\n",
      "Epoch: [24/50]         || Step: [1340/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [24/50]         || Step: [1360/1579]     || Average Training Loss: 2.3651\n",
      "Epoch: [24/50]         || Step: [1380/1579]     || Average Training Loss: 2.3652\n",
      "Epoch: [24/50]         || Step: [1400/1579]     || Average Training Loss: 2.3651\n",
      "Epoch: [24/50]         || Step: [1420/1579]     || Average Training Loss: 2.3656\n",
      "Epoch: [24/50]         || Step: [1440/1579]     || Average Training Loss: 2.3654\n",
      "Epoch: [24/50]         || Step: [1460/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [24/50]         || Step: [1480/1579]     || Average Training Loss: 2.3649\n",
      "Epoch: [24/50]         || Step: [1500/1579]     || Average Training Loss: 2.3646\n",
      "Epoch: [24/50]         || Step: [1520/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [24/50]         || Step: [1540/1579]     || Average Training Loss: 2.3655\n",
      "Epoch: [24/50]         || Step: [1560/1579]     || Average Training Loss: 2.3663\n",
      "Epoch: [24/50]         || Step: [0/142]         || Average Validation Loss: 2.1326\n",
      "Epoch: [24/50]         || Step: [20/142]        || Average Validation Loss: 2.2846\n",
      "Epoch: [24/50]         || Step: [40/142]        || Average Validation Loss: 2.3138\n",
      "Epoch: [24/50]         || Step: [60/142]        || Average Validation Loss: 2.3205\n",
      "Epoch: [24/50]         || Step: [80/142]        || Average Validation Loss: 2.3248\n",
      "Epoch: [24/50]         || Step: [100/142]       || Average Validation Loss: 2.3266\n",
      "Epoch: [24/50]         || Step: [120/142]       || Average Validation Loss: 2.3374\n",
      "Epoch: [24/50]         || Step: [140/142]       || Average Validation Loss: 2.3454\n",
      "****************************************************************************************************\n",
      "Epoch: [24/50] || Training Loss = 2.37 || Validation Loss: 2.35 || Time: 28.641143\n",
      "****************************************************************************************************\n",
      "Epoch: [25/50]         || Step: [0/1579]        || Average Training Loss: 2.3183\n",
      "Epoch: [25/50]         || Step: [20/1579]       || Average Training Loss: 2.3823\n",
      "Epoch: [25/50]         || Step: [40/1579]       || Average Training Loss: 2.3906\n",
      "Epoch: [25/50]         || Step: [60/1579]       || Average Training Loss: 2.3816\n",
      "Epoch: [25/50]         || Step: [80/1579]       || Average Training Loss: 2.3765\n",
      "Epoch: [25/50]         || Step: [100/1579]      || Average Training Loss: 2.3782\n",
      "Epoch: [25/50]         || Step: [120/1579]      || Average Training Loss: 2.3742\n",
      "Epoch: [25/50]         || Step: [140/1579]      || Average Training Loss: 2.3684\n",
      "Epoch: [25/50]         || Step: [160/1579]      || Average Training Loss: 2.3674\n",
      "Epoch: [25/50]         || Step: [180/1579]      || Average Training Loss: 2.3716\n",
      "Epoch: [25/50]         || Step: [200/1579]      || Average Training Loss: 2.3723\n",
      "Epoch: [25/50]         || Step: [220/1579]      || Average Training Loss: 2.3733\n",
      "Epoch: [25/50]         || Step: [240/1579]      || Average Training Loss: 2.3730\n",
      "Epoch: [25/50]         || Step: [260/1579]      || Average Training Loss: 2.3698\n",
      "Epoch: [25/50]         || Step: [280/1579]      || Average Training Loss: 2.3703\n",
      "Epoch: [25/50]         || Step: [300/1579]      || Average Training Loss: 2.3721\n",
      "Epoch: [25/50]         || Step: [320/1579]      || Average Training Loss: 2.3692\n",
      "Epoch: [25/50]         || Step: [340/1579]      || Average Training Loss: 2.3726\n",
      "Epoch: [25/50]         || Step: [360/1579]      || Average Training Loss: 2.3728\n",
      "Epoch: [25/50]         || Step: [380/1579]      || Average Training Loss: 2.3725\n",
      "Epoch: [25/50]         || Step: [400/1579]      || Average Training Loss: 2.3704\n",
      "Epoch: [25/50]         || Step: [420/1579]      || Average Training Loss: 2.3714\n",
      "Epoch: [25/50]         || Step: [440/1579]      || Average Training Loss: 2.3693\n",
      "Epoch: [25/50]         || Step: [460/1579]      || Average Training Loss: 2.3712\n",
      "Epoch: [25/50]         || Step: [480/1579]      || Average Training Loss: 2.3729\n",
      "Epoch: [25/50]         || Step: [500/1579]      || Average Training Loss: 2.3715\n",
      "Epoch: [25/50]         || Step: [520/1579]      || Average Training Loss: 2.3700\n",
      "Epoch: [25/50]         || Step: [540/1579]      || Average Training Loss: 2.3701\n",
      "Epoch: [25/50]         || Step: [560/1579]      || Average Training Loss: 2.3684\n",
      "Epoch: [25/50]         || Step: [580/1579]      || Average Training Loss: 2.3689\n",
      "Epoch: [25/50]         || Step: [600/1579]      || Average Training Loss: 2.3678\n",
      "Epoch: [25/50]         || Step: [620/1579]      || Average Training Loss: 2.3660\n",
      "Epoch: [25/50]         || Step: [640/1579]      || Average Training Loss: 2.3667\n",
      "Epoch: [25/50]         || Step: [660/1579]      || Average Training Loss: 2.3676\n",
      "Epoch: [25/50]         || Step: [680/1579]      || Average Training Loss: 2.3679\n",
      "Epoch: [25/50]         || Step: [700/1579]      || Average Training Loss: 2.3677\n",
      "Epoch: [25/50]         || Step: [720/1579]      || Average Training Loss: 2.3674\n",
      "Epoch: [25/50]         || Step: [740/1579]      || Average Training Loss: 2.3680\n",
      "Epoch: [25/50]         || Step: [760/1579]      || Average Training Loss: 2.3678\n",
      "Epoch: [25/50]         || Step: [780/1579]      || Average Training Loss: 2.3682\n",
      "Epoch: [25/50]         || Step: [800/1579]      || Average Training Loss: 2.3689\n",
      "Epoch: [25/50]         || Step: [820/1579]      || Average Training Loss: 2.3694\n",
      "Epoch: [25/50]         || Step: [840/1579]      || Average Training Loss: 2.3693\n",
      "Epoch: [25/50]         || Step: [860/1579]      || Average Training Loss: 2.3699\n",
      "Epoch: [25/50]         || Step: [880/1579]      || Average Training Loss: 2.3698\n",
      "Epoch: [25/50]         || Step: [900/1579]      || Average Training Loss: 2.3694\n",
      "Epoch: [25/50]         || Step: [920/1579]      || Average Training Loss: 2.3701\n",
      "Epoch: [25/50]         || Step: [940/1579]      || Average Training Loss: 2.3703\n",
      "Epoch: [25/50]         || Step: [960/1579]      || Average Training Loss: 2.3694\n",
      "Epoch: [25/50]         || Step: [980/1579]      || Average Training Loss: 2.3691\n",
      "Epoch: [25/50]         || Step: [1000/1579]     || Average Training Loss: 2.3685\n",
      "Epoch: [25/50]         || Step: [1020/1579]     || Average Training Loss: 2.3682\n",
      "Epoch: [25/50]         || Step: [1040/1579]     || Average Training Loss: 2.3669\n",
      "Epoch: [25/50]         || Step: [1060/1579]     || Average Training Loss: 2.3662\n",
      "Epoch: [25/50]         || Step: [1080/1579]     || Average Training Loss: 2.3659\n",
      "Epoch: [25/50]         || Step: [1100/1579]     || Average Training Loss: 2.3658\n",
      "Epoch: [25/50]         || Step: [1120/1579]     || Average Training Loss: 2.3657\n",
      "Epoch: [25/50]         || Step: [1140/1579]     || Average Training Loss: 2.3661\n",
      "Epoch: [25/50]         || Step: [1160/1579]     || Average Training Loss: 2.3658\n",
      "Epoch: [25/50]         || Step: [1180/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [25/50]         || Step: [1200/1579]     || Average Training Loss: 2.3654\n",
      "Epoch: [25/50]         || Step: [1220/1579]     || Average Training Loss: 2.3657\n",
      "Epoch: [25/50]         || Step: [1240/1579]     || Average Training Loss: 2.3659\n",
      "Epoch: [25/50]         || Step: [1260/1579]     || Average Training Loss: 2.3656\n",
      "Epoch: [25/50]         || Step: [1280/1579]     || Average Training Loss: 2.3658\n",
      "Epoch: [25/50]         || Step: [1300/1579]     || Average Training Loss: 2.3663\n",
      "Epoch: [25/50]         || Step: [1320/1579]     || Average Training Loss: 2.3662\n",
      "Epoch: [25/50]         || Step: [1340/1579]     || Average Training Loss: 2.3651\n",
      "Epoch: [25/50]         || Step: [1360/1579]     || Average Training Loss: 2.3650\n",
      "Epoch: [25/50]         || Step: [1380/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [25/50]         || Step: [1400/1579]     || Average Training Loss: 2.3650\n",
      "Epoch: [25/50]         || Step: [1420/1579]     || Average Training Loss: 2.3646\n",
      "Epoch: [25/50]         || Step: [1440/1579]     || Average Training Loss: 2.3649\n",
      "Epoch: [25/50]         || Step: [1460/1579]     || Average Training Loss: 2.3651\n",
      "Epoch: [25/50]         || Step: [1480/1579]     || Average Training Loss: 2.3652\n",
      "Epoch: [25/50]         || Step: [1500/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [25/50]         || Step: [1520/1579]     || Average Training Loss: 2.3654\n",
      "Epoch: [25/50]         || Step: [1540/1579]     || Average Training Loss: 2.3657\n",
      "Epoch: [25/50]         || Step: [1560/1579]     || Average Training Loss: 2.3656\n",
      "Epoch: [25/50]         || Step: [0/142]         || Average Validation Loss: 2.4196\n",
      "Epoch: [25/50]         || Step: [20/142]        || Average Validation Loss: 2.4362\n",
      "Epoch: [25/50]         || Step: [40/142]        || Average Validation Loss: 2.4685\n",
      "Epoch: [25/50]         || Step: [60/142]        || Average Validation Loss: 2.4772\n",
      "Epoch: [25/50]         || Step: [80/142]        || Average Validation Loss: 2.4643\n",
      "Epoch: [25/50]         || Step: [100/142]       || Average Validation Loss: 2.4742\n",
      "Epoch: [25/50]         || Step: [120/142]       || Average Validation Loss: 2.4782\n",
      "Epoch: [25/50]         || Step: [140/142]       || Average Validation Loss: 2.4735\n",
      "****************************************************************************************************\n",
      "Epoch: [25/50] || Training Loss = 2.37 || Validation Loss: 2.47 || Time: 29.658410\n",
      "****************************************************************************************************\n",
      "Epoch: [26/50]         || Step: [0/1579]        || Average Training Loss: 2.5800\n",
      "Epoch: [26/50]         || Step: [20/1579]       || Average Training Loss: 2.4109\n",
      "Epoch: [26/50]         || Step: [40/1579]       || Average Training Loss: 2.3843\n",
      "Epoch: [26/50]         || Step: [60/1579]       || Average Training Loss: 2.3662\n",
      "Epoch: [26/50]         || Step: [80/1579]       || Average Training Loss: 2.3602\n",
      "Epoch: [26/50]         || Step: [100/1579]      || Average Training Loss: 2.3633\n",
      "Epoch: [26/50]         || Step: [120/1579]      || Average Training Loss: 2.3574\n",
      "Epoch: [26/50]         || Step: [140/1579]      || Average Training Loss: 2.3541\n",
      "Epoch: [26/50]         || Step: [160/1579]      || Average Training Loss: 2.3535\n",
      "Epoch: [26/50]         || Step: [180/1579]      || Average Training Loss: 2.3576\n",
      "Epoch: [26/50]         || Step: [200/1579]      || Average Training Loss: 2.3586\n",
      "Epoch: [26/50]         || Step: [220/1579]      || Average Training Loss: 2.3632\n",
      "Epoch: [26/50]         || Step: [240/1579]      || Average Training Loss: 2.3628\n",
      "Epoch: [26/50]         || Step: [260/1579]      || Average Training Loss: 2.3613\n",
      "Epoch: [26/50]         || Step: [280/1579]      || Average Training Loss: 2.3647\n",
      "Epoch: [26/50]         || Step: [300/1579]      || Average Training Loss: 2.3668\n",
      "Epoch: [26/50]         || Step: [320/1579]      || Average Training Loss: 2.3668\n",
      "Epoch: [26/50]         || Step: [340/1579]      || Average Training Loss: 2.3683\n",
      "Epoch: [26/50]         || Step: [360/1579]      || Average Training Loss: 2.3682\n",
      "Epoch: [26/50]         || Step: [380/1579]      || Average Training Loss: 2.3681\n",
      "Epoch: [26/50]         || Step: [400/1579]      || Average Training Loss: 2.3672\n",
      "Epoch: [26/50]         || Step: [420/1579]      || Average Training Loss: 2.3680\n",
      "Epoch: [26/50]         || Step: [440/1579]      || Average Training Loss: 2.3688\n",
      "Epoch: [26/50]         || Step: [460/1579]      || Average Training Loss: 2.3689\n",
      "Epoch: [26/50]         || Step: [480/1579]      || Average Training Loss: 2.3706\n",
      "Epoch: [26/50]         || Step: [500/1579]      || Average Training Loss: 2.3676\n",
      "Epoch: [26/50]         || Step: [520/1579]      || Average Training Loss: 2.3669\n",
      "Epoch: [26/50]         || Step: [540/1579]      || Average Training Loss: 2.3678\n",
      "Epoch: [26/50]         || Step: [560/1579]      || Average Training Loss: 2.3667\n",
      "Epoch: [26/50]         || Step: [580/1579]      || Average Training Loss: 2.3648\n",
      "Epoch: [26/50]         || Step: [600/1579]      || Average Training Loss: 2.3642\n",
      "Epoch: [26/50]         || Step: [620/1579]      || Average Training Loss: 2.3637\n",
      "Epoch: [26/50]         || Step: [640/1579]      || Average Training Loss: 2.3644\n",
      "Epoch: [26/50]         || Step: [660/1579]      || Average Training Loss: 2.3642\n",
      "Epoch: [26/50]         || Step: [680/1579]      || Average Training Loss: 2.3650\n",
      "Epoch: [26/50]         || Step: [700/1579]      || Average Training Loss: 2.3646\n",
      "Epoch: [26/50]         || Step: [720/1579]      || Average Training Loss: 2.3644\n",
      "Epoch: [26/50]         || Step: [740/1579]      || Average Training Loss: 2.3634\n",
      "Epoch: [26/50]         || Step: [760/1579]      || Average Training Loss: 2.3633\n",
      "Epoch: [26/50]         || Step: [780/1579]      || Average Training Loss: 2.3629\n",
      "Epoch: [26/50]         || Step: [800/1579]      || Average Training Loss: 2.3644\n",
      "Epoch: [26/50]         || Step: [820/1579]      || Average Training Loss: 2.3637\n",
      "Epoch: [26/50]         || Step: [840/1579]      || Average Training Loss: 2.3625\n",
      "Epoch: [26/50]         || Step: [860/1579]      || Average Training Loss: 2.3625\n",
      "Epoch: [26/50]         || Step: [880/1579]      || Average Training Loss: 2.3626\n",
      "Epoch: [26/50]         || Step: [900/1579]      || Average Training Loss: 2.3620\n",
      "Epoch: [26/50]         || Step: [920/1579]      || Average Training Loss: 2.3617\n",
      "Epoch: [26/50]         || Step: [940/1579]      || Average Training Loss: 2.3610\n",
      "Epoch: [26/50]         || Step: [960/1579]      || Average Training Loss: 2.3608\n",
      "Epoch: [26/50]         || Step: [980/1579]      || Average Training Loss: 2.3611\n",
      "Epoch: [26/50]         || Step: [1000/1579]     || Average Training Loss: 2.3612\n",
      "Epoch: [26/50]         || Step: [1020/1579]     || Average Training Loss: 2.3621\n",
      "Epoch: [26/50]         || Step: [1040/1579]     || Average Training Loss: 2.3632\n",
      "Epoch: [26/50]         || Step: [1060/1579]     || Average Training Loss: 2.3637\n",
      "Epoch: [26/50]         || Step: [1080/1579]     || Average Training Loss: 2.3639\n",
      "Epoch: [26/50]         || Step: [1100/1579]     || Average Training Loss: 2.3640\n",
      "Epoch: [26/50]         || Step: [1120/1579]     || Average Training Loss: 2.3646\n",
      "Epoch: [26/50]         || Step: [1140/1579]     || Average Training Loss: 2.3641\n",
      "Epoch: [26/50]         || Step: [1160/1579]     || Average Training Loss: 2.3636\n",
      "Epoch: [26/50]         || Step: [1180/1579]     || Average Training Loss: 2.3636\n",
      "Epoch: [26/50]         || Step: [1200/1579]     || Average Training Loss: 2.3637\n",
      "Epoch: [26/50]         || Step: [1220/1579]     || Average Training Loss: 2.3639\n",
      "Epoch: [26/50]         || Step: [1240/1579]     || Average Training Loss: 2.3643\n",
      "Epoch: [26/50]         || Step: [1260/1579]     || Average Training Loss: 2.3652\n",
      "Epoch: [26/50]         || Step: [1280/1579]     || Average Training Loss: 2.3651\n",
      "Epoch: [26/50]         || Step: [1300/1579]     || Average Training Loss: 2.3645\n",
      "Epoch: [26/50]         || Step: [1320/1579]     || Average Training Loss: 2.3650\n",
      "Epoch: [26/50]         || Step: [1340/1579]     || Average Training Loss: 2.3648\n",
      "Epoch: [26/50]         || Step: [1360/1579]     || Average Training Loss: 2.3647\n",
      "Epoch: [26/50]         || Step: [1380/1579]     || Average Training Loss: 2.3646\n",
      "Epoch: [26/50]         || Step: [1400/1579]     || Average Training Loss: 2.3645\n",
      "Epoch: [26/50]         || Step: [1420/1579]     || Average Training Loss: 2.3642\n",
      "Epoch: [26/50]         || Step: [1440/1579]     || Average Training Loss: 2.3641\n",
      "Epoch: [26/50]         || Step: [1460/1579]     || Average Training Loss: 2.3643\n",
      "Epoch: [26/50]         || Step: [1480/1579]     || Average Training Loss: 2.3645\n",
      "Epoch: [26/50]         || Step: [1500/1579]     || Average Training Loss: 2.3642\n",
      "Epoch: [26/50]         || Step: [1520/1579]     || Average Training Loss: 2.3639\n",
      "Epoch: [26/50]         || Step: [1540/1579]     || Average Training Loss: 2.3641\n",
      "Epoch: [26/50]         || Step: [1560/1579]     || Average Training Loss: 2.3644\n",
      "Epoch: [26/50]         || Step: [0/142]         || Average Validation Loss: 2.4672\n",
      "Epoch: [26/50]         || Step: [20/142]        || Average Validation Loss: 2.4885\n",
      "Epoch: [26/50]         || Step: [40/142]        || Average Validation Loss: 2.4699\n",
      "Epoch: [26/50]         || Step: [60/142]        || Average Validation Loss: 2.4745\n",
      "Epoch: [26/50]         || Step: [80/142]        || Average Validation Loss: 2.4617\n",
      "Epoch: [26/50]         || Step: [100/142]       || Average Validation Loss: 2.4564\n",
      "Epoch: [26/50]         || Step: [120/142]       || Average Validation Loss: 2.4563\n",
      "Epoch: [26/50]         || Step: [140/142]       || Average Validation Loss: 2.4624\n",
      "****************************************************************************************************\n",
      "Epoch: [26/50] || Training Loss = 2.37 || Validation Loss: 2.46 || Time: 30.517135\n",
      "****************************************************************************************************\n",
      "Epoch: [27/50]         || Step: [0/1579]        || Average Training Loss: 2.7070\n",
      "Epoch: [27/50]         || Step: [20/1579]       || Average Training Loss: 2.4563\n",
      "Epoch: [27/50]         || Step: [40/1579]       || Average Training Loss: 2.3966\n",
      "Epoch: [27/50]         || Step: [60/1579]       || Average Training Loss: 2.3888\n",
      "Epoch: [27/50]         || Step: [80/1579]       || Average Training Loss: 2.3909\n",
      "Epoch: [27/50]         || Step: [100/1579]      || Average Training Loss: 2.3800\n",
      "Epoch: [27/50]         || Step: [120/1579]      || Average Training Loss: 2.3814\n",
      "Epoch: [27/50]         || Step: [140/1579]      || Average Training Loss: 2.3806\n",
      "Epoch: [27/50]         || Step: [160/1579]      || Average Training Loss: 2.3804\n",
      "Epoch: [27/50]         || Step: [180/1579]      || Average Training Loss: 2.3791\n",
      "Epoch: [27/50]         || Step: [200/1579]      || Average Training Loss: 2.3749\n",
      "Epoch: [27/50]         || Step: [220/1579]      || Average Training Loss: 2.3751\n",
      "Epoch: [27/50]         || Step: [240/1579]      || Average Training Loss: 2.3777\n",
      "Epoch: [27/50]         || Step: [260/1579]      || Average Training Loss: 2.3773\n",
      "Epoch: [27/50]         || Step: [280/1579]      || Average Training Loss: 2.3762\n",
      "Epoch: [27/50]         || Step: [300/1579]      || Average Training Loss: 2.3743\n",
      "Epoch: [27/50]         || Step: [320/1579]      || Average Training Loss: 2.3723\n",
      "Epoch: [27/50]         || Step: [340/1579]      || Average Training Loss: 2.3702\n",
      "Epoch: [27/50]         || Step: [360/1579]      || Average Training Loss: 2.3700\n",
      "Epoch: [27/50]         || Step: [380/1579]      || Average Training Loss: 2.3689\n",
      "Epoch: [27/50]         || Step: [400/1579]      || Average Training Loss: 2.3668\n",
      "Epoch: [27/50]         || Step: [420/1579]      || Average Training Loss: 2.3653\n",
      "Epoch: [27/50]         || Step: [440/1579]      || Average Training Loss: 2.3655\n",
      "Epoch: [27/50]         || Step: [460/1579]      || Average Training Loss: 2.3640\n",
      "Epoch: [27/50]         || Step: [480/1579]      || Average Training Loss: 2.3651\n",
      "Epoch: [27/50]         || Step: [500/1579]      || Average Training Loss: 2.3649\n",
      "Epoch: [27/50]         || Step: [520/1579]      || Average Training Loss: 2.3656\n",
      "Epoch: [27/50]         || Step: [540/1579]      || Average Training Loss: 2.3653\n",
      "Epoch: [27/50]         || Step: [560/1579]      || Average Training Loss: 2.3649\n",
      "Epoch: [27/50]         || Step: [580/1579]      || Average Training Loss: 2.3644\n",
      "Epoch: [27/50]         || Step: [600/1579]      || Average Training Loss: 2.3644\n",
      "Epoch: [27/50]         || Step: [620/1579]      || Average Training Loss: 2.3650\n",
      "Epoch: [27/50]         || Step: [640/1579]      || Average Training Loss: 2.3659\n",
      "Epoch: [27/50]         || Step: [660/1579]      || Average Training Loss: 2.3660\n",
      "Epoch: [27/50]         || Step: [680/1579]      || Average Training Loss: 2.3654\n",
      "Epoch: [27/50]         || Step: [700/1579]      || Average Training Loss: 2.3666\n",
      "Epoch: [27/50]         || Step: [720/1579]      || Average Training Loss: 2.3660\n",
      "Epoch: [27/50]         || Step: [740/1579]      || Average Training Loss: 2.3649\n",
      "Epoch: [27/50]         || Step: [760/1579]      || Average Training Loss: 2.3647\n",
      "Epoch: [27/50]         || Step: [780/1579]      || Average Training Loss: 2.3644\n",
      "Epoch: [27/50]         || Step: [800/1579]      || Average Training Loss: 2.3645\n",
      "Epoch: [27/50]         || Step: [820/1579]      || Average Training Loss: 2.3642\n",
      "Epoch: [27/50]         || Step: [840/1579]      || Average Training Loss: 2.3635\n",
      "Epoch: [27/50]         || Step: [860/1579]      || Average Training Loss: 2.3630\n",
      "Epoch: [27/50]         || Step: [880/1579]      || Average Training Loss: 2.3634\n",
      "Epoch: [27/50]         || Step: [900/1579]      || Average Training Loss: 2.3634\n",
      "Epoch: [27/50]         || Step: [920/1579]      || Average Training Loss: 2.3643\n",
      "Epoch: [27/50]         || Step: [940/1579]      || Average Training Loss: 2.3637\n",
      "Epoch: [27/50]         || Step: [960/1579]      || Average Training Loss: 2.3650\n",
      "Epoch: [27/50]         || Step: [980/1579]      || Average Training Loss: 2.3646\n",
      "Epoch: [27/50]         || Step: [1000/1579]     || Average Training Loss: 2.3644\n",
      "Epoch: [27/50]         || Step: [1020/1579]     || Average Training Loss: 2.3651\n",
      "Epoch: [27/50]         || Step: [1040/1579]     || Average Training Loss: 2.3660\n",
      "Epoch: [27/50]         || Step: [1060/1579]     || Average Training Loss: 2.3661\n",
      "Epoch: [27/50]         || Step: [1080/1579]     || Average Training Loss: 2.3670\n",
      "Epoch: [27/50]         || Step: [1100/1579]     || Average Training Loss: 2.3668\n",
      "Epoch: [27/50]         || Step: [1120/1579]     || Average Training Loss: 2.3675\n",
      "Epoch: [27/50]         || Step: [1140/1579]     || Average Training Loss: 2.3674\n",
      "Epoch: [27/50]         || Step: [1160/1579]     || Average Training Loss: 2.3675\n",
      "Epoch: [27/50]         || Step: [1180/1579]     || Average Training Loss: 2.3671\n",
      "Epoch: [27/50]         || Step: [1200/1579]     || Average Training Loss: 2.3669\n",
      "Epoch: [27/50]         || Step: [1220/1579]     || Average Training Loss: 2.3666\n",
      "Epoch: [27/50]         || Step: [1240/1579]     || Average Training Loss: 2.3652\n",
      "Epoch: [27/50]         || Step: [1260/1579]     || Average Training Loss: 2.3645\n",
      "Epoch: [27/50]         || Step: [1280/1579]     || Average Training Loss: 2.3646\n",
      "Epoch: [27/50]         || Step: [1300/1579]     || Average Training Loss: 2.3651\n",
      "Epoch: [27/50]         || Step: [1320/1579]     || Average Training Loss: 2.3645\n",
      "Epoch: [27/50]         || Step: [1340/1579]     || Average Training Loss: 2.3646\n",
      "Epoch: [27/50]         || Step: [1360/1579]     || Average Training Loss: 2.3632\n",
      "Epoch: [27/50]         || Step: [1380/1579]     || Average Training Loss: 2.3633\n",
      "Epoch: [27/50]         || Step: [1400/1579]     || Average Training Loss: 2.3625\n",
      "Epoch: [27/50]         || Step: [1420/1579]     || Average Training Loss: 2.3630\n",
      "Epoch: [27/50]         || Step: [1440/1579]     || Average Training Loss: 2.3635\n",
      "Epoch: [27/50]         || Step: [1460/1579]     || Average Training Loss: 2.3636\n",
      "Epoch: [27/50]         || Step: [1480/1579]     || Average Training Loss: 2.3641\n",
      "Epoch: [27/50]         || Step: [1500/1579]     || Average Training Loss: 2.3636\n",
      "Epoch: [27/50]         || Step: [1520/1579]     || Average Training Loss: 2.3630\n",
      "Epoch: [27/50]         || Step: [1540/1579]     || Average Training Loss: 2.3633\n",
      "Epoch: [27/50]         || Step: [1560/1579]     || Average Training Loss: 2.3638\n",
      "Epoch: [27/50]         || Step: [0/142]         || Average Validation Loss: 2.3618\n",
      "Epoch: [27/50]         || Step: [20/142]        || Average Validation Loss: 2.3578\n",
      "Epoch: [27/50]         || Step: [40/142]        || Average Validation Loss: 2.3730\n",
      "Epoch: [27/50]         || Step: [60/142]        || Average Validation Loss: 2.3697\n",
      "Epoch: [27/50]         || Step: [80/142]        || Average Validation Loss: 2.3693\n",
      "Epoch: [27/50]         || Step: [100/142]       || Average Validation Loss: 2.3782\n",
      "Epoch: [27/50]         || Step: [120/142]       || Average Validation Loss: 2.3776\n",
      "Epoch: [27/50]         || Step: [140/142]       || Average Validation Loss: 2.3808\n",
      "****************************************************************************************************\n",
      "Epoch: [27/50] || Training Loss = 2.36 || Validation Loss: 2.38 || Time: 30.365110\n",
      "****************************************************************************************************\n",
      "Epoch: [28/50]         || Step: [0/1579]        || Average Training Loss: 2.4708\n",
      "Epoch: [28/50]         || Step: [20/1579]       || Average Training Loss: 2.4200\n",
      "Epoch: [28/50]         || Step: [40/1579]       || Average Training Loss: 2.3814\n",
      "Epoch: [28/50]         || Step: [60/1579]       || Average Training Loss: 2.3861\n",
      "Epoch: [28/50]         || Step: [80/1579]       || Average Training Loss: 2.3833\n",
      "Epoch: [28/50]         || Step: [100/1579]      || Average Training Loss: 2.3780\n",
      "Epoch: [28/50]         || Step: [120/1579]      || Average Training Loss: 2.3758\n",
      "Epoch: [28/50]         || Step: [140/1579]      || Average Training Loss: 2.3707\n",
      "Epoch: [28/50]         || Step: [160/1579]      || Average Training Loss: 2.3683\n",
      "Epoch: [28/50]         || Step: [180/1579]      || Average Training Loss: 2.3640\n",
      "Epoch: [28/50]         || Step: [200/1579]      || Average Training Loss: 2.3626\n",
      "Epoch: [28/50]         || Step: [220/1579]      || Average Training Loss: 2.3684\n",
      "Epoch: [28/50]         || Step: [240/1579]      || Average Training Loss: 2.3722\n",
      "Epoch: [28/50]         || Step: [260/1579]      || Average Training Loss: 2.3736\n",
      "Epoch: [28/50]         || Step: [280/1579]      || Average Training Loss: 2.3752\n",
      "Epoch: [28/50]         || Step: [300/1579]      || Average Training Loss: 2.3738\n",
      "Epoch: [28/50]         || Step: [320/1579]      || Average Training Loss: 2.3761\n",
      "Epoch: [28/50]         || Step: [340/1579]      || Average Training Loss: 2.3748\n",
      "Epoch: [28/50]         || Step: [360/1579]      || Average Training Loss: 2.3718\n",
      "Epoch: [28/50]         || Step: [380/1579]      || Average Training Loss: 2.3738\n",
      "Epoch: [28/50]         || Step: [400/1579]      || Average Training Loss: 2.3712\n",
      "Epoch: [28/50]         || Step: [420/1579]      || Average Training Loss: 2.3722\n",
      "Epoch: [28/50]         || Step: [440/1579]      || Average Training Loss: 2.3709\n",
      "Epoch: [28/50]         || Step: [460/1579]      || Average Training Loss: 2.3723\n",
      "Epoch: [28/50]         || Step: [480/1579]      || Average Training Loss: 2.3705\n",
      "Epoch: [28/50]         || Step: [500/1579]      || Average Training Loss: 2.3688\n",
      "Epoch: [28/50]         || Step: [520/1579]      || Average Training Loss: 2.3663\n",
      "Epoch: [28/50]         || Step: [540/1579]      || Average Training Loss: 2.3649\n",
      "Epoch: [28/50]         || Step: [560/1579]      || Average Training Loss: 2.3648\n",
      "Epoch: [28/50]         || Step: [580/1579]      || Average Training Loss: 2.3657\n",
      "Epoch: [28/50]         || Step: [600/1579]      || Average Training Loss: 2.3666\n",
      "Epoch: [28/50]         || Step: [620/1579]      || Average Training Loss: 2.3676\n",
      "Epoch: [28/50]         || Step: [640/1579]      || Average Training Loss: 2.3682\n",
      "Epoch: [28/50]         || Step: [660/1579]      || Average Training Loss: 2.3683\n",
      "Epoch: [28/50]         || Step: [680/1579]      || Average Training Loss: 2.3674\n",
      "Epoch: [28/50]         || Step: [700/1579]      || Average Training Loss: 2.3661\n",
      "Epoch: [28/50]         || Step: [720/1579]      || Average Training Loss: 2.3656\n",
      "Epoch: [28/50]         || Step: [740/1579]      || Average Training Loss: 2.3652\n",
      "Epoch: [28/50]         || Step: [760/1579]      || Average Training Loss: 2.3659\n",
      "Epoch: [28/50]         || Step: [780/1579]      || Average Training Loss: 2.3664\n",
      "Epoch: [28/50]         || Step: [800/1579]      || Average Training Loss: 2.3653\n",
      "Epoch: [28/50]         || Step: [820/1579]      || Average Training Loss: 2.3643\n",
      "Epoch: [28/50]         || Step: [840/1579]      || Average Training Loss: 2.3640\n",
      "Epoch: [28/50]         || Step: [860/1579]      || Average Training Loss: 2.3652\n",
      "Epoch: [28/50]         || Step: [880/1579]      || Average Training Loss: 2.3656\n",
      "Epoch: [28/50]         || Step: [900/1579]      || Average Training Loss: 2.3650\n",
      "Epoch: [28/50]         || Step: [920/1579]      || Average Training Loss: 2.3652\n",
      "Epoch: [28/50]         || Step: [940/1579]      || Average Training Loss: 2.3648\n",
      "Epoch: [28/50]         || Step: [960/1579]      || Average Training Loss: 2.3651\n",
      "Epoch: [28/50]         || Step: [980/1579]      || Average Training Loss: 2.3653\n",
      "Epoch: [28/50]         || Step: [1000/1579]     || Average Training Loss: 2.3648\n",
      "Epoch: [28/50]         || Step: [1020/1579]     || Average Training Loss: 2.3651\n",
      "Epoch: [28/50]         || Step: [1040/1579]     || Average Training Loss: 2.3649\n",
      "Epoch: [28/50]         || Step: [1060/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [28/50]         || Step: [1080/1579]     || Average Training Loss: 2.3647\n",
      "Epoch: [28/50]         || Step: [1100/1579]     || Average Training Loss: 2.3650\n",
      "Epoch: [28/50]         || Step: [1120/1579]     || Average Training Loss: 2.3653\n",
      "Epoch: [28/50]         || Step: [1140/1579]     || Average Training Loss: 2.3651\n",
      "Epoch: [28/50]         || Step: [1160/1579]     || Average Training Loss: 2.3644\n",
      "Epoch: [28/50]         || Step: [1180/1579]     || Average Training Loss: 2.3632\n",
      "Epoch: [28/50]         || Step: [1200/1579]     || Average Training Loss: 2.3638\n",
      "Epoch: [28/50]         || Step: [1220/1579]     || Average Training Loss: 2.3642\n",
      "Epoch: [28/50]         || Step: [1240/1579]     || Average Training Loss: 2.3645\n",
      "Epoch: [28/50]         || Step: [1260/1579]     || Average Training Loss: 2.3643\n",
      "Epoch: [28/50]         || Step: [1280/1579]     || Average Training Loss: 2.3643\n",
      "Epoch: [28/50]         || Step: [1300/1579]     || Average Training Loss: 2.3644\n",
      "Epoch: [28/50]         || Step: [1320/1579]     || Average Training Loss: 2.3634\n",
      "Epoch: [28/50]         || Step: [1340/1579]     || Average Training Loss: 2.3635\n",
      "Epoch: [28/50]         || Step: [1360/1579]     || Average Training Loss: 2.3636\n",
      "Epoch: [28/50]         || Step: [1380/1579]     || Average Training Loss: 2.3626\n",
      "Epoch: [28/50]         || Step: [1400/1579]     || Average Training Loss: 2.3629\n",
      "Epoch: [28/50]         || Step: [1420/1579]     || Average Training Loss: 2.3632\n",
      "Epoch: [28/50]         || Step: [1440/1579]     || Average Training Loss: 2.3625\n",
      "Epoch: [28/50]         || Step: [1460/1579]     || Average Training Loss: 2.3631\n",
      "Epoch: [28/50]         || Step: [1480/1579]     || Average Training Loss: 2.3635\n",
      "Epoch: [28/50]         || Step: [1500/1579]     || Average Training Loss: 2.3634\n",
      "Epoch: [28/50]         || Step: [1520/1579]     || Average Training Loss: 2.3627\n",
      "Epoch: [28/50]         || Step: [1540/1579]     || Average Training Loss: 2.3629\n",
      "Epoch: [28/50]         || Step: [1560/1579]     || Average Training Loss: 2.3637\n",
      "Epoch: [28/50]         || Step: [0/142]         || Average Validation Loss: 2.3617\n",
      "Epoch: [28/50]         || Step: [20/142]        || Average Validation Loss: 2.4656\n",
      "Epoch: [28/50]         || Step: [40/142]        || Average Validation Loss: 2.4679\n",
      "Epoch: [28/50]         || Step: [60/142]        || Average Validation Loss: 2.4868\n",
      "Epoch: [28/50]         || Step: [80/142]        || Average Validation Loss: 2.4895\n",
      "Epoch: [28/50]         || Step: [100/142]       || Average Validation Loss: 2.4986\n",
      "Epoch: [28/50]         || Step: [120/142]       || Average Validation Loss: 2.5042\n",
      "Epoch: [28/50]         || Step: [140/142]       || Average Validation Loss: 2.5015\n",
      "****************************************************************************************************\n",
      "Epoch: [28/50] || Training Loss = 2.36 || Validation Loss: 2.50 || Time: 30.643615\n",
      "****************************************************************************************************\n",
      "Epoch: [29/50]         || Step: [0/1579]        || Average Training Loss: 2.3942\n",
      "Epoch: [29/50]         || Step: [20/1579]       || Average Training Loss: 2.4606\n",
      "Epoch: [29/50]         || Step: [40/1579]       || Average Training Loss: 2.4135\n",
      "Epoch: [29/50]         || Step: [60/1579]       || Average Training Loss: 2.3953\n",
      "Epoch: [29/50]         || Step: [80/1579]       || Average Training Loss: 2.3871\n",
      "Epoch: [29/50]         || Step: [100/1579]      || Average Training Loss: 2.3777\n",
      "Epoch: [29/50]         || Step: [120/1579]      || Average Training Loss: 2.3748\n",
      "Epoch: [29/50]         || Step: [140/1579]      || Average Training Loss: 2.3769\n",
      "Epoch: [29/50]         || Step: [160/1579]      || Average Training Loss: 2.3762\n",
      "Epoch: [29/50]         || Step: [180/1579]      || Average Training Loss: 2.3741\n",
      "Epoch: [29/50]         || Step: [200/1579]      || Average Training Loss: 2.3766\n",
      "Epoch: [29/50]         || Step: [220/1579]      || Average Training Loss: 2.3763\n",
      "Epoch: [29/50]         || Step: [240/1579]      || Average Training Loss: 2.3721\n",
      "Epoch: [29/50]         || Step: [260/1579]      || Average Training Loss: 2.3705\n",
      "Epoch: [29/50]         || Step: [280/1579]      || Average Training Loss: 2.3729\n",
      "Epoch: [29/50]         || Step: [300/1579]      || Average Training Loss: 2.3705\n",
      "Epoch: [29/50]         || Step: [320/1579]      || Average Training Loss: 2.3711\n",
      "Epoch: [29/50]         || Step: [340/1579]      || Average Training Loss: 2.3672\n",
      "Epoch: [29/50]         || Step: [360/1579]      || Average Training Loss: 2.3676\n",
      "Epoch: [29/50]         || Step: [380/1579]      || Average Training Loss: 2.3688\n",
      "Epoch: [29/50]         || Step: [400/1579]      || Average Training Loss: 2.3681\n",
      "Epoch: [29/50]         || Step: [420/1579]      || Average Training Loss: 2.3714\n",
      "Epoch: [29/50]         || Step: [440/1579]      || Average Training Loss: 2.3717\n",
      "Epoch: [29/50]         || Step: [460/1579]      || Average Training Loss: 2.3728\n",
      "Epoch: [29/50]         || Step: [480/1579]      || Average Training Loss: 2.3732\n",
      "Epoch: [29/50]         || Step: [500/1579]      || Average Training Loss: 2.3743\n",
      "Epoch: [29/50]         || Step: [520/1579]      || Average Training Loss: 2.3738\n",
      "Epoch: [29/50]         || Step: [540/1579]      || Average Training Loss: 2.3740\n",
      "Epoch: [29/50]         || Step: [560/1579]      || Average Training Loss: 2.3738\n",
      "Epoch: [29/50]         || Step: [580/1579]      || Average Training Loss: 2.3729\n",
      "Epoch: [29/50]         || Step: [600/1579]      || Average Training Loss: 2.3729\n",
      "Epoch: [29/50]         || Step: [620/1579]      || Average Training Loss: 2.3724\n",
      "Epoch: [29/50]         || Step: [640/1579]      || Average Training Loss: 2.3718\n",
      "Epoch: [29/50]         || Step: [660/1579]      || Average Training Loss: 2.3705\n",
      "Epoch: [29/50]         || Step: [680/1579]      || Average Training Loss: 2.3703\n",
      "Epoch: [29/50]         || Step: [700/1579]      || Average Training Loss: 2.3693\n",
      "Epoch: [29/50]         || Step: [720/1579]      || Average Training Loss: 2.3695\n",
      "Epoch: [29/50]         || Step: [740/1579]      || Average Training Loss: 2.3685\n",
      "Epoch: [29/50]         || Step: [760/1579]      || Average Training Loss: 2.3683\n",
      "Epoch: [29/50]         || Step: [780/1579]      || Average Training Loss: 2.3679\n",
      "Epoch: [29/50]         || Step: [800/1579]      || Average Training Loss: 2.3675\n",
      "Epoch: [29/50]         || Step: [820/1579]      || Average Training Loss: 2.3676\n",
      "Epoch: [29/50]         || Step: [840/1579]      || Average Training Loss: 2.3673\n",
      "Epoch: [29/50]         || Step: [860/1579]      || Average Training Loss: 2.3677\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a3c91-2b07-4ac6-9b61-7bf85f1667b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ea319-7ec4-4d50-88d9-7f14d809724b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
