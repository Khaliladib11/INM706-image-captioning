{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147089c7-5ce7-4a94-86d6-2058a8927377",
   "metadata": {},
   "source": [
    "# Image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803575f0-80ac-4309-aa48-0649a0f73152",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dcc675-185d-4e34-bc2f-847594ffee19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# faster version of json\n",
    "# !pip install ujson\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models import Encoder, Decoder\n",
    "\n",
    "from pathlib import Path\n",
    "from get_loader import MSCOCODataset, get_loader\n",
    "\n",
    "from data_prep_utils import *\n",
    "from utils import train, save_model, load_model, plot_loss\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "###### download the data we need\n",
    "# !cd ~/INM706-image-captioning/Datasets/coco/images/\n",
    "# !wget http://images.cocodataset.org/zips/train2017.zip\n",
    "# !wget http://images.cocodataset.org/zips/val2017.zip\n",
    "# !unzip train2017.zip\n",
    "# !unzip val2017.zip\n",
    "# !rm train2017.zip\n",
    "# !rm val2017.zip\n",
    "\n",
    "##### run code below if nltk hasn't been set up in clound instance yet\n",
    "# !python -m nltk.downloader -d /usr/local/share/nltk_data all\n",
    "\n",
    "###### run code below to save pre-trained weights if needed\n",
    "# cd ~/INM706-image-captioning/model\n",
    "# !wget https://download.pytorch.org/models/resnet152-394f9c45.pth\n",
    "# !mv resnet152-394f9c45.pth resnet152_model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf8cd3-2efb-47b3-ac83-628419c664a7",
   "metadata": {},
   "source": [
    "### Choose hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c984c7-0355-4b17-a4ee-293902fb181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for building vocab\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPTIONS_FILE = 'captions_train2017.json'\n",
    "\n",
    "# for data loader\n",
    "BATCH_SIZE = 32\n",
    "CAPS_PER_IMAGE = 5 # how many captions for each image to include in data set\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512 # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1 #hidden layers in LTSM\n",
    "\n",
    "# optimiser parameters\n",
    "OPT_PARAMS = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 15\n",
    "CHECKPOINT = '../model/image_captioning_model_v10.pth'\n",
    "PRINT_EVERY = 300 # run print_every batches and then\n",
    "# print running results. For bigger batch size make this \n",
    "# number smaller if you want to see regular output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830b0db-8fe8-4b85-b99b-50ee29182d28",
   "metadata": {},
   "source": [
    "## Load dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b68b6-e95a-45d7-97d1-df98d456535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('../Datasets/coco')\n",
    "imgs_path = root/'images'/'train2017'\n",
    "imgs_path_test = root/'images'/'val2017'\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=None,\n",
    "                    max_train=45000, max_val=15000, max_test=5000)\n",
    "\n",
    "train_captions_path = root/'annotations'/'custom_captions_train.json'\n",
    "val_captions_path = root/'annotations'/'custom_captions_val.json'\n",
    "test_captions_path = root/'annotations'/'custom_captions_test.json'\n",
    "\n",
    "#### build vocab using full original coco train\n",
    "# build_vocab(freq_threshold=FREQ_THRESHOLD,\n",
    "#             captions_file=CAPTIONS_FILE)\n",
    "\n",
    "# load vocab\n",
    "with open('../vocabulary/string_to_index.json') as json_file:\n",
    "    word2idx = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a4dc6-0e99-4b86-b2bf-2d376f2f09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to boost the performence of CUDA use:\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8baa40-b22f-4c9d-a445-c311eb3c190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': imgs_path,\n",
    "    'captions_path': train_captions_path,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': CAPS_PER_IMAGE,\n",
    "    'mode': \"train\",\n",
    "    'transform': None,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'word2idx': word2idx,\n",
    "}\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': imgs_path,\n",
    "    'captions_path': val_captions_path,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 1,\n",
    "    'mode': \"train\",\n",
    "    'transform': None,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'word2idx': word2idx,\n",
    "}\n",
    "\n",
    "train_dl, train_dataset = get_loader(**train_loader_params)\n",
    "val_dl, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "vocab_size = len(train_dataset.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678bc463-7410-4bfe-bada-adb3beb89559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is so we can save hyper params in checkpoint for future use\n",
    "hyper_params = {'vocab': train_dataset.idx2word,\n",
    "                'vocab_size': len(train_dataset.idx2word),\n",
    "                'vocab_captions_file': CAPTIONS_FILE,\n",
    "                'freq_threshold': FREQ_THRESHOLD,\n",
    "                'train_imgs_size': len(set([d[0] for d in train_dataset.img_deque])),\n",
    "                'train_sample_size': len(train_dataset.img_deque),\n",
    "                'val_imgs_size': len(set([d[0] for d in val_dataset.img_deque])),\n",
    "                'val_sample_size': len(val_dataset.img_deque),\n",
    "                'embed_size': EMBED_SIZE,\n",
    "                'hidden_size': HIDDEN_SIZE,\n",
    "                'num_layers': NUM_LAYERS,\n",
    "                'optimizer_params': OPT_PARAMS,\n",
    "                'batch_size': BATCH_SIZE\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ec7b5-a57e-4d97-ae68-cb766ebed31d",
   "metadata": {},
   "source": [
    "### Encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8a532-f6f7-42ff-840f-c5655e79c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=False, model_weight_path=\"../model/resnet152_model.pth\")\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141ddfe-efc0-4cb9-bc2b-f73beb911ddb",
   "metadata": {},
   "source": [
    "### Training paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48931ad-e9a8-4572-af11-0463772d6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "# not sure we need this with pad method in pytorch\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.word2idx[\"<PAD>\"]) ############\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(params, **OPT_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e5edc-a28a-4d8a-963a-fb748da3fa8d",
   "metadata": {},
   "source": [
    "#### Load checkpoints if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d3799-d7cf-4356-b74b-872123d114a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(CHECKPOINT).exists():\n",
    "    encoder, decoder, training_loss, validation_loss, hyper_params = load_model(encoder, decoder, CHECKPOINT)\n",
    "else:\n",
    "    print(f'{CHECKPOINT} file does not exist, training startging from scratch')\n",
    "    training_loss = None\n",
    "    validation_loss = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c3369c-7acf-41ad-b207-822538325db3",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270b0f5-fe3c-4137-be4a-da8b85a575c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'device': device,\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_dl,\n",
    "    'val_loader': val_dl,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'training_loss': training_loss,\n",
    "    'validation_loss': validation_loss,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aadc3e6-a77e-4662-8f42-12e0258e77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa66fde-7aae-4246-a2be-da634d9925e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in batch:\n",
    "    print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188f79a-904e-4443-a8db-4f41f6d9c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "capts = [1,2,3,4,5]\n",
    "torch.Tensor(capts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae6c7a-fefd-4429-ac9f-df81eacd62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c15b8-b3ba-4cf3-a255-fafe0af0e86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
