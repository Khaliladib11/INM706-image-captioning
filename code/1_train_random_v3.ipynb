{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09b4c42",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a3db4",
   "metadata": {},
   "source": [
    "In this notebook, we will run models, also this notebook can be a template to run other models with different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606af7ca",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fccfe284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_loader import get_loader\n",
    "from models import Encoder, Decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from data_prep_utils import *\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff406e5e",
   "metadata": {},
   "source": [
    "## Load train and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd484d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "#IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "#CAPTIONS_PATH = '../Datasets/coco/annotations/' #captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "SHUFFLE = True\n",
    "\n",
    "# root of the name to save or load captions files\n",
    "CAPTIONS_NAME = 'random_v3'\n",
    "\n",
    "# for data loader\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a247cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100052 captions in the data set\n",
      "With FREQ_THRESHOLD = 5, vocab size is 4699\n"
     ]
    }
   ],
   "source": [
    "# create custom data set if we need it. We can choose to work with certain types\n",
    "# of images or reduce the size of the data\n",
    "# this will write files to 'Datasets/coco/annotations' as \n",
    "#     [save_name]_captions_train.json\n",
    "#     [save_name]_captions_val.json\n",
    "#     [save_name]_captions_test.json\n",
    "\n",
    "prepare_datasets(train_percent = 0.87, super_categories=None,\n",
    "                 max_train=20000, max_val=2000, max_test=2000,\n",
    "                 save_name=CAPTIONS_NAME, random_seed=42)\n",
    "\n",
    "# we explicitly build the vocab here. We use frequency threshold, and we build\n",
    "# vocab from the specified captions file: we're using the training data\n",
    "# we save the vocab to a name consistent with our training captions data so that \n",
    "# we can load a vocab consistent with the specific training run we've used.\n",
    "build_vocab(freq_threshold = FREQ_THRESHOLD, \n",
    "            captions_file=f'{CAPTIONS_NAME}_captions_train.json',\n",
    "            vocab_save_name=CAPTIONS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d977f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../vocabulary/{CAPTIONS_NAME}word2idx.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4115c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataloader: 782, Length of testing dataloader: 79\n",
      "Length of vocabulary: 4699\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_train.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    # 'idx2word': None,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH + f'{CAPTIONS_NAME}_captions_val.json',\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    # 'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_loader)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde1d50",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593a54d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8405f913-3836-4994-b6a8-69f6d5f81ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for encoder and decoder\n",
    "EMBED_SIZE = 1024 # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3 #hidden layers in LTSM\n",
    "vocab_size = len(train_dataset.vocab.idx2word)\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 20\n",
    "CHECKPOINT = '../model/model_v3'\n",
    "PRINT_EVERY = 100 # run print_every batches and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a5f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'save_path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec546e0d-59fc-4a1f-aa63-3920345f3ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(CHECKPOINT, 'word2idx.json'), \"w\") as outfile:\n",
    "    json.dump(word2idx, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a086978",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder_ = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e07a2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder_.parameters()) + list(encoder_.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':3e-4, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296dccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]          || Step: [0/782]         || Average Training Loss: 8.4613\n",
      "Epoch: [0/20]          || Step: [100/782]       || Average Training Loss: 5.6644\n",
      "Epoch: [0/20]          || Step: [200/782]       || Average Training Loss: 5.1405\n",
      "Epoch: [0/20]          || Step: [300/782]       || Average Training Loss: 4.8450\n",
      "Epoch: [0/20]          || Step: [400/782]       || Average Training Loss: 4.6501\n",
      "Epoch: [0/20]          || Step: [500/782]       || Average Training Loss: 4.5044\n",
      "Epoch: [0/20]          || Step: [600/782]       || Average Training Loss: 4.3923\n",
      "Epoch: [0/20]          || Step: [700/782]       || Average Training Loss: 4.3015\n",
      "Epoch: [0/20]          || Step: [0/79]          || Average Validation Loss: 3.6771\n",
      "****************************************************************************************************\n",
      "Epoch: [0/20] || Training Loss = 4.24 || Validation Loss: 3.68 || Time: 21.254969\n",
      "****************************************************************************************************\n",
      "Epoch: [1/20]          || Step: [0/782]         || Average Training Loss: 3.6046\n",
      "Epoch: [1/20]          || Step: [100/782]       || Average Training Loss: 3.6320\n",
      "Epoch: [1/20]          || Step: [200/782]       || Average Training Loss: 3.6075\n",
      "Epoch: [1/20]          || Step: [300/782]       || Average Training Loss: 3.5791\n",
      "Epoch: [1/20]          || Step: [400/782]       || Average Training Loss: 3.5521\n",
      "Epoch: [1/20]          || Step: [500/782]       || Average Training Loss: 3.5263\n",
      "Epoch: [1/20]          || Step: [600/782]       || Average Training Loss: 3.4992\n",
      "Epoch: [1/20]          || Step: [700/782]       || Average Training Loss: 3.4731\n",
      "Epoch: [1/20]          || Step: [0/79]          || Average Validation Loss: 3.4560\n",
      "****************************************************************************************************\n",
      "Epoch: [1/20] || Training Loss = 3.46 || Validation Loss: 3.29 || Time: 42.366619\n",
      "****************************************************************************************************\n",
      "Epoch: [2/20]          || Step: [0/782]         || Average Training Loss: 3.2529\n",
      "Epoch: [2/20]          || Step: [100/782]       || Average Training Loss: 3.2472\n",
      "Epoch: [2/20]          || Step: [200/782]       || Average Training Loss: 3.2411\n",
      "Epoch: [2/20]          || Step: [300/782]       || Average Training Loss: 3.2295\n",
      "Epoch: [2/20]          || Step: [400/782]       || Average Training Loss: 3.2204\n",
      "Epoch: [2/20]          || Step: [500/782]       || Average Training Loss: 3.2103\n",
      "Epoch: [2/20]          || Step: [600/782]       || Average Training Loss: 3.1991\n",
      "Epoch: [2/20]          || Step: [700/782]       || Average Training Loss: 3.1923\n",
      "Epoch: [2/20]          || Step: [0/79]          || Average Validation Loss: 3.2604\n",
      "****************************************************************************************************\n",
      "Epoch: [2/20] || Training Loss = 3.18 || Validation Loss: 3.16 || Time: 63.437500\n",
      "****************************************************************************************************\n",
      "Epoch: [3/20]          || Step: [0/782]         || Average Training Loss: 2.9946\n",
      "Epoch: [3/20]          || Step: [100/782]       || Average Training Loss: 3.0915\n",
      "Epoch: [3/20]          || Step: [200/782]       || Average Training Loss: 3.0966\n",
      "Epoch: [3/20]          || Step: [300/782]       || Average Training Loss: 3.0879\n",
      "Epoch: [3/20]          || Step: [400/782]       || Average Training Loss: 3.0859\n",
      "Epoch: [3/20]          || Step: [500/782]       || Average Training Loss: 3.0862\n",
      "Epoch: [3/20]          || Step: [600/782]       || Average Training Loss: 3.0814\n",
      "Epoch: [3/20]          || Step: [700/782]       || Average Training Loss: 3.0768\n",
      "Epoch: [3/20]          || Step: [0/79]          || Average Validation Loss: 3.0216\n",
      "****************************************************************************************************\n",
      "Epoch: [3/20] || Training Loss = 3.07 || Validation Loss: 3.05 || Time: 84.584037\n",
      "****************************************************************************************************\n",
      "Epoch: [4/20]          || Step: [0/782]         || Average Training Loss: 3.0256\n",
      "Epoch: [4/20]          || Step: [100/782]       || Average Training Loss: 3.0111\n",
      "Epoch: [4/20]          || Step: [200/782]       || Average Training Loss: 3.0181\n",
      "Epoch: [4/20]          || Step: [300/782]       || Average Training Loss: 3.0147\n",
      "Epoch: [4/20]          || Step: [400/782]       || Average Training Loss: 3.0159\n",
      "Epoch: [4/20]          || Step: [500/782]       || Average Training Loss: 3.0133\n",
      "Epoch: [4/20]          || Step: [600/782]       || Average Training Loss: 3.0136\n",
      "Epoch: [4/20]          || Step: [700/782]       || Average Training Loss: 3.0131\n",
      "Epoch: [4/20]          || Step: [0/79]          || Average Validation Loss: 3.0662\n",
      "****************************************************************************************************\n",
      "Epoch: [4/20] || Training Loss = 3.01 || Validation Loss: 3.04 || Time: 105.714380\n",
      "****************************************************************************************************\n",
      "Epoch: [5/20]          || Step: [0/782]         || Average Training Loss: 2.9900\n",
      "Epoch: [5/20]          || Step: [100/782]       || Average Training Loss: 2.9839\n",
      "Epoch: [5/20]          || Step: [200/782]       || Average Training Loss: 2.9850\n",
      "Epoch: [5/20]          || Step: [300/782]       || Average Training Loss: 2.9841\n",
      "Epoch: [5/20]          || Step: [400/782]       || Average Training Loss: 2.9812\n",
      "Epoch: [5/20]          || Step: [500/782]       || Average Training Loss: 2.9795\n",
      "Epoch: [5/20]          || Step: [600/782]       || Average Training Loss: 2.9771\n",
      "Epoch: [5/20]          || Step: [700/782]       || Average Training Loss: 2.9752\n",
      "Epoch: [5/20]          || Step: [0/79]          || Average Validation Loss: 2.9316\n",
      "****************************************************************************************************\n",
      "Epoch: [5/20] || Training Loss = 2.97 || Validation Loss: 2.97 || Time: 126.866051\n",
      "****************************************************************************************************\n",
      "Epoch: [6/20]          || Step: [0/782]         || Average Training Loss: 3.0034\n",
      "Epoch: [6/20]          || Step: [100/782]       || Average Training Loss: 2.9546\n",
      "Epoch: [6/20]          || Step: [200/782]       || Average Training Loss: 2.9567\n",
      "Epoch: [6/20]          || Step: [300/782]       || Average Training Loss: 2.9605\n",
      "Epoch: [6/20]          || Step: [400/782]       || Average Training Loss: 2.9620\n",
      "Epoch: [6/20]          || Step: [500/782]       || Average Training Loss: 2.9558\n",
      "Epoch: [6/20]          || Step: [600/782]       || Average Training Loss: 2.9585\n",
      "Epoch: [6/20]          || Step: [700/782]       || Average Training Loss: 2.9557\n",
      "Epoch: [6/20]          || Step: [0/79]          || Average Validation Loss: 3.0387\n",
      "****************************************************************************************************\n",
      "Epoch: [6/20] || Training Loss = 2.96 || Validation Loss: 2.97 || Time: 148.012826\n",
      "****************************************************************************************************\n",
      "Epoch: [7/20]          || Step: [0/782]         || Average Training Loss: 2.9605\n",
      "Epoch: [7/20]          || Step: [100/782]       || Average Training Loss: 2.9300\n",
      "Epoch: [7/20]          || Step: [200/782]       || Average Training Loss: 2.9343\n",
      "Epoch: [7/20]          || Step: [300/782]       || Average Training Loss: 2.9340\n",
      "Epoch: [7/20]          || Step: [400/782]       || Average Training Loss: 2.9328\n",
      "Epoch: [7/20]          || Step: [500/782]       || Average Training Loss: 2.9373\n",
      "Epoch: [7/20]          || Step: [600/782]       || Average Training Loss: 2.9402\n",
      "Epoch: [7/20]          || Step: [700/782]       || Average Training Loss: 2.9393\n",
      "Epoch: [7/20]          || Step: [0/79]          || Average Validation Loss: 2.9317\n",
      "****************************************************************************************************\n",
      "Epoch: [7/20] || Training Loss = 2.94 || Validation Loss: 2.96 || Time: 169.118477\n",
      "****************************************************************************************************\n",
      "Epoch: [8/20]          || Step: [0/782]         || Average Training Loss: 2.9985\n",
      "Epoch: [8/20]          || Step: [100/782]       || Average Training Loss: 2.9234\n",
      "Epoch: [8/20]          || Step: [200/782]       || Average Training Loss: 2.9322\n",
      "Epoch: [8/20]          || Step: [300/782]       || Average Training Loss: 2.9299\n",
      "Epoch: [8/20]          || Step: [400/782]       || Average Training Loss: 2.9317\n",
      "Epoch: [8/20]          || Step: [500/782]       || Average Training Loss: 2.9317\n",
      "Epoch: [8/20]          || Step: [600/782]       || Average Training Loss: 2.9321\n",
      "Epoch: [8/20]          || Step: [700/782]       || Average Training Loss: 2.9335\n",
      "Epoch: [8/20]          || Step: [0/79]          || Average Validation Loss: 3.0291\n",
      "****************************************************************************************************\n",
      "Epoch: [8/20] || Training Loss = 2.93 || Validation Loss: 2.98 || Time: 190.233220\n",
      "****************************************************************************************************\n",
      "Epoch: [9/20]          || Step: [0/782]         || Average Training Loss: 3.0921\n",
      "Epoch: [9/20]          || Step: [100/782]       || Average Training Loss: 2.9193\n",
      "Epoch: [9/20]          || Step: [200/782]       || Average Training Loss: 2.9246\n",
      "Epoch: [9/20]          || Step: [300/782]       || Average Training Loss: 2.9306\n",
      "Epoch: [9/20]          || Step: [400/782]       || Average Training Loss: 2.9290\n",
      "Epoch: [9/20]          || Step: [500/782]       || Average Training Loss: 2.9285\n",
      "Epoch: [9/20]          || Step: [600/782]       || Average Training Loss: 2.9282\n",
      "Epoch: [9/20]          || Step: [700/782]       || Average Training Loss: 2.9266\n",
      "Epoch: [9/20]          || Step: [0/79]          || Average Validation Loss: 2.9736\n",
      "****************************************************************************************************\n",
      "Epoch: [9/20] || Training Loss = 2.93 || Validation Loss: 2.94 || Time: 214.723837\n",
      "****************************************************************************************************\n",
      "Epoch: [10/20]         || Step: [0/782]         || Average Training Loss: 2.7379\n",
      "Epoch: [10/20]         || Step: [100/782]       || Average Training Loss: 2.9065\n",
      "Epoch: [10/20]         || Step: [200/782]       || Average Training Loss: 2.9136\n",
      "Epoch: [10/20]         || Step: [300/782]       || Average Training Loss: 2.9163\n",
      "Epoch: [10/20]         || Step: [400/782]       || Average Training Loss: 2.9189\n",
      "Epoch: [10/20]         || Step: [500/782]       || Average Training Loss: 2.9225\n",
      "Epoch: [10/20]         || Step: [600/782]       || Average Training Loss: 2.9232\n",
      "Epoch: [10/20]         || Step: [700/782]       || Average Training Loss: 2.9246\n",
      "Epoch: [10/20]         || Step: [0/79]          || Average Validation Loss: 2.9778\n",
      "****************************************************************************************************\n",
      "Epoch: [10/20] || Training Loss = 2.92 || Validation Loss: 2.94 || Time: 244.097419\n",
      "****************************************************************************************************\n",
      "Epoch: [11/20]         || Step: [0/782]         || Average Training Loss: 2.9956\n",
      "Epoch: [11/20]         || Step: [100/782]       || Average Training Loss: 2.8981\n",
      "Epoch: [11/20]         || Step: [200/782]       || Average Training Loss: 2.9067\n",
      "Epoch: [11/20]         || Step: [300/782]       || Average Training Loss: 2.9139\n",
      "Epoch: [11/20]         || Step: [400/782]       || Average Training Loss: 2.9164\n",
      "Epoch: [11/20]         || Step: [500/782]       || Average Training Loss: 2.9181\n",
      "Epoch: [11/20]         || Step: [600/782]       || Average Training Loss: 2.9199\n",
      "Epoch: [11/20]         || Step: [700/782]       || Average Training Loss: 2.9189\n",
      "Epoch: [11/20]         || Step: [0/79]          || Average Validation Loss: 2.9739\n",
      "****************************************************************************************************\n",
      "Epoch: [11/20] || Training Loss = 2.92 || Validation Loss: 2.93 || Time: 266.455679\n",
      "****************************************************************************************************\n",
      "Epoch: [12/20]         || Step: [0/782]         || Average Training Loss: 2.8970\n",
      "Epoch: [12/20]         || Step: [100/782]       || Average Training Loss: 2.9051\n",
      "Epoch: [12/20]         || Step: [200/782]       || Average Training Loss: 2.9113\n",
      "Epoch: [12/20]         || Step: [300/782]       || Average Training Loss: 2.9137\n",
      "Epoch: [12/20]         || Step: [400/782]       || Average Training Loss: 2.9136\n",
      "Epoch: [12/20]         || Step: [500/782]       || Average Training Loss: 2.9136\n",
      "Epoch: [12/20]         || Step: [600/782]       || Average Training Loss: 2.9140\n",
      "Epoch: [12/20]         || Step: [700/782]       || Average Training Loss: 2.9147\n",
      "Epoch: [12/20]         || Step: [0/79]          || Average Validation Loss: 3.0245\n",
      "****************************************************************************************************\n",
      "Epoch: [12/20] || Training Loss = 2.92 || Validation Loss: 2.97 || Time: 287.581714\n",
      "****************************************************************************************************\n",
      "Epoch: [13/20]         || Step: [0/782]         || Average Training Loss: 2.8593\n",
      "Epoch: [13/20]         || Step: [100/782]       || Average Training Loss: 2.9121\n",
      "Epoch: [13/20]         || Step: [200/782]       || Average Training Loss: 2.9116\n",
      "Epoch: [13/20]         || Step: [300/782]       || Average Training Loss: 2.9096\n",
      "Epoch: [13/20]         || Step: [400/782]       || Average Training Loss: 2.9115\n",
      "Epoch: [13/20]         || Step: [500/782]       || Average Training Loss: 2.9110\n",
      "Epoch: [13/20]         || Step: [600/782]       || Average Training Loss: 2.9144\n",
      "Epoch: [13/20]         || Step: [700/782]       || Average Training Loss: 2.9134\n",
      "Epoch: [13/20]         || Step: [0/79]          || Average Validation Loss: 3.0263\n",
      "****************************************************************************************************\n",
      "Epoch: [13/20] || Training Loss = 2.91 || Validation Loss: 2.94 || Time: 308.683473\n",
      "****************************************************************************************************\n",
      "Epoch: [14/20]         || Step: [0/782]         || Average Training Loss: 2.7982\n",
      "Epoch: [14/20]         || Step: [100/782]       || Average Training Loss: 2.8959\n",
      "Epoch: [14/20]         || Step: [200/782]       || Average Training Loss: 2.9078\n",
      "Epoch: [14/20]         || Step: [300/782]       || Average Training Loss: 2.9063\n",
      "Epoch: [14/20]         || Step: [400/782]       || Average Training Loss: 2.9070\n",
      "Epoch: [14/20]         || Step: [500/782]       || Average Training Loss: 2.9085\n",
      "Epoch: [14/20]         || Step: [600/782]       || Average Training Loss: 2.9084\n",
      "Epoch: [14/20]         || Step: [700/782]       || Average Training Loss: 2.9110\n",
      "Epoch: [14/20]         || Step: [0/79]          || Average Validation Loss: 2.9357\n",
      "****************************************************************************************************\n",
      "Epoch: [14/20] || Training Loss = 2.91 || Validation Loss: 2.94 || Time: 331.037776\n",
      "****************************************************************************************************\n",
      "Epoch: [15/20]         || Step: [0/782]         || Average Training Loss: 2.9306\n",
      "Epoch: [15/20]         || Step: [100/782]       || Average Training Loss: 2.8985\n",
      "Epoch: [15/20]         || Step: [200/782]       || Average Training Loss: 2.8972\n",
      "Epoch: [15/20]         || Step: [300/782]       || Average Training Loss: 2.8990\n",
      "Epoch: [15/20]         || Step: [400/782]       || Average Training Loss: 2.9019\n",
      "Epoch: [15/20]         || Step: [500/782]       || Average Training Loss: 2.9027\n",
      "Epoch: [15/20]         || Step: [600/782]       || Average Training Loss: 2.9043\n",
      "Epoch: [15/20]         || Step: [700/782]       || Average Training Loss: 2.9044\n",
      "Epoch: [15/20]         || Step: [0/79]          || Average Validation Loss: 2.8325\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m: encoder_,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m'\u001b[39m: decoder_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m training_loss, validation_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/utils.py:119\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, criterion, optimizer, train_loader, val_loader, total_epoch, device, checkpoint_path, print_every, load_checkpoint)\u001b[0m\n\u001b[1;32m    116\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    117\u001b[0m decoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_loader):\n\u001b[1;32m    120\u001b[0m     idx, images, captions \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    121\u001b[0m     images, captions \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), captions\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/get_loader.py:179\u001b[0m, in \u001b[0;36mMSCOCODataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# get X: Image\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# get y: Image Caption\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_deque[idx][\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/get_loader.py:152\u001b[0m, in \u001b[0;36mMSCOCODataset.load_img\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    150\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m img_file_name)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/get_loader.py:138\u001b[0m, in \u001b[0;36mMSCOCODataset.img_transforms\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimg_transforms\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    129\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m    130\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m    131\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m     ])\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py:97\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 97\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/functional.py:153\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    151\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(pic\u001b[38;5;241m.\u001b[39mgetbands()))\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder_,\n",
    "    'decoder': decoder_,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f176d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
