{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f09b5e-1181-446b-a4fc-c5ded0cad3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from models import Encoder, Decoder\n",
    "\n",
    "from pathlib import Path\n",
    "from get_loader import MSCOCODataset, get_loader\n",
    "\n",
    "from data_prep_utils import *\n",
    "from utils import train, save_model, load_model, plot_loss, predict\n",
    "import json\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae83992-9d6e-4b59-b875-bd8ce4cad22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for building vocab\n",
    "FREQ_THRESHOLD = 5\n",
    "\n",
    "# for data loader\n",
    "BATCH_SIZE = 32\n",
    "CAPS_PER_IMAGE = 5 # how many captions for each image to include in data set\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512 # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1 #hidden layers in LTSM\n",
    "\n",
    "# optimiser parameters\n",
    "OPT_PARAMS = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "\n",
    "# checkpoint file\n",
    "CHECKPOINT = '../model/image_captioning_model_v10.pth'\n",
    "PRINT_EVERY = 300 # run print_every batches and then\n",
    "# print running results. For bigger batch size make this \n",
    "# number smaller if you want to see regular output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9c37fb-ccc1-485d-a244-52c67134dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('../Datasets/coco')\n",
    "imgs_path = root/'images'/'train2017'\n",
    "imgs_path_test = root/'images'/'val2017'\n",
    "\n",
    "# prepare_datasets(train_percent = 0.87, super_categories=['sports'],\n",
    "#                     max_train=15000, max_val=3000, max_test=3000)\n",
    "\n",
    "train_captions_path = root/'annotations'/'sports_captions_train.json'\n",
    "val_captions_path = root/'annotations'/'sports_captions_val.json'\n",
    "test_captions_path = root/'annotations'/'sports_captions_test.json'\n",
    "\n",
    "#### build vocab using full original coco train\n",
    "# build_vocab(freq_threshold=FREQ_THRESHOLD)\n",
    "\n",
    "# load vocab\n",
    "with open('../vocabulary/string_to_index.json') as json_file:\n",
    "    word2idx = json.load(json_file)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86dad570-4804-4060-9d27-35bf551b935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_params = {\n",
    "    'images_path': imgs_path_test,\n",
    "    'captions_path': test_captions_path,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 1,\n",
    "    'mode': \"train\",\n",
    "    'transform': None,\n",
    "    'batch_size': 1,\n",
    "    'shuffle': True,\n",
    "    'word2idx': word2idx,\n",
    "}\n",
    "\n",
    "test_dl, test_dataset = get_loader(**test_loader_params)\n",
    "\n",
    "vocab_size = len(test_dataset.word2idx)\n",
    "\n",
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True, model_weight_path=\"../model/resnet152_model.pth\")\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f186e0-8aec-46e5-9ffa-c8b7d9af74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(CHECKPOINT).exists():\n",
    "    encoder, decoder, training_loss, validation_loss, hyper_params = load_model(encoder, decoder, CHECKPOINT)\n",
    "else:\n",
    "    print(f'{CHECKPOINT} file does not exist, training startging from scratch')\n",
    "    training_loss = None\n",
    "    validation_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665277c2-ba24-489b-8e38-a6463b039aa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m decoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      8\u001b[0m features \u001b[38;5;241m=\u001b[39m encoder(im)\n\u001b[0;32m----> 9\u001b[0m caption \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                           \u001b[49m\u001b[43midx2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx2word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted caption for image \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([test_dataset\u001b[38;5;241m.\u001b[39midx2word[idx] \n\u001b[1;32m     13\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m caption]))\n",
      "\u001b[0;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'max_length'"
     ]
    }
   ],
   "source": [
    "idx, im, cap, capt_lens = next(iter(test_dl))\n",
    "im.to(device)\n",
    "cap.to(device)\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "features = encoder(im)\n",
    "caption = decoder.predict(features, max_length=40,\n",
    "                           idx2word=test_dataset.idx2word)\n",
    "print('predicted caption for image {}:'.format(idx[0]))\n",
    "print(' '.join([test_dataset.idx2word[idx] \n",
    "                for idx in caption]))\n",
    "\n",
    "image = test_dataset.coco.get_img(idx[0])\n",
    "# plt.imshow(torch.tensor(image).permute(1,2,0));\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b81498-c64a-4922-bad8-abd88df9e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(training_loss, validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2fc71-bb86-4e6e-a3c1-994ee90864af",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fcd0b-0735-415a-adfc-7dff5e44a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(encoder, decoder, image, test_dataset.idx2word, test_dataset.word2idx, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "404413c2-44c6-405b-91cc-b65be1293278",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CAPTIONS_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m test_captions_path \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msports_captions_test.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#### build vocab using full original coco train\u001b[39;00m\n\u001b[1;32m     13\u001b[0m build_vocab(freq_threshold\u001b[38;5;241m=\u001b[39mFREQ_THRESHOLD,\n\u001b[0;32m---> 14\u001b[0m             captions_file\u001b[38;5;241m=\u001b[39m\u001b[43mCAPTIONS_FILE\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# load vocab\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../vocabulary/string_to_index.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CAPTIONS_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "root = Path('../Datasets/coco')\n",
    "imgs_path = root/'images'/'train2017'\n",
    "imgs_path_test = root/'images'/'val2017'\n",
    "\n",
    "# prepare_datasets(train_percent = 0.87, super_categories=['sports'],\n",
    "#                     max_train=15000, max_val=3000, max_test=3000)\n",
    "\n",
    "train_captions_path = root/'annotations'/'sports_captions_train.json'\n",
    "val_captions_path = root/'annotations'/'sports_captions_val.json'\n",
    "test_captions_path = root/'annotations'/'sports_captions_test.json'\n",
    "\n",
    "#### build vocab using full original coco train\n",
    "build_vocab(freq_threshold=FREQ_THRESHOLD,\n",
    "            captions_file=CAPTIONS_FILE)\n",
    "\n",
    "# load vocab\n",
    "with open('../vocabulary/string_to_index.json') as json_file:\n",
    "    word2idx = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26160c8f-4c7d-4b84-9dab-36c9ebdf12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': imgs_path,\n",
    "    'captions_path': train_captions_path,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': CAPS_PER_IMAGE,\n",
    "    'mode': \"train\",\n",
    "    'transform': None,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'word2idx': word2idx,\n",
    "}\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': imgs_path,\n",
    "    'captions_path': val_captions_path,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 1,\n",
    "    'mode': \"train\",\n",
    "    'transform': None,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'word2idx': word2idx,\n",
    "}\n",
    "\n",
    "train_dl, train_dataset = get_loader(**train_loader_params)\n",
    "val_dl, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "vocab_size = len(train_dataset.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52343d50-ed63-4e00-991a-887e5a857694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.img_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "937ab9e7-0ae4-43ae-b21d-5b0e8dbe18a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([d[0] for d in train_dataset.img_deque]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ceb1c954-c967-4e87-8210-d7fcabe813ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'zip' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dl))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'zip' has no len()"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "dir(zip(*batch))\n",
    "len(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f7e3571-ccc6-4a06-bea0-2b3365d10721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26619\n",
      "74975\n",
      "30938\n",
      "69524\n",
      "48167\n",
      "62258\n",
      "66599\n",
      "11675\n",
      "48735\n",
      "52131\n",
      "66514\n",
      "14908\n",
      "51218\n",
      "62562\n",
      "9990\n",
      "67815\n",
      "417\n",
      "16824\n",
      "22131\n",
      "34759\n",
      "38040\n",
      "53261\n",
      "54655\n",
      "50356\n",
      "6691\n",
      "53155\n",
      "40020\n",
      "71626\n",
      "16588\n",
      "17586\n",
      "26323\n",
      "23428\n"
     ]
    }
   ],
   "source": [
    "for a, b, c in zip(*batch):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78288c73-cd8e-4e97-a584-71eb30f6e398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
