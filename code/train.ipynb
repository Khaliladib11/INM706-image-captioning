{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256e51d2",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff24f55",
   "metadata": {},
   "source": [
    "In this notebook, we will run models, also this notebook can be a template to run other models with different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7767e",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a880f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_loader import get_loader\n",
    "from models import Encoder, Decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd4d49",
   "metadata": {},
   "source": [
    "## Load train and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523632e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE = True\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 10\n",
    "CHECKPOINT = '../model/model_v1/model_v1_0.pth'\n",
    "check_path = Path(CHECKPOINT)\n",
    "check_path.parent.mkdir(exist_ok=True) # make folder to avoid error further down notebook\n",
    "\n",
    "PRINT_EVERY = 500 # run print_every batches and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0113db3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of training dataloader: 50000, Lenght of testing dataloader: 188\n",
      "Lenght of vocabulary: 3387\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    'idx2word': None,\n",
    "    'word2idx': None\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': train_dataset.vocab.word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Lenght of training dataloader: {len(train_dataset)}, Lenght of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Lenght of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5622d7ab-d04b-4d5e-ae2a-fb323979833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.vocab.export_vocab('../vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e031a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(train_dataset.vocab.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb037c",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a492102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b4ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "673155db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed65aa52-7d2c-434e-b317-e3619bc82a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'path': check_path.parent/'model_v1_0_param.json',\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45158cda",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ab890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/10] || Step: [0/1563] || Average Training Loss: 8.1324\n",
      "Epoch: [0/10] || Step: [500/1563] || Average Training Loss: 3.8319\n",
      "Epoch: [0/10] || Step: [1000/1563] || Average Training Loss: 3.5866\n",
      "Epoch: [0/10] || Step: [1500/1563] || Average Training Loss: 3.4698\n",
      "Epoch: [0/10] || Step: [0/188] || Average Validation Loss: 3.2409\n",
      "****************************************************************************************************\n",
      "Epoch: [0/10] || Training Loss = 3.46 || Validation Loss: 3.15 || Time: 14.850750\n",
      "****************************************************************************************************\n",
      "Epoch: [1/10] || Step: [0/1563] || Average Training Loss: 3.0344\n",
      "Epoch: [1/10] || Step: [500/1563] || Average Training Loss: 3.2017\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0a2c2-8bf7-4a91-9ebb-ffe9601c1ef4",
   "metadata": {},
   "source": [
    "## Try with different hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c88d1-e10a-4b3b-a017-a28e227d509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data loader\n",
    "BATCH_SIZE = 128\n",
    "CAPS_PER_IMAGE = 5 # how many captions for each image to include in data set\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 1024 # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3 #hidden layers in LTSM\n",
    "vocab_size = len(train_dataset.vocab.idx2word)\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 10\n",
    "CHECKPOINT = '../model/model_v2/model_v2_1.pth'\n",
    "PRINT_EVERY = 500 # run print_every batches and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'path': '../model/model_v2/model_v2_1_param.json',\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67378d-6e62-4b7c-9557-ea91b2f06802",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder_ = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e938c-e9cc-47bd-9290-343304a18e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder_.parameters()) + list(encoder_.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':3e-4, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d3cbe-6ae1-4619-98ac-a3f15e3f7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder_,\n",
    "    'decoder': decoder_,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa16cd-fa96-4164-923f-8fedfc4c78d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
