{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256e51d2",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7767e",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a880f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_loader import get_loader\n",
    "from models import Encoder, Decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd4d49",
   "metadata": {},
   "source": [
    "## Load train and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523632e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "image_path = '../../Data/train2017/train2017'\n",
    "captions_path = '../../Data/annotations/captions_train2017.json'\n",
    "freq_threshold = 5\n",
    "caps_per_image = 5\n",
    "batch_size = 32\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0113db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': image_path,\n",
    "    'captions_path': captions_path,\n",
    "    'freq_threshold': freq_threshold,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': shuffle,\n",
    "    'mode': 'train',\n",
    "    'idx2word': None,\n",
    "    'word2idx': None\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': image_path,\n",
    "    'captions_path': captions_path,\n",
    "    'freq_threshold': freq_threshold,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': shuffle,\n",
    "    'mode': 'validation',\n",
    "    'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': train_dataset.vocab.word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5622d7ab-d04b-4d5e-ae2a-fb323979833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.vocab.export_vocab('../vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e2414b-169f-498f-b157-f05df03529fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3387"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.vocab.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb037c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a492102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e031a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data loader\n",
    "BATCH_SIZE = 32\n",
    "CAPS_PER_IMAGE = 5 # how many captions for each image to include in data set\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512 # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1 #hidden layers in LTSM\n",
    "vocab_size = len(train_dataset.vocab.idx2word)\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 1000\n",
    "CHECKPOINT = '../model/model_v2/model_v2_0.pth'\n",
    "PRINT_EVERY = 500 # run print_every batches and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b4ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "673155db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed65aa52-7d2c-434e-b317-e3619bc82a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params('../model/model_v2/model_v2_0_param.json', 32, 512, 512, 1, len(train_dataset.vocab.idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2ab890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/1000] || Step: [0/1563] || Average Training Loss: 8.1233\n",
      "Epoch: [0/1000] || Step: [500/1563] || Average Training Loss: 3.8293\n",
      "Epoch: [0/1000] || Step: [1000/1563] || Average Training Loss: 3.5724\n",
      "Epoch: [0/1000] || Step: [1500/1563] || Average Training Loss: 3.4573\n",
      "Epoch: [0/1000] || Step: [0/188] || Average Validation Loss: 3.1172\n",
      "****************************************************************************************************\n",
      "Epoch: [0/1000] || Training Loss = 3.45 || Validation Loss: 3.14 || Time: 11.907353\n",
      "****************************************************************************************************\n",
      "Epoch: [1/1000] || Step: [0/1563] || Average Training Loss: 3.2215\n",
      "Epoch: [1/1000] || Step: [500/1563] || Average Training Loss: 3.1854\n",
      "Epoch: [1/1000] || Step: [1000/1563] || Average Training Loss: 3.1809\n",
      "Epoch: [1/1000] || Step: [1500/1563] || Average Training Loss: 3.1810\n",
      "Epoch: [1/1000] || Step: [0/188] || Average Validation Loss: 2.8488\n",
      "****************************************************************************************************\n",
      "Epoch: [1/1000] || Training Loss = 3.18 || Validation Loss: 3.09 || Time: 10.665041\n",
      "****************************************************************************************************\n",
      "Epoch: [2/1000] || Step: [0/1563] || Average Training Loss: 3.4237\n",
      "Epoch: [2/1000] || Step: [500/1563] || Average Training Loss: 3.1711\n",
      "Epoch: [2/1000] || Step: [1000/1563] || Average Training Loss: 3.1773\n",
      "Epoch: [2/1000] || Step: [1500/1563] || Average Training Loss: 3.1733\n",
      "Epoch: [2/1000] || Step: [0/188] || Average Validation Loss: 3.0244\n",
      "****************************************************************************************************\n",
      "Epoch: [2/1000] || Training Loss = 3.17 || Validation Loss: 3.08 || Time: 10.723867\n",
      "****************************************************************************************************\n",
      "Epoch: [3/1000] || Step: [0/1563] || Average Training Loss: 2.8222\n",
      "Epoch: [3/1000] || Step: [1000/1563] || Average Training Loss: 3.1554\n",
      "Epoch: [3/1000] || Step: [1500/1563] || Average Training Loss: 3.1573\n",
      "Epoch: [3/1000] || Step: [0/188] || Average Validation Loss: 2.9145\n",
      "****************************************************************************************************\n",
      "Epoch: [3/1000] || Training Loss = 3.16 || Validation Loss: 3.08 || Time: 10.587364\n",
      "****************************************************************************************************\n",
      "Epoch: [4/1000] || Step: [0/1563] || Average Training Loss: 3.0725\n",
      "Epoch: [4/1000] || Step: [500/1563] || Average Training Loss: 3.1438\n",
      "Epoch: [4/1000] || Step: [1000/1563] || Average Training Loss: 3.1406\n",
      "Epoch: [4/1000] || Step: [1500/1563] || Average Training Loss: 3.1391\n",
      "Epoch: [4/1000] || Step: [0/188] || Average Validation Loss: 3.0545\n",
      "****************************************************************************************************\n",
      "Epoch: [4/1000] || Training Loss = 3.14 || Validation Loss: 3.06 || Time: 10.530759\n",
      "****************************************************************************************************\n",
      "Epoch: [5/1000] || Step: [0/1563] || Average Training Loss: 3.3869\n",
      "Epoch: [5/1000] || Step: [500/1563] || Average Training Loss: 3.1375\n",
      "Epoch: [5/1000] || Step: [1000/1563] || Average Training Loss: 3.1320\n",
      "Epoch: [5/1000] || Step: [1500/1563] || Average Training Loss: 3.1281\n",
      "Epoch: [5/1000] || Step: [0/188] || Average Validation Loss: 3.3188\n",
      "****************************************************************************************************\n",
      "Epoch: [5/1000] || Training Loss = 3.13 || Validation Loss: 3.05 || Time: 10.483617\n",
      "****************************************************************************************************\n",
      "Epoch: [6/1000] || Step: [0/1563] || Average Training Loss: 3.3483\n",
      "Epoch: [6/1000] || Step: [500/1563] || Average Training Loss: 3.1084\n",
      "Epoch: [6/1000] || Step: [1000/1563] || Average Training Loss: 3.1099\n",
      "Epoch: [6/1000] || Step: [1500/1563] || Average Training Loss: 3.1153\n",
      "Epoch: [6/1000] || Step: [0/188] || Average Validation Loss: 2.9974\n",
      "****************************************************************************************************\n",
      "Epoch: [6/1000] || Training Loss = 3.12 || Validation Loss: 3.05 || Time: 10.551144\n",
      "****************************************************************************************************\n",
      "Epoch: [7/1000] || Step: [0/1563] || Average Training Loss: 3.0856\n",
      "Epoch: [7/1000] || Step: [500/1563] || Average Training Loss: 3.1013\n",
      "Epoch: [7/1000] || Step: [1000/1563] || Average Training Loss: 3.1074\n",
      "Epoch: [7/1000] || Step: [1500/1563] || Average Training Loss: 3.1090\n",
      "Epoch: [7/1000] || Step: [0/188] || Average Validation Loss: 2.8765\n",
      "****************************************************************************************************\n",
      "Epoch: [7/1000] || Training Loss = 3.11 || Validation Loss: 3.04 || Time: 10.589264\n",
      "****************************************************************************************************\n",
      "Epoch: [8/1000] || Step: [0/1563] || Average Training Loss: 2.9984\n",
      "Epoch: [8/1000] || Step: [500/1563] || Average Training Loss: 3.0928\n",
      "Epoch: [8/1000] || Step: [1000/1563] || Average Training Loss: 3.0969\n",
      "Epoch: [8/1000] || Step: [1500/1563] || Average Training Loss: 3.1048\n",
      "Epoch: [8/1000] || Step: [0/188] || Average Validation Loss: 2.9472\n",
      "****************************************************************************************************\n",
      "Epoch: [8/1000] || Training Loss = 3.11 || Validation Loss: 3.03 || Time: 10.588996\n",
      "****************************************************************************************************\n",
      "Epoch: [9/1000] || Step: [0/1563] || Average Training Loss: 3.3560\n",
      "Epoch: [9/1000] || Step: [500/1563] || Average Training Loss: 3.0976\n",
      "Epoch: [9/1000] || Step: [1000/1563] || Average Training Loss: 3.0988\n",
      "Epoch: [9/1000] || Step: [1500/1563] || Average Training Loss: 3.0994\n",
      "Epoch: [9/1000] || Step: [0/188] || Average Validation Loss: 2.9304\n",
      "****************************************************************************************************\n",
      "Epoch: [9/1000] || Training Loss = 3.10 || Validation Loss: 3.01 || Time: 10.684462\n",
      "****************************************************************************************************\n",
      "Epoch: [10/1000] || Step: [0/1563] || Average Training Loss: 2.8975\n",
      "Epoch: [10/1000] || Step: [500/1563] || Average Training Loss: 3.0809\n",
      "Epoch: [10/1000] || Step: [1000/1563] || Average Training Loss: 3.0954\n",
      "Epoch: [10/1000] || Step: [1500/1563] || Average Training Loss: 3.0970\n",
      "Epoch: [10/1000] || Step: [0/188] || Average Validation Loss: 3.0944\n",
      "****************************************************************************************************\n",
      "Epoch: [10/1000] || Training Loss = 3.10 || Validation Loss: 3.02 || Time: 10.696837\n",
      "****************************************************************************************************\n",
      "Epoch: [11/1000] || Step: [0/1563] || Average Training Loss: 3.0778\n",
      "Epoch: [11/1000] || Step: [500/1563] || Average Training Loss: 3.0805\n",
      "Epoch: [11/1000] || Step: [1000/1563] || Average Training Loss: 3.0835\n",
      "Epoch: [11/1000] || Step: [1500/1563] || Average Training Loss: 3.0952\n",
      "Epoch: [11/1000] || Step: [0/188] || Average Validation Loss: 2.9429\n",
      "****************************************************************************************************\n",
      "Epoch: [11/1000] || Training Loss = 3.10 || Validation Loss: 3.02 || Time: 10.624531\n",
      "****************************************************************************************************\n",
      "Epoch: [12/1000] || Step: [0/1563] || Average Training Loss: 3.2984\n",
      "Epoch: [12/1000] || Step: [500/1563] || Average Training Loss: 3.0953\n",
      "Epoch: [12/1000] || Step: [1000/1563] || Average Training Loss: 3.0971\n",
      "Epoch: [12/1000] || Step: [1500/1563] || Average Training Loss: 3.0948\n",
      "Epoch: [12/1000] || Step: [0/188] || Average Validation Loss: 3.0627\n",
      "****************************************************************************************************\n",
      "Epoch: [12/1000] || Training Loss = 3.10 || Validation Loss: 3.02 || Time: 10.640958\n",
      "****************************************************************************************************\n",
      "Epoch: [13/1000] || Step: [0/1563] || Average Training Loss: 3.0360\n",
      "Epoch: [13/1000] || Step: [500/1563] || Average Training Loss: 3.0868\n",
      "Epoch: [13/1000] || Step: [1000/1563] || Average Training Loss: 3.0897\n",
      "Epoch: [13/1000] || Step: [1500/1563] || Average Training Loss: 3.0931\n",
      "Epoch: [13/1000] || Step: [0/188] || Average Validation Loss: 2.9188\n",
      "****************************************************************************************************\n",
      "Epoch: [13/1000] || Training Loss = 3.09 || Validation Loss: 3.03 || Time: 10.512390\n",
      "****************************************************************************************************\n",
      "Epoch: [14/1000] || Step: [0/1563] || Average Training Loss: 3.3241\n",
      "Epoch: [14/1000] || Step: [500/1563] || Average Training Loss: 3.0946\n",
      "Epoch: [14/1000] || Step: [1000/1563] || Average Training Loss: 3.0934\n",
      "Epoch: [14/1000] || Step: [1500/1563] || Average Training Loss: 3.0905\n",
      "Epoch: [14/1000] || Step: [0/188] || Average Validation Loss: 3.0481\n",
      "****************************************************************************************************\n",
      "Epoch: [14/1000] || Training Loss = 3.09 || Validation Loss: 3.01 || Time: 10.693642\n",
      "****************************************************************************************************\n",
      "Epoch: [15/1000] || Step: [0/1563] || Average Training Loss: 3.2874\n",
      "Epoch: [15/1000] || Step: [500/1563] || Average Training Loss: 3.0867\n",
      "Epoch: [15/1000] || Step: [1000/1563] || Average Training Loss: 3.0898\n",
      "Epoch: [15/1000] || Step: [1500/1563] || Average Training Loss: 3.0883\n",
      "Epoch: [15/1000] || Step: [0/188] || Average Validation Loss: 3.1094\n",
      "****************************************************************************************************\n",
      "Epoch: [15/1000] || Training Loss = 3.09 || Validation Loss: 3.02 || Time: 10.734652\n",
      "****************************************************************************************************\n",
      "Epoch: [16/1000] || Step: [0/1563] || Average Training Loss: 3.3093\n",
      "Epoch: [16/1000] || Step: [500/1563] || Average Training Loss: 3.0862\n",
      "Epoch: [16/1000] || Step: [1000/1563] || Average Training Loss: 3.0858\n",
      "Epoch: [16/1000] || Step: [1500/1563] || Average Training Loss: 3.0856\n",
      "Epoch: [16/1000] || Step: [0/188] || Average Validation Loss: 3.0263\n",
      "****************************************************************************************************\n",
      "Epoch: [16/1000] || Training Loss = 3.09 || Validation Loss: 3.02 || Time: 10.652277\n",
      "****************************************************************************************************\n",
      "Epoch: [17/1000] || Step: [0/1563] || Average Training Loss: 2.9174\n",
      "Epoch: [17/1000] || Step: [500/1563] || Average Training Loss: 3.0782\n",
      "Epoch: [17/1000] || Step: [1000/1563] || Average Training Loss: 3.0833\n",
      "Epoch: [17/1000] || Step: [1500/1563] || Average Training Loss: 3.0876\n",
      "Epoch: [17/1000] || Step: [0/188] || Average Validation Loss: 2.9742\n",
      "****************************************************************************************************\n",
      "Epoch: [17/1000] || Training Loss = 3.09 || Validation Loss: 3.01 || Time: 10.649237\n",
      "****************************************************************************************************\n",
      "Epoch: [18/1000] || Step: [0/1563] || Average Training Loss: 3.1312\n",
      "Epoch: [18/1000] || Step: [500/1563] || Average Training Loss: 3.0780\n",
      "Epoch: [18/1000] || Step: [1000/1563] || Average Training Loss: 3.0851\n",
      "Epoch: [18/1000] || Step: [1500/1563] || Average Training Loss: 3.0855\n",
      "Epoch: [18/1000] || Step: [0/188] || Average Validation Loss: 2.8024\n",
      "****************************************************************************************************\n",
      "Epoch: [18/1000] || Training Loss = 3.09 || Validation Loss: 3.01 || Time: 10.606977\n",
      "****************************************************************************************************\n",
      "Epoch: [19/1000] || Step: [0/1563] || Average Training Loss: 3.0839\n",
      "Epoch: [19/1000] || Step: [500/1563] || Average Training Loss: 3.0771\n",
      "Epoch: [19/1000] || Step: [1000/1563] || Average Training Loss: 3.0841\n",
      "Epoch: [19/1000] || Step: [1500/1563] || Average Training Loss: 3.0828\n",
      "Epoch: [19/1000] || Step: [0/188] || Average Validation Loss: 3.1030\n",
      "****************************************************************************************************\n",
      "Epoch: [19/1000] || Training Loss = 3.08 || Validation Loss: 3.01 || Time: 10.630528\n",
      "****************************************************************************************************\n",
      "Epoch: [20/1000] || Step: [0/1563] || Average Training Loss: 3.5503\n",
      "Epoch: [20/1000] || Step: [500/1563] || Average Training Loss: 3.0726\n",
      "Epoch: [20/1000] || Step: [1000/1563] || Average Training Loss: 3.0812\n",
      "Epoch: [20/1000] || Step: [1500/1563] || Average Training Loss: 3.0829\n",
      "Epoch: [20/1000] || Step: [0/188] || Average Validation Loss: 3.0742\n",
      "****************************************************************************************************\n",
      "Epoch: [20/1000] || Training Loss = 3.08 || Validation Loss: 3.00 || Time: 10.570503\n",
      "****************************************************************************************************\n",
      "Epoch: [21/1000] || Step: [0/1563] || Average Training Loss: 3.2144\n",
      "Epoch: [21/1000] || Step: [500/1563] || Average Training Loss: 3.0818\n",
      "Epoch: [21/1000] || Step: [1000/1563] || Average Training Loss: 3.0815\n",
      "Epoch: [21/1000] || Step: [1500/1563] || Average Training Loss: 3.0815\n",
      "Epoch: [21/1000] || Step: [0/188] || Average Validation Loss: 3.0391\n",
      "****************************************************************************************************\n",
      "Epoch: [21/1000] || Training Loss = 3.08 || Validation Loss: 3.01 || Time: 10.831005\n",
      "****************************************************************************************************\n",
      "Epoch: [22/1000] || Step: [0/1563] || Average Training Loss: 2.6234\n",
      "Epoch: [22/1000] || Step: [500/1563] || Average Training Loss: 3.0685\n",
      "Epoch: [22/1000] || Step: [1000/1563] || Average Training Loss: 3.0750\n",
      "Epoch: [22/1000] || Step: [1500/1563] || Average Training Loss: 3.0781\n",
      "Epoch: [22/1000] || Step: [0/188] || Average Validation Loss: 2.8778\n",
      "****************************************************************************************************\n",
      "Epoch: [22/1000] || Training Loss = 3.08 || Validation Loss: 3.09 || Time: 10.734302\n",
      "****************************************************************************************************\n",
      "Epoch: [23/1000] || Step: [0/1563] || Average Training Loss: 3.3196\n",
      "Epoch: [23/1000] || Step: [500/1563] || Average Training Loss: 3.0843\n",
      "Epoch: [23/1000] || Step: [1000/1563] || Average Training Loss: 3.0819\n",
      "Epoch: [23/1000] || Step: [1500/1563] || Average Training Loss: 3.0807\n",
      "Epoch: [23/1000] || Step: [0/188] || Average Validation Loss: 2.8121\n",
      "****************************************************************************************************\n",
      "Epoch: [23/1000] || Training Loss = 3.08 || Validation Loss: 3.01 || Time: 10.613399\n",
      "****************************************************************************************************\n",
      "Epoch: [24/1000] || Step: [0/1563] || Average Training Loss: 2.9804\n",
      "Epoch: [24/1000] || Step: [500/1563] || Average Training Loss: 3.0757\n",
      "Epoch: [24/1000] || Step: [1000/1563] || Average Training Loss: 3.0766\n",
      "Epoch: [24/1000] || Step: [1500/1563] || Average Training Loss: 3.0782\n",
      "Epoch: [24/1000] || Step: [0/188] || Average Validation Loss: 3.0357\n",
      "****************************************************************************************************\n",
      "Epoch: [24/1000] || Training Loss = 3.08 || Validation Loss: 3.01 || Time: 11.132902\n",
      "****************************************************************************************************\n",
      "Epoch: [25/1000] || Step: [0/1563] || Average Training Loss: 2.9905\n",
      "Epoch: [25/1000] || Step: [500/1563] || Average Training Loss: 3.0730\n",
      "Epoch: [25/1000] || Step: [1000/1563] || Average Training Loss: 3.0772\n",
      "Epoch: [25/1000] || Step: [1500/1563] || Average Training Loss: 3.0771\n",
      "Epoch: [25/1000] || Step: [0/188] || Average Validation Loss: 2.9286\n",
      "****************************************************************************************************\n",
      "Epoch: [25/1000] || Training Loss = 3.08 || Validation Loss: 3.01 || Time: 10.984709\n",
      "****************************************************************************************************\n",
      "Epoch: [26/1000] || Step: [0/1563] || Average Training Loss: 2.7217\n",
      "Epoch: [26/1000] || Step: [500/1563] || Average Training Loss: 3.0664\n",
      "Epoch: [26/1000] || Step: [1000/1563] || Average Training Loss: 3.0766\n",
      "Epoch: [26/1000] || Step: [1500/1563] || Average Training Loss: 3.0782\n",
      "Epoch: [26/1000] || Step: [0/188] || Average Validation Loss: 2.6891\n",
      "****************************************************************************************************\n",
      "Epoch: [26/1000] || Training Loss = 3.08 || Validation Loss: 3.01 || Time: 11.184853\n",
      "****************************************************************************************************\n",
      "Epoch: [27/1000] || Step: [0/1563] || Average Training Loss: 3.2033\n",
      "Epoch: [27/1000] || Step: [500/1563] || Average Training Loss: 3.0630\n",
      "Epoch: [27/1000] || Step: [1000/1563] || Average Training Loss: 3.0715\n",
      "Epoch: [27/1000] || Step: [1500/1563] || Average Training Loss: 3.0750\n",
      "Epoch: [27/1000] || Step: [0/188] || Average Validation Loss: 2.6157\n",
      "****************************************************************************************************\n",
      "Epoch: [27/1000] || Training Loss = 3.08 || Validation Loss: 3.00 || Time: 10.986140\n",
      "****************************************************************************************************\n",
      "Epoch: [28/1000] || Step: [0/1563] || Average Training Loss: 3.1992\n",
      "Epoch: [28/1000] || Step: [500/1563] || Average Training Loss: 3.0739\n",
      "Epoch: [28/1000] || Step: [1000/1563] || Average Training Loss: 3.0722\n",
      "Epoch: [28/1000] || Step: [1500/1563] || Average Training Loss: 3.0780\n",
      "Epoch: [28/1000] || Step: [0/188] || Average Validation Loss: 2.8545\n",
      "****************************************************************************************************\n",
      "Epoch: [28/1000] || Training Loss = 3.08 || Validation Loss: 3.03 || Time: 10.597311\n",
      "****************************************************************************************************\n",
      "Epoch: [29/1000] || Step: [0/1563] || Average Training Loss: 3.1980\n",
      "Epoch: [29/1000] || Step: [500/1563] || Average Training Loss: 3.0740\n",
      "Epoch: [29/1000] || Step: [1000/1563] || Average Training Loss: 3.0735\n",
      "Epoch: [29/1000] || Step: [1500/1563] || Average Training Loss: 3.0743\n",
      "Epoch: [29/1000] || Step: [0/188] || Average Validation Loss: 3.0126\n",
      "****************************************************************************************************\n",
      "Epoch: [29/1000] || Training Loss = 3.08 || Validation Loss: 3.01 || Time: 10.719098\n",
      "****************************************************************************************************\n",
      "Epoch: [30/1000] || Step: [0/1563] || Average Training Loss: 3.0567\n",
      "Epoch: [30/1000] || Step: [500/1563] || Average Training Loss: 3.0728\n",
      "Epoch: [30/1000] || Step: [1000/1563] || Average Training Loss: 3.0717\n",
      "Epoch: [30/1000] || Step: [1500/1563] || Average Training Loss: 3.0753\n",
      "Epoch: [30/1000] || Step: [0/188] || Average Validation Loss: 2.8902\n",
      "****************************************************************************************************\n",
      "Epoch: [30/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 11.337105\n",
      "****************************************************************************************************\n",
      "Epoch: [31/1000] || Step: [0/1563] || Average Training Loss: 2.9418\n",
      "Epoch: [31/1000] || Step: [500/1563] || Average Training Loss: 3.0707\n",
      "Epoch: [31/1000] || Step: [1000/1563] || Average Training Loss: 3.0722\n",
      "Epoch: [31/1000] || Step: [1500/1563] || Average Training Loss: 3.0765\n",
      "Epoch: [31/1000] || Step: [0/188] || Average Validation Loss: 2.9593\n",
      "****************************************************************************************************\n",
      "Epoch: [31/1000] || Training Loss = 3.08 || Validation Loss: 3.00 || Time: 11.405494\n",
      "****************************************************************************************************\n",
      "Epoch: [32/1000] || Step: [0/1563] || Average Training Loss: 3.0720\n",
      "Epoch: [32/1000] || Step: [500/1563] || Average Training Loss: 3.0713\n",
      "Epoch: [32/1000] || Step: [1000/1563] || Average Training Loss: 3.0733\n",
      "Epoch: [32/1000] || Step: [1500/1563] || Average Training Loss: 3.0751\n",
      "Epoch: [32/1000] || Step: [0/188] || Average Validation Loss: 2.7518\n",
      "****************************************************************************************************\n",
      "Epoch: [32/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 10.795148\n",
      "****************************************************************************************************\n",
      "Epoch: [33/1000] || Step: [0/1563] || Average Training Loss: 3.1373\n",
      "Epoch: [33/1000] || Step: [500/1563] || Average Training Loss: 3.0760\n",
      "Epoch: [33/1000] || Step: [1000/1563] || Average Training Loss: 3.0697\n",
      "Epoch: [33/1000] || Step: [1500/1563] || Average Training Loss: 3.0718\n",
      "Epoch: [33/1000] || Step: [0/188] || Average Validation Loss: 2.9286\n",
      "****************************************************************************************************\n",
      "Epoch: [33/1000] || Training Loss = 3.07 || Validation Loss: 3.01 || Time: 10.598040\n",
      "****************************************************************************************************\n",
      "Epoch: [34/1000] || Step: [0/1563] || Average Training Loss: 3.3268\n",
      "Epoch: [34/1000] || Step: [500/1563] || Average Training Loss: 3.0696\n",
      "Epoch: [34/1000] || Step: [1000/1563] || Average Training Loss: 3.0711\n",
      "Epoch: [34/1000] || Step: [1500/1563] || Average Training Loss: 3.0719\n",
      "Epoch: [34/1000] || Step: [0/188] || Average Validation Loss: 3.1636\n",
      "****************************************************************************************************\n",
      "Epoch: [34/1000] || Training Loss = 3.07 || Validation Loss: 3.01 || Time: 10.673946\n",
      "****************************************************************************************************\n",
      "Epoch: [35/1000] || Step: [0/1563] || Average Training Loss: 2.6377\n",
      "Epoch: [35/1000] || Step: [500/1563] || Average Training Loss: 3.0678\n",
      "Epoch: [35/1000] || Step: [1000/1563] || Average Training Loss: 3.0730\n",
      "Epoch: [35/1000] || Step: [1500/1563] || Average Training Loss: 3.0752\n",
      "Epoch: [35/1000] || Step: [0/188] || Average Validation Loss: 3.1233\n",
      "****************************************************************************************************\n",
      "Epoch: [35/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 10.837253\n",
      "****************************************************************************************************\n",
      "Epoch: [36/1000] || Step: [0/1563] || Average Training Loss: 2.8173\n",
      "Epoch: [36/1000] || Step: [500/1563] || Average Training Loss: 3.0815\n",
      "Epoch: [36/1000] || Step: [1000/1563] || Average Training Loss: 3.0738\n",
      "Epoch: [36/1000] || Step: [1500/1563] || Average Training Loss: 3.0761\n",
      "Epoch: [36/1000] || Step: [0/188] || Average Validation Loss: 3.1798\n",
      "****************************************************************************************************\n",
      "Epoch: [36/1000] || Training Loss = 3.08 || Validation Loss: 3.02 || Time: 11.197633\n",
      "****************************************************************************************************\n",
      "Epoch: [37/1000] || Step: [0/1563] || Average Training Loss: 2.9912\n",
      "Epoch: [37/1000] || Step: [500/1563] || Average Training Loss: 3.0565\n",
      "Epoch: [37/1000] || Step: [1000/1563] || Average Training Loss: 3.0712\n",
      "Epoch: [37/1000] || Step: [1500/1563] || Average Training Loss: 3.0732\n",
      "Epoch: [37/1000] || Step: [0/188] || Average Validation Loss: 2.8639\n",
      "****************************************************************************************************\n",
      "Epoch: [37/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 10.660870\n",
      "****************************************************************************************************\n",
      "Epoch: [38/1000] || Step: [0/1563] || Average Training Loss: 3.1685\n",
      "Epoch: [38/1000] || Step: [500/1563] || Average Training Loss: 3.0526\n",
      "Epoch: [38/1000] || Step: [1000/1563] || Average Training Loss: 3.0694\n",
      "Epoch: [38/1000] || Step: [1500/1563] || Average Training Loss: 3.0728\n",
      "Epoch: [38/1000] || Step: [0/188] || Average Validation Loss: 3.2655\n",
      "****************************************************************************************************\n",
      "Epoch: [38/1000] || Training Loss = 3.07 || Validation Loss: 3.01 || Time: 10.835480\n",
      "****************************************************************************************************\n",
      "Epoch: [39/1000] || Step: [0/1563] || Average Training Loss: 3.0977\n",
      "Epoch: [39/1000] || Step: [500/1563] || Average Training Loss: 3.0667\n",
      "Epoch: [39/1000] || Step: [1000/1563] || Average Training Loss: 3.0734\n",
      "Epoch: [39/1000] || Step: [1500/1563] || Average Training Loss: 3.0717\n",
      "Epoch: [39/1000] || Step: [0/188] || Average Validation Loss: 3.0092\n",
      "****************************************************************************************************\n",
      "Epoch: [39/1000] || Training Loss = 3.07 || Validation Loss: 3.01 || Time: 10.886263\n",
      "****************************************************************************************************\n",
      "Epoch: [40/1000] || Step: [0/1563] || Average Training Loss: 2.9580\n",
      "Epoch: [40/1000] || Step: [500/1563] || Average Training Loss: 3.0557\n",
      "Epoch: [40/1000] || Step: [1000/1563] || Average Training Loss: 3.0694\n",
      "Epoch: [40/1000] || Step: [1500/1563] || Average Training Loss: 3.0719\n",
      "Epoch: [40/1000] || Step: [0/188] || Average Validation Loss: 2.7414\n",
      "****************************************************************************************************\n",
      "Epoch: [40/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 11.069361\n",
      "****************************************************************************************************\n",
      "Epoch: [41/1000] || Step: [0/1563] || Average Training Loss: 3.3347\n",
      "Epoch: [41/1000] || Step: [500/1563] || Average Training Loss: 3.0690\n",
      "Epoch: [41/1000] || Step: [1000/1563] || Average Training Loss: 3.0715\n",
      "Epoch: [41/1000] || Step: [1500/1563] || Average Training Loss: 3.0718\n",
      "Epoch: [41/1000] || Step: [0/188] || Average Validation Loss: 2.9952\n",
      "****************************************************************************************************\n",
      "Epoch: [41/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 10.879560\n",
      "****************************************************************************************************\n",
      "Epoch: [42/1000] || Step: [0/1563] || Average Training Loss: 3.1593\n",
      "Epoch: [42/1000] || Step: [500/1563] || Average Training Loss: 3.0623\n",
      "Epoch: [42/1000] || Step: [1000/1563] || Average Training Loss: 3.0670\n",
      "Epoch: [42/1000] || Step: [1500/1563] || Average Training Loss: 3.0692\n",
      "Epoch: [42/1000] || Step: [0/188] || Average Validation Loss: 3.1055\n",
      "****************************************************************************************************\n",
      "Epoch: [42/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 10.621388\n",
      "****************************************************************************************************\n",
      "Epoch: [43/1000] || Step: [0/1563] || Average Training Loss: 3.2258\n",
      "Epoch: [43/1000] || Step: [500/1563] || Average Training Loss: 3.0544\n",
      "Epoch: [43/1000] || Step: [1000/1563] || Average Training Loss: 3.0580\n",
      "Epoch: [43/1000] || Step: [1500/1563] || Average Training Loss: 3.0722\n",
      "Epoch: [43/1000] || Step: [0/188] || Average Validation Loss: 3.3700\n",
      "****************************************************************************************************\n",
      "Epoch: [43/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 11.166122\n",
      "****************************************************************************************************\n",
      "Epoch: [44/1000] || Step: [0/1563] || Average Training Loss: 3.4463\n",
      "Epoch: [44/1000] || Step: [500/1563] || Average Training Loss: 3.0574\n",
      "Epoch: [44/1000] || Step: [1000/1563] || Average Training Loss: 3.0634\n",
      "Epoch: [44/1000] || Step: [1500/1563] || Average Training Loss: 3.0688\n",
      "Epoch: [44/1000] || Step: [0/188] || Average Validation Loss: 3.1531\n",
      "****************************************************************************************************\n",
      "Epoch: [44/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 11.140257\n",
      "****************************************************************************************************\n",
      "Epoch: [45/1000] || Step: [0/1563] || Average Training Loss: 2.7721\n",
      "Epoch: [45/1000] || Step: [500/1563] || Average Training Loss: 3.0638\n",
      "Epoch: [45/1000] || Step: [1500/1563] || Average Training Loss: 3.0686\n",
      "Epoch: [45/1000] || Step: [0/188] || Average Validation Loss: 2.8766\n",
      "****************************************************************************************************\n",
      "Epoch: [45/1000] || Training Loss = 3.07 || Validation Loss: 3.01 || Time: 11.289033\n",
      "****************************************************************************************************\n",
      "Epoch: [46/1000] || Step: [0/1563] || Average Training Loss: 2.6453\n",
      "Epoch: [46/1000] || Step: [500/1563] || Average Training Loss: 3.0573\n",
      "Epoch: [46/1000] || Step: [1000/1563] || Average Training Loss: 3.0675\n",
      "Epoch: [46/1000] || Step: [1500/1563] || Average Training Loss: 3.0683\n",
      "Epoch: [46/1000] || Step: [0/188] || Average Validation Loss: 3.2269\n",
      "****************************************************************************************************\n",
      "Epoch: [46/1000] || Training Loss = 3.07 || Validation Loss: 2.99 || Time: 10.927046\n",
      "****************************************************************************************************\n",
      "Epoch: [47/1000] || Step: [0/1563] || Average Training Loss: 3.2867\n",
      "Epoch: [47/1000] || Step: [500/1563] || Average Training Loss: 3.0683\n",
      "Epoch: [47/1000] || Step: [1000/1563] || Average Training Loss: 3.0653\n",
      "Epoch: [47/1000] || Step: [1500/1563] || Average Training Loss: 3.0707\n",
      "Epoch: [47/1000] || Step: [0/188] || Average Validation Loss: 2.9750\n",
      "****************************************************************************************************\n",
      "Epoch: [47/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 11.230830\n",
      "****************************************************************************************************\n",
      "Epoch: [48/1000] || Step: [0/1563] || Average Training Loss: 3.1134\n",
      "Epoch: [48/1000] || Step: [500/1563] || Average Training Loss: 3.0552\n",
      "Epoch: [48/1000] || Step: [1000/1563] || Average Training Loss: 3.0681\n",
      "Epoch: [48/1000] || Step: [1500/1563] || Average Training Loss: 3.0685\n",
      "Epoch: [48/1000] || Step: [0/188] || Average Validation Loss: 2.8893\n",
      "****************************************************************************************************\n",
      "Epoch: [48/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 10.646906\n",
      "****************************************************************************************************\n",
      "Epoch: [49/1000] || Step: [0/1563] || Average Training Loss: 3.3981\n",
      "Epoch: [49/1000] || Step: [500/1563] || Average Training Loss: 3.0631\n",
      "Epoch: [49/1000] || Step: [1000/1563] || Average Training Loss: 3.0627\n",
      "Epoch: [49/1000] || Step: [1500/1563] || Average Training Loss: 3.0685\n",
      "Epoch: [49/1000] || Step: [0/188] || Average Validation Loss: 3.1579\n",
      "Epoch: [50/1000] || Step: [500/1563] || Average Training Loss: 3.0560\n",
      "Epoch: [50/1000] || Step: [1000/1563] || Average Training Loss: 3.0683\n",
      "Epoch: [50/1000] || Step: [1500/1563] || Average Training Loss: 3.0691\n",
      "Epoch: [50/1000] || Step: [0/188] || Average Validation Loss: 2.9849\n",
      "****************************************************************************************************\n",
      "Epoch: [50/1000] || Training Loss = 3.07 || Validation Loss: 3.01 || Time: 10.670728\n",
      "****************************************************************************************************\n",
      "Epoch: [51/1000] || Step: [0/1563] || Average Training Loss: 2.5850\n",
      "Epoch: [51/1000] || Step: [500/1563] || Average Training Loss: 3.0605\n",
      "Epoch: [51/1000] || Step: [1000/1563] || Average Training Loss: 3.0672\n",
      "Epoch: [51/1000] || Step: [1500/1563] || Average Training Loss: 3.0694\n",
      "Epoch: [51/1000] || Step: [0/188] || Average Validation Loss: 3.1124\n",
      "****************************************************************************************************\n",
      "Epoch: [51/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 10.720811\n",
      "****************************************************************************************************\n",
      "Epoch: [52/1000] || Step: [0/1563] || Average Training Loss: 2.9045\n",
      "Epoch: [52/1000] || Step: [500/1563] || Average Training Loss: 3.0502\n",
      "Epoch: [52/1000] || Step: [1000/1563] || Average Training Loss: 3.0625\n",
      "Epoch: [52/1000] || Step: [1500/1563] || Average Training Loss: 3.0686\n",
      "Epoch: [52/1000] || Step: [0/188] || Average Validation Loss: 2.8470\n",
      "****************************************************************************************************\n",
      "Epoch: [52/1000] || Training Loss = 3.07 || Validation Loss: 2.98 || Time: 10.647516\n",
      "****************************************************************************************************\n",
      "Epoch: [53/1000] || Step: [0/1563] || Average Training Loss: 3.2303\n",
      "Epoch: [53/1000] || Step: [500/1563] || Average Training Loss: 3.0638\n",
      "Epoch: [53/1000] || Step: [1000/1563] || Average Training Loss: 3.0689\n",
      "Epoch: [53/1000] || Step: [1500/1563] || Average Training Loss: 3.0679\n",
      "Epoch: [53/1000] || Step: [0/188] || Average Validation Loss: 3.1823\n",
      "****************************************************************************************************\n",
      "Epoch: [53/1000] || Training Loss = 3.07 || Validation Loss: 3.01 || Time: 10.658098\n",
      "****************************************************************************************************\n",
      "Epoch: [54/1000] || Step: [0/1563] || Average Training Loss: 2.9200\n",
      "Epoch: [54/1000] || Step: [500/1563] || Average Training Loss: 3.0616\n",
      "Epoch: [54/1000] || Step: [1000/1563] || Average Training Loss: 3.0626\n",
      "Epoch: [54/1000] || Step: [1500/1563] || Average Training Loss: 3.0667\n",
      "Epoch: [54/1000] || Step: [0/188] || Average Validation Loss: 2.8796\n",
      "****************************************************************************************************\n",
      "Epoch: [54/1000] || Training Loss = 3.07 || Validation Loss: 3.00 || Time: 10.691494\n",
      "****************************************************************************************************\n",
      "Epoch: [55/1000] || Step: [0/1563] || Average Training Loss: 2.9856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m: encoder,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m'\u001b[39m: decoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m training_loss, validation_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/utils.py:74\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, criterion, optimizer, train_loader, val_loader, total_epoch, device, checkpoint_path, print_every, load_checkpoint)\u001b[0m\n\u001b[1;32m     71\u001b[0m encoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     72\u001b[0m decoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     75\u001b[0m     idx, images, captions \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     76\u001b[0m     images, captions \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), captions\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/get_loader.py:160\u001b[0m, in \u001b[0;36mMSCOCODataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# get X: Image\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# get y: Image Caption\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_deque[idx][\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/get_loader.py:138\u001b[0m, in \u001b[0;36mMSCOCODataset.load_img\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    136\u001b[0m img_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_deque[idx][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# convert the image to RGB to make sure all the images are 3D, because there are some images in grayscale\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_file_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_transforms(img)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0a2c2-8bf7-4a91-9ebb-ffe9601c1ef4",
   "metadata": {},
   "source": [
    "## try with different hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3cecb05-3032-4637-807d-ca57b87d8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params('../model/model_v2/model_v2_1_param.json', 128, 1024, 512, 3, len(train_dataset.vocab.idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e31c88d1-e10a-4b3b-a017-a28e227d509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data loader\n",
    "BATCH_SIZE = 128\n",
    "CAPS_PER_IMAGE = 5 # how many captions for each image to include in data set\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 1024 # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3 #hidden layers in LTSM\n",
    "vocab_size = len(train_dataset.vocab.idx2word)\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 1000\n",
    "CHECKPOINT = '../model/model_v2/model_v2_1.pth'\n",
    "PRINT_EVERY = 500 # run print_every batches and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef67378d-6e62-4b7c-9557-ea91b2f06802",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder_ = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e18e938c-e9cc-47bd-9290-343304a18e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder_.parameters()) + list(encoder_.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':3e-4, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03d3cbe-6ae1-4619-98ac-a3f15e3f7984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/1000] || Step: [0/1563] || Average Training Loss: 8.1291\n",
      "Epoch: [0/1000] || Step: [500/1563] || Average Training Loss: 4.5005\n",
      "Epoch: [0/1000] || Step: [1000/1563] || Average Training Loss: 4.1576\n",
      "Epoch: [0/1000] || Step: [1500/1563] || Average Training Loss: 3.9860\n",
      "Epoch: [0/1000] || Step: [0/188] || Average Validation Loss: 3.6459\n",
      "****************************************************************************************************\n",
      "Epoch: [0/1000] || Training Loss = 3.97 || Validation Loss: 3.51 || Time: 10.881993\n",
      "****************************************************************************************************\n",
      "Epoch: [1/1000] || Step: [0/1563] || Average Training Loss: 3.3882\n",
      "Epoch: [1/1000] || Step: [500/1563] || Average Training Loss: 3.5151\n",
      "Epoch: [1/1000] || Step: [1500/1563] || Average Training Loss: 3.4319\n",
      "Epoch: [1/1000] || Step: [0/188] || Average Validation Loss: 3.3433\n",
      "****************************************************************************************************\n",
      "Epoch: [1/1000] || Training Loss = 3.43 || Validation Loss: 3.22 || Time: 11.113175\n",
      "****************************************************************************************************\n",
      "Epoch: [2/1000] || Step: [0/1563] || Average Training Loss: 3.0324\n",
      "Epoch: [2/1000] || Step: [500/1563] || Average Training Loss: 3.2476\n",
      "Epoch: [2/1000] || Step: [1000/1563] || Average Training Loss: 3.2207\n",
      "Epoch: [2/1000] || Step: [1500/1563] || Average Training Loss: 3.2046\n",
      "Epoch: [2/1000] || Step: [0/188] || Average Validation Loss: 3.1534\n",
      "****************************************************************************************************\n",
      "Epoch: [2/1000] || Training Loss = 3.20 || Validation Loss: 3.06 || Time: 10.553092\n",
      "****************************************************************************************************\n",
      "Epoch: [3/1000] || Step: [0/1563] || Average Training Loss: 3.2479\n",
      "Epoch: [3/1000] || Step: [500/1563] || Average Training Loss: 3.1143\n",
      "Epoch: [3/1000] || Step: [1000/1563] || Average Training Loss: 3.1070\n",
      "Epoch: [3/1000] || Step: [1500/1563] || Average Training Loss: 3.1055\n",
      "Epoch: [3/1000] || Step: [0/188] || Average Validation Loss: 2.9755\n",
      "****************************************************************************************************\n",
      "Epoch: [3/1000] || Training Loss = 3.10 || Validation Loss: 2.99 || Time: 10.799999\n",
      "****************************************************************************************************\n",
      "Epoch: [4/1000] || Step: [0/1563] || Average Training Loss: 2.9298\n",
      "Epoch: [4/1000] || Step: [500/1563] || Average Training Loss: 3.0795\n",
      "Epoch: [4/1000] || Step: [1000/1563] || Average Training Loss: 3.0740\n",
      "Epoch: [4/1000] || Step: [1500/1563] || Average Training Loss: 3.0630\n",
      "Epoch: [4/1000] || Step: [0/188] || Average Validation Loss: 2.9998\n",
      "****************************************************************************************************\n",
      "Epoch: [4/1000] || Training Loss = 3.06 || Validation Loss: 2.98 || Time: 10.838698\n",
      "****************************************************************************************************\n",
      "Epoch: [5/1000] || Step: [0/1563] || Average Training Loss: 2.8692\n",
      "Epoch: [5/1000] || Step: [500/1563] || Average Training Loss: 3.0375\n",
      "Epoch: [5/1000] || Step: [1000/1563] || Average Training Loss: 3.0406\n",
      "Epoch: [5/1000] || Step: [1500/1563] || Average Training Loss: 3.0390\n",
      "Epoch: [5/1000] || Step: [0/188] || Average Validation Loss: 2.8464\n",
      "****************************************************************************************************\n",
      "Epoch: [5/1000] || Training Loss = 3.04 || Validation Loss: 2.94 || Time: 10.916674\n",
      "****************************************************************************************************\n",
      "Epoch: [6/1000] || Step: [0/1563] || Average Training Loss: 3.0297\n",
      "Epoch: [6/1000] || Step: [500/1563] || Average Training Loss: 3.0207\n",
      "Epoch: [6/1000] || Step: [1000/1563] || Average Training Loss: 3.0241\n",
      "Epoch: [6/1000] || Step: [1500/1563] || Average Training Loss: 3.0197\n",
      "Epoch: [6/1000] || Step: [0/188] || Average Validation Loss: 2.8228\n",
      "****************************************************************************************************\n",
      "Epoch: [6/1000] || Training Loss = 3.02 || Validation Loss: 2.94 || Time: 10.980462\n",
      "****************************************************************************************************\n",
      "Epoch: [7/1000] || Step: [0/1563] || Average Training Loss: 2.8141\n",
      "Epoch: [7/1000] || Step: [500/1563] || Average Training Loss: 3.0074\n",
      "Epoch: [7/1000] || Step: [1000/1563] || Average Training Loss: 3.0092\n",
      "Epoch: [7/1000] || Step: [1500/1563] || Average Training Loss: 3.0061\n",
      "Epoch: [7/1000] || Step: [0/188] || Average Validation Loss: 2.8889\n",
      "****************************************************************************************************\n",
      "Epoch: [7/1000] || Training Loss = 3.01 || Validation Loss: 2.93 || Time: 10.904708\n",
      "****************************************************************************************************\n",
      "Epoch: [8/1000] || Step: [0/1563] || Average Training Loss: 3.0682\n",
      "Epoch: [8/1000] || Step: [500/1563] || Average Training Loss: 2.9970\n",
      "Epoch: [8/1000] || Step: [1000/1563] || Average Training Loss: 2.9914\n",
      "Epoch: [8/1000] || Step: [1500/1563] || Average Training Loss: 2.9880\n",
      "Epoch: [8/1000] || Step: [0/188] || Average Validation Loss: 2.7970\n",
      "****************************************************************************************************\n",
      "Epoch: [8/1000] || Training Loss = 2.99 || Validation Loss: 2.91 || Time: 10.857540\n",
      "****************************************************************************************************\n",
      "Epoch: [9/1000] || Step: [0/1563] || Average Training Loss: 2.7726\n",
      "Epoch: [9/1000] || Step: [500/1563] || Average Training Loss: 2.9718\n",
      "Epoch: [9/1000] || Step: [1000/1563] || Average Training Loss: 2.9752\n",
      "Epoch: [9/1000] || Step: [1500/1563] || Average Training Loss: 2.9760\n",
      "Epoch: [9/1000] || Step: [0/188] || Average Validation Loss: 3.0135\n",
      "****************************************************************************************************\n",
      "Epoch: [9/1000] || Training Loss = 2.98 || Validation Loss: 2.90 || Time: 10.967340\n",
      "****************************************************************************************************\n",
      "Epoch: [10/1000] || Step: [0/1563] || Average Training Loss: 3.0534\n",
      "Epoch: [10/1000] || Step: [500/1563] || Average Training Loss: 2.9551\n",
      "Epoch: [10/1000] || Step: [1000/1563] || Average Training Loss: 2.9588\n",
      "Epoch: [10/1000] || Step: [1500/1563] || Average Training Loss: 2.9600\n",
      "Epoch: [10/1000] || Step: [0/188] || Average Validation Loss: 3.0653\n",
      "****************************************************************************************************\n",
      "Epoch: [10/1000] || Training Loss = 2.96 || Validation Loss: 2.88 || Time: 11.414180\n",
      "****************************************************************************************************\n",
      "Epoch: [11/1000] || Step: [0/1563] || Average Training Loss: 3.1433\n",
      "Epoch: [11/1000] || Step: [500/1563] || Average Training Loss: 2.9522\n",
      "Epoch: [11/1000] || Step: [1000/1563] || Average Training Loss: 2.9508\n",
      "Epoch: [11/1000] || Step: [1500/1563] || Average Training Loss: 2.9512\n",
      "Epoch: [11/1000] || Step: [0/188] || Average Validation Loss: 2.9582\n",
      "****************************************************************************************************\n",
      "Epoch: [11/1000] || Training Loss = 2.95 || Validation Loss: 2.87 || Time: 11.073924\n",
      "****************************************************************************************************\n",
      "Epoch: [12/1000] || Step: [0/1563] || Average Training Loss: 3.1410\n",
      "Epoch: [12/1000] || Step: [500/1563] || Average Training Loss: 2.9208\n",
      "Epoch: [12/1000] || Step: [1000/1563] || Average Training Loss: 2.9360\n",
      "Epoch: [12/1000] || Step: [1500/1563] || Average Training Loss: 2.9399\n",
      "Epoch: [12/1000] || Step: [0/188] || Average Validation Loss: 2.6331\n",
      "****************************************************************************************************\n",
      "Epoch: [12/1000] || Training Loss = 2.94 || Validation Loss: 2.89 || Time: 11.170306\n",
      "****************************************************************************************************\n",
      "Epoch: [13/1000] || Step: [0/1563] || Average Training Loss: 3.0219\n",
      "Epoch: [13/1000] || Step: [500/1563] || Average Training Loss: 2.9241\n",
      "Epoch: [13/1000] || Step: [1000/1563] || Average Training Loss: 2.9216\n",
      "Epoch: [13/1000] || Step: [1500/1563] || Average Training Loss: 2.9292\n",
      "Epoch: [13/1000] || Step: [0/188] || Average Validation Loss: 2.7647\n",
      "****************************************************************************************************\n",
      "Epoch: [13/1000] || Training Loss = 2.93 || Validation Loss: 2.86 || Time: 10.897432\n",
      "****************************************************************************************************\n",
      "Epoch: [14/1000] || Step: [0/1563] || Average Training Loss: 3.0893\n",
      "Epoch: [14/1000] || Step: [500/1563] || Average Training Loss: 2.9087\n",
      "Epoch: [14/1000] || Step: [1000/1563] || Average Training Loss: 2.9161\n",
      "Epoch: [14/1000] || Step: [1500/1563] || Average Training Loss: 2.9218\n",
      "Epoch: [14/1000] || Step: [0/188] || Average Validation Loss: 2.7540\n",
      "****************************************************************************************************\n",
      "Epoch: [14/1000] || Training Loss = 2.92 || Validation Loss: 2.85 || Time: 10.813676\n",
      "****************************************************************************************************\n",
      "Epoch: [15/1000] || Step: [0/1563] || Average Training Loss: 2.7973\n",
      "Epoch: [15/1000] || Step: [500/1563] || Average Training Loss: 2.9094\n",
      "Epoch: [15/1000] || Step: [1000/1563] || Average Training Loss: 2.9081\n",
      "Epoch: [15/1000] || Step: [1500/1563] || Average Training Loss: 2.9125\n",
      "Epoch: [15/1000] || Step: [0/188] || Average Validation Loss: 3.2314\n",
      "****************************************************************************************************\n",
      "Epoch: [15/1000] || Training Loss = 2.91 || Validation Loss: 2.83 || Time: 11.049912\n",
      "****************************************************************************************************\n",
      "Epoch: [16/1000] || Step: [0/1563] || Average Training Loss: 2.8508\n",
      "Epoch: [16/1000] || Step: [500/1563] || Average Training Loss: 2.9053\n",
      "Epoch: [16/1000] || Step: [1000/1563] || Average Training Loss: 2.9050\n",
      "Epoch: [16/1000] || Step: [1500/1563] || Average Training Loss: 2.9109\n",
      "Epoch: [16/1000] || Step: [0/188] || Average Validation Loss: 2.8070\n",
      "****************************************************************************************************\n",
      "Epoch: [16/1000] || Training Loss = 2.91 || Validation Loss: 2.83 || Time: 11.372610\n",
      "****************************************************************************************************\n",
      "Epoch: [17/1000] || Step: [0/1563] || Average Training Loss: 2.8165\n",
      "Epoch: [17/1000] || Step: [500/1563] || Average Training Loss: 2.8995\n",
      "Epoch: [17/1000] || Step: [1000/1563] || Average Training Loss: 2.9019\n",
      "Epoch: [17/1000] || Step: [1500/1563] || Average Training Loss: 2.9041\n",
      "Epoch: [17/1000] || Step: [0/188] || Average Validation Loss: 2.8944\n",
      "****************************************************************************************************\n",
      "Epoch: [17/1000] || Training Loss = 2.90 || Validation Loss: 2.83 || Time: 10.776036\n",
      "****************************************************************************************************\n",
      "Epoch: [18/1000] || Step: [0/1563] || Average Training Loss: 2.5364\n",
      "Epoch: [18/1000] || Step: [500/1563] || Average Training Loss: 2.8946\n",
      "Epoch: [18/1000] || Step: [1000/1563] || Average Training Loss: 2.8917\n",
      "Epoch: [18/1000] || Step: [1500/1563] || Average Training Loss: 2.8975\n",
      "Epoch: [18/1000] || Step: [0/188] || Average Validation Loss: 2.7627\n",
      "****************************************************************************************************\n",
      "Epoch: [18/1000] || Training Loss = 2.90 || Validation Loss: 2.82 || Time: 10.961276\n",
      "****************************************************************************************************\n",
      "Epoch: [19/1000] || Step: [0/1563] || Average Training Loss: 2.9666\n",
      "Epoch: [19/1000] || Step: [500/1563] || Average Training Loss: 2.8905\n",
      "Epoch: [19/1000] || Step: [1000/1563] || Average Training Loss: 2.8969\n",
      "Epoch: [19/1000] || Step: [1500/1563] || Average Training Loss: 2.8942\n",
      "Epoch: [19/1000] || Step: [0/188] || Average Validation Loss: 2.6852\n",
      "****************************************************************************************************\n",
      "Epoch: [19/1000] || Training Loss = 2.89 || Validation Loss: 2.84 || Time: 10.816030\n",
      "****************************************************************************************************\n",
      "Epoch: [20/1000] || Step: [0/1563] || Average Training Loss: 2.8308\n",
      "Epoch: [20/1000] || Step: [500/1563] || Average Training Loss: 2.8964\n",
      "Epoch: [20/1000] || Step: [1000/1563] || Average Training Loss: 2.8877\n",
      "Epoch: [20/1000] || Step: [1500/1563] || Average Training Loss: 2.8893\n",
      "Epoch: [20/1000] || Step: [0/188] || Average Validation Loss: 2.7672\n",
      "****************************************************************************************************\n",
      "Epoch: [20/1000] || Training Loss = 2.89 || Validation Loss: 2.83 || Time: 10.721898\n",
      "****************************************************************************************************\n",
      "Epoch: [21/1000] || Step: [0/1563] || Average Training Loss: 2.7922\n",
      "Epoch: [21/1000] || Step: [500/1563] || Average Training Loss: 2.8846\n",
      "Epoch: [21/1000] || Step: [1000/1563] || Average Training Loss: 2.8826\n",
      "Epoch: [21/1000] || Step: [1500/1563] || Average Training Loss: 2.8847\n",
      "Epoch: [21/1000] || Step: [0/188] || Average Validation Loss: 2.5749\n",
      "****************************************************************************************************\n",
      "Epoch: [21/1000] || Training Loss = 2.89 || Validation Loss: 2.80 || Time: 11.107235\n",
      "****************************************************************************************************\n",
      "Epoch: [22/1000] || Step: [0/1563] || Average Training Loss: 2.7925\n",
      "Epoch: [22/1000] || Step: [500/1563] || Average Training Loss: 2.8765\n",
      "Epoch: [22/1000] || Step: [1000/1563] || Average Training Loss: 2.8825\n",
      "Epoch: [22/1000] || Step: [1500/1563] || Average Training Loss: 2.8825\n",
      "Epoch: [22/1000] || Step: [0/188] || Average Validation Loss: 3.0334\n",
      "****************************************************************************************************\n",
      "Epoch: [22/1000] || Training Loss = 2.88 || Validation Loss: 2.82 || Time: 10.830268\n",
      "****************************************************************************************************\n",
      "Epoch: [23/1000] || Step: [0/1563] || Average Training Loss: 2.8357\n",
      "Epoch: [23/1000] || Step: [500/1563] || Average Training Loss: 2.8786\n",
      "Epoch: [23/1000] || Step: [1000/1563] || Average Training Loss: 2.8796\n",
      "Epoch: [23/1000] || Step: [1500/1563] || Average Training Loss: 2.8804\n",
      "Epoch: [23/1000] || Step: [0/188] || Average Validation Loss: 2.7821\n",
      "****************************************************************************************************\n",
      "Epoch: [23/1000] || Training Loss = 2.88 || Validation Loss: 2.82 || Time: 10.843539\n",
      "****************************************************************************************************\n",
      "Epoch: [24/1000] || Step: [0/1563] || Average Training Loss: 2.9753\n",
      "Epoch: [24/1000] || Step: [500/1563] || Average Training Loss: 2.8664\n",
      "Epoch: [24/1000] || Step: [1000/1563] || Average Training Loss: 2.8711\n",
      "Epoch: [24/1000] || Step: [1500/1563] || Average Training Loss: 2.8761\n",
      "Epoch: [24/1000] || Step: [0/188] || Average Validation Loss: 2.6375\n",
      "****************************************************************************************************\n",
      "Epoch: [24/1000] || Training Loss = 2.88 || Validation Loss: 2.82 || Time: 10.808923\n",
      "****************************************************************************************************\n",
      "Epoch: [25/1000] || Step: [0/1563] || Average Training Loss: 2.8525\n",
      "Epoch: [25/1000] || Step: [500/1563] || Average Training Loss: 2.8560\n",
      "Epoch: [25/1000] || Step: [1000/1563] || Average Training Loss: 2.8666\n",
      "Epoch: [25/1000] || Step: [1500/1563] || Average Training Loss: 2.8706\n",
      "Epoch: [25/1000] || Step: [0/188] || Average Validation Loss: 2.5809\n",
      "****************************************************************************************************\n",
      "Epoch: [25/1000] || Training Loss = 2.87 || Validation Loss: 2.82 || Time: 10.837935\n",
      "****************************************************************************************************\n",
      "Epoch: [26/1000] || Step: [0/1563] || Average Training Loss: 2.8180\n",
      "Epoch: [26/1000] || Step: [500/1563] || Average Training Loss: 2.8540\n",
      "Epoch: [26/1000] || Step: [1000/1563] || Average Training Loss: 2.8631\n",
      "Epoch: [26/1000] || Step: [1500/1563] || Average Training Loss: 2.8695\n",
      "Epoch: [26/1000] || Step: [0/188] || Average Validation Loss: 2.6730\n",
      "****************************************************************************************************\n",
      "Epoch: [26/1000] || Training Loss = 2.87 || Validation Loss: 2.80 || Time: 10.807699\n",
      "****************************************************************************************************\n",
      "Epoch: [27/1000] || Step: [0/1563] || Average Training Loss: 2.9624\n",
      "Epoch: [27/1000] || Step: [500/1563] || Average Training Loss: 2.8544\n",
      "Epoch: [27/1000] || Step: [1000/1563] || Average Training Loss: 2.8639\n",
      "Epoch: [27/1000] || Step: [1500/1563] || Average Training Loss: 2.8673\n",
      "Epoch: [27/1000] || Step: [0/188] || Average Validation Loss: 3.0946\n",
      "****************************************************************************************************\n",
      "Epoch: [27/1000] || Training Loss = 2.87 || Validation Loss: 2.81 || Time: 10.853537\n",
      "****************************************************************************************************\n",
      "Epoch: [28/1000] || Step: [0/1563] || Average Training Loss: 2.6561\n",
      "Epoch: [28/1000] || Step: [500/1563] || Average Training Loss: 2.8580\n",
      "Epoch: [28/1000] || Step: [1000/1563] || Average Training Loss: 2.8614\n",
      "Epoch: [28/1000] || Step: [1500/1563] || Average Training Loss: 2.8640\n",
      "Epoch: [28/1000] || Step: [0/188] || Average Validation Loss: 2.8835\n",
      "****************************************************************************************************\n",
      "Epoch: [28/1000] || Training Loss = 2.87 || Validation Loss: 2.81 || Time: 10.689964\n",
      "****************************************************************************************************\n",
      "Epoch: [29/1000] || Step: [0/1563] || Average Training Loss: 2.9893\n",
      "Epoch: [29/1000] || Step: [500/1563] || Average Training Loss: 2.8597\n",
      "Epoch: [29/1000] || Step: [1000/1563] || Average Training Loss: 2.8646\n",
      "Epoch: [29/1000] || Step: [1500/1563] || Average Training Loss: 2.8615\n",
      "Epoch: [29/1000] || Step: [0/188] || Average Validation Loss: 2.8612\n",
      "****************************************************************************************************\n",
      "Epoch: [29/1000] || Training Loss = 2.86 || Validation Loss: 2.82 || Time: 11.031590\n",
      "****************************************************************************************************\n",
      "Epoch: [30/1000] || Step: [0/1563] || Average Training Loss: 2.9606\n",
      "Epoch: [30/1000] || Step: [500/1563] || Average Training Loss: 2.8586\n",
      "Epoch: [30/1000] || Step: [1000/1563] || Average Training Loss: 2.8607\n",
      "Epoch: [30/1000] || Step: [1500/1563] || Average Training Loss: 2.8608\n",
      "Epoch: [30/1000] || Step: [0/188] || Average Validation Loss: 2.7529\n",
      "****************************************************************************************************\n",
      "Epoch: [30/1000] || Training Loss = 2.86 || Validation Loss: 2.80 || Time: 10.481655\n",
      "****************************************************************************************************\n",
      "Epoch: [31/1000] || Step: [0/1563] || Average Training Loss: 2.9582\n",
      "Epoch: [31/1000] || Step: [500/1563] || Average Training Loss: 2.8530\n",
      "Epoch: [31/1000] || Step: [1000/1563] || Average Training Loss: 2.8526\n",
      "Epoch: [31/1000] || Step: [1500/1563] || Average Training Loss: 2.8588\n",
      "Epoch: [31/1000] || Step: [0/188] || Average Validation Loss: 2.8182\n",
      "****************************************************************************************************\n",
      "Epoch: [31/1000] || Training Loss = 2.86 || Validation Loss: 2.81 || Time: 10.780980\n",
      "****************************************************************************************************\n",
      "Epoch: [32/1000] || Step: [0/1563] || Average Training Loss: 3.1162\n",
      "Epoch: [32/1000] || Step: [500/1563] || Average Training Loss: 2.8490\n",
      "Epoch: [32/1000] || Step: [1000/1563] || Average Training Loss: 2.8565\n",
      "Epoch: [32/1000] || Step: [1500/1563] || Average Training Loss: 2.8598\n",
      "Epoch: [32/1000] || Step: [0/188] || Average Validation Loss: 2.8053\n",
      "****************************************************************************************************\n",
      "Epoch: [32/1000] || Training Loss = 2.86 || Validation Loss: 2.80 || Time: 10.799081\n",
      "****************************************************************************************************\n",
      "Epoch: [33/1000] || Step: [0/1563] || Average Training Loss: 2.9321\n",
      "Epoch: [33/1000] || Step: [500/1563] || Average Training Loss: 2.8487\n",
      "Epoch: [33/1000] || Step: [1000/1563] || Average Training Loss: 2.8507\n",
      "Epoch: [33/1000] || Step: [1500/1563] || Average Training Loss: 2.8534\n",
      "Epoch: [33/1000] || Step: [0/188] || Average Validation Loss: 2.7176\n",
      "****************************************************************************************************\n",
      "Epoch: [33/1000] || Training Loss = 2.85 || Validation Loss: 2.79 || Time: 10.883725\n",
      "****************************************************************************************************\n",
      "Epoch: [34/1000] || Step: [0/1563] || Average Training Loss: 2.6422\n",
      "Epoch: [34/1000] || Step: [500/1563] || Average Training Loss: 2.8355\n",
      "Epoch: [34/1000] || Step: [1000/1563] || Average Training Loss: 2.8474\n",
      "Epoch: [34/1000] || Step: [1500/1563] || Average Training Loss: 2.8517\n",
      "Epoch: [34/1000] || Step: [0/188] || Average Validation Loss: 2.6837\n",
      "****************************************************************************************************\n",
      "Epoch: [34/1000] || Training Loss = 2.85 || Validation Loss: 2.81 || Time: 10.903755\n",
      "****************************************************************************************************\n",
      "Epoch: [35/1000] || Step: [0/1563] || Average Training Loss: 2.9548\n",
      "Epoch: [35/1000] || Step: [500/1563] || Average Training Loss: 2.8328\n",
      "Epoch: [35/1000] || Step: [1000/1563] || Average Training Loss: 2.8464\n",
      "Epoch: [35/1000] || Step: [1500/1563] || Average Training Loss: 2.8507\n",
      "Epoch: [35/1000] || Step: [0/188] || Average Validation Loss: 2.5843\n",
      "****************************************************************************************************\n",
      "Epoch: [35/1000] || Training Loss = 2.85 || Validation Loss: 2.80 || Time: 11.627357\n",
      "****************************************************************************************************\n",
      "Epoch: [36/1000] || Step: [0/1563] || Average Training Loss: 2.8066\n",
      "Epoch: [36/1000] || Step: [500/1563] || Average Training Loss: 2.8403\n",
      "Epoch: [36/1000] || Step: [1000/1563] || Average Training Loss: 2.8504\n",
      "Epoch: [36/1000] || Step: [1500/1563] || Average Training Loss: 2.8501\n",
      "Epoch: [36/1000] || Step: [0/188] || Average Validation Loss: 2.7950\n",
      "****************************************************************************************************\n",
      "Epoch: [36/1000] || Training Loss = 2.85 || Validation Loss: 2.82 || Time: 12.427204\n",
      "****************************************************************************************************\n",
      "Epoch: [37/1000] || Step: [0/1563] || Average Training Loss: 2.9634\n",
      "Epoch: [37/1000] || Step: [500/1563] || Average Training Loss: 2.8382\n",
      "Epoch: [37/1000] || Step: [1000/1563] || Average Training Loss: 2.8436\n",
      "Epoch: [37/1000] || Step: [1500/1563] || Average Training Loss: 2.8480\n",
      "Epoch: [37/1000] || Step: [0/188] || Average Validation Loss: 2.8359\n",
      "****************************************************************************************************\n",
      "Epoch: [37/1000] || Training Loss = 2.85 || Validation Loss: 2.80 || Time: 10.915771\n",
      "****************************************************************************************************\n",
      "Epoch: [38/1000] || Step: [0/1563] || Average Training Loss: 2.8422\n",
      "Epoch: [38/1000] || Step: [500/1563] || Average Training Loss: 2.8430\n",
      "Epoch: [38/1000] || Step: [1000/1563] || Average Training Loss: 2.8442\n",
      "Epoch: [38/1000] || Step: [1500/1563] || Average Training Loss: 2.8468\n",
      "Epoch: [38/1000] || Step: [0/188] || Average Validation Loss: 2.9270\n",
      "****************************************************************************************************\n",
      "Epoch: [38/1000] || Training Loss = 2.85 || Validation Loss: 2.81 || Time: 11.122869\n",
      "****************************************************************************************************\n",
      "Epoch: [39/1000] || Step: [0/1563] || Average Training Loss: 2.8564\n",
      "Epoch: [39/1000] || Step: [500/1563] || Average Training Loss: 2.8410\n",
      "Epoch: [39/1000] || Step: [1000/1563] || Average Training Loss: 2.8442\n",
      "Epoch: [39/1000] || Step: [1500/1563] || Average Training Loss: 2.8450\n",
      "Epoch: [39/1000] || Step: [0/188] || Average Validation Loss: 3.0693\n",
      "****************************************************************************************************\n",
      "Epoch: [39/1000] || Training Loss = 2.85 || Validation Loss: 2.79 || Time: 10.860529\n",
      "****************************************************************************************************\n",
      "Epoch: [40/1000] || Step: [0/1563] || Average Training Loss: 2.6487\n",
      "Epoch: [40/1000] || Step: [500/1563] || Average Training Loss: 2.8382\n",
      "Epoch: [40/1000] || Step: [1000/1563] || Average Training Loss: 2.8340\n",
      "Epoch: [40/1000] || Step: [1500/1563] || Average Training Loss: 2.8432\n",
      "Epoch: [40/1000] || Step: [0/188] || Average Validation Loss: 2.7794\n",
      "****************************************************************************************************\n",
      "Epoch: [40/1000] || Training Loss = 2.84 || Validation Loss: 2.80 || Time: 10.963843\n",
      "****************************************************************************************************\n",
      "Epoch: [41/1000] || Step: [0/1563] || Average Training Loss: 2.9557\n",
      "Epoch: [41/1000] || Step: [500/1563] || Average Training Loss: 2.8372\n",
      "Epoch: [41/1000] || Step: [1000/1563] || Average Training Loss: 2.8436\n",
      "Epoch: [41/1000] || Step: [1500/1563] || Average Training Loss: 2.8425\n",
      "Epoch: [41/1000] || Step: [0/188] || Average Validation Loss: 2.8920\n",
      "****************************************************************************************************\n",
      "Epoch: [41/1000] || Training Loss = 2.84 || Validation Loss: 2.79 || Time: 10.756384\n",
      "****************************************************************************************************\n",
      "Epoch: [42/1000] || Step: [0/1563] || Average Training Loss: 2.8396\n",
      "Epoch: [42/1000] || Step: [500/1563] || Average Training Loss: 2.8328\n",
      "Epoch: [42/1000] || Step: [1000/1563] || Average Training Loss: 2.8397\n",
      "Epoch: [42/1000] || Step: [1500/1563] || Average Training Loss: 2.8439\n",
      "Epoch: [42/1000] || Step: [0/188] || Average Validation Loss: 2.7778\n",
      "****************************************************************************************************\n",
      "Epoch: [42/1000] || Training Loss = 2.84 || Validation Loss: 2.79 || Time: 10.771615\n",
      "****************************************************************************************************\n",
      "Epoch: [43/1000] || Step: [0/1563] || Average Training Loss: 3.0257\n",
      "Epoch: [43/1000] || Step: [500/1563] || Average Training Loss: 2.8343\n",
      "Epoch: [43/1000] || Step: [1000/1563] || Average Training Loss: 2.8367\n",
      "Epoch: [43/1000] || Step: [1500/1563] || Average Training Loss: 2.8414\n",
      "Epoch: [43/1000] || Step: [0/188] || Average Validation Loss: 2.8133\n",
      "****************************************************************************************************\n",
      "Epoch: [43/1000] || Training Loss = 2.84 || Validation Loss: 2.79 || Time: 10.775504\n",
      "****************************************************************************************************\n",
      "Epoch: [44/1000] || Step: [0/1563] || Average Training Loss: 2.7912\n",
      "Epoch: [44/1000] || Step: [500/1563] || Average Training Loss: 2.8246\n",
      "Epoch: [44/1000] || Step: [1000/1563] || Average Training Loss: 2.8332\n",
      "Epoch: [44/1000] || Step: [1500/1563] || Average Training Loss: 2.8403\n",
      "Epoch: [44/1000] || Step: [0/188] || Average Validation Loss: 2.5424\n",
      "****************************************************************************************************\n",
      "Epoch: [44/1000] || Training Loss = 2.84 || Validation Loss: 2.79 || Time: 10.878572\n",
      "****************************************************************************************************\n",
      "Epoch: [45/1000] || Step: [0/1563] || Average Training Loss: 3.0581\n",
      "Epoch: [45/1000] || Step: [500/1563] || Average Training Loss: 2.8245\n",
      "Epoch: [45/1000] || Step: [1000/1563] || Average Training Loss: 2.8322\n",
      "****************************************************************************************************\n",
      "Epoch: [45/1000] || Training Loss = 2.84 || Validation Loss: 2.79 || Time: 10.684285\n",
      "****************************************************************************************************\n",
      "Epoch: [46/1000] || Step: [0/1563] || Average Training Loss: 2.7093\n",
      "Epoch: [46/1000] || Step: [500/1563] || Average Training Loss: 2.8269\n",
      "Epoch: [46/1000] || Step: [1000/1563] || Average Training Loss: 2.8337\n",
      "Epoch: [46/1000] || Step: [1500/1563] || Average Training Loss: 2.8380\n",
      "Epoch: [46/1000] || Step: [0/188] || Average Validation Loss: 2.3670\n",
      "****************************************************************************************************\n",
      "Epoch: [46/1000] || Training Loss = 2.84 || Validation Loss: 2.79 || Time: 10.636222\n",
      "****************************************************************************************************\n",
      "Epoch: [47/1000] || Step: [0/1563] || Average Training Loss: 2.8618\n",
      "Epoch: [47/1000] || Step: [500/1563] || Average Training Loss: 2.8320\n",
      "Epoch: [47/1000] || Step: [1000/1563] || Average Training Loss: 2.8314\n",
      "Epoch: [47/1000] || Step: [1500/1563] || Average Training Loss: 2.8369\n",
      "Epoch: [47/1000] || Step: [0/188] || Average Validation Loss: 2.9229\n",
      "****************************************************************************************************\n",
      "Epoch: [47/1000] || Training Loss = 2.84 || Validation Loss: 2.78 || Time: 10.609601\n",
      "****************************************************************************************************\n",
      "Epoch: [48/1000] || Step: [0/1563] || Average Training Loss: 3.0317\n",
      "Epoch: [48/1000] || Step: [500/1563] || Average Training Loss: 2.8281\n",
      "Epoch: [48/1000] || Step: [1000/1563] || Average Training Loss: 2.8326\n",
      "Epoch: [48/1000] || Step: [1500/1563] || Average Training Loss: 2.8392\n",
      "Epoch: [48/1000] || Step: [0/188] || Average Validation Loss: 2.7130\n",
      "****************************************************************************************************\n",
      "Epoch: [48/1000] || Training Loss = 2.84 || Validation Loss: 2.78 || Time: 10.620267\n",
      "****************************************************************************************************\n",
      "Epoch: [49/1000] || Step: [0/1563] || Average Training Loss: 2.6528\n",
      "Epoch: [49/1000] || Step: [500/1563] || Average Training Loss: 2.8294\n",
      "Epoch: [49/1000] || Step: [1000/1563] || Average Training Loss: 2.8300\n",
      "Epoch: [49/1000] || Step: [1500/1563] || Average Training Loss: 2.8346\n",
      "Epoch: [49/1000] || Step: [0/188] || Average Validation Loss: 2.9371\n",
      "****************************************************************************************************\n",
      "Epoch: [49/1000] || Training Loss = 2.84 || Validation Loss: 2.78 || Time: 10.868046\n",
      "****************************************************************************************************\n",
      "Epoch: [50/1000] || Step: [0/1563] || Average Training Loss: 2.6754\n",
      "Epoch: [50/1000] || Step: [500/1563] || Average Training Loss: 2.8152\n",
      "Epoch: [50/1000] || Step: [1000/1563] || Average Training Loss: 2.8292\n",
      "Epoch: [50/1000] || Step: [1500/1563] || Average Training Loss: 2.8344\n",
      "Epoch: [50/1000] || Step: [0/188] || Average Validation Loss: 2.7672\n",
      "****************************************************************************************************\n",
      "Epoch: [50/1000] || Training Loss = 2.83 || Validation Loss: 2.79 || Time: 10.681017\n",
      "****************************************************************************************************\n",
      "Epoch: [51/1000] || Step: [0/1563] || Average Training Loss: 2.5863\n",
      "Epoch: [51/1000] || Step: [500/1563] || Average Training Loss: 2.8142\n",
      "Epoch: [51/1000] || Step: [1000/1563] || Average Training Loss: 2.8261\n",
      "Epoch: [51/1000] || Step: [1500/1563] || Average Training Loss: 2.8329\n",
      "Epoch: [51/1000] || Step: [0/188] || Average Validation Loss: 2.9493\n",
      "****************************************************************************************************\n",
      "Epoch: [51/1000] || Training Loss = 2.83 || Validation Loss: 2.78 || Time: 10.790009\n",
      "****************************************************************************************************\n",
      "Epoch: [52/1000] || Step: [0/1563] || Average Training Loss: 2.7021\n",
      "Epoch: [52/1000] || Step: [500/1563] || Average Training Loss: 2.8080\n",
      "Epoch: [52/1000] || Step: [1000/1563] || Average Training Loss: 2.8275\n",
      "Epoch: [52/1000] || Step: [1500/1563] || Average Training Loss: 2.8327\n",
      "Epoch: [52/1000] || Step: [0/188] || Average Validation Loss: 2.5928\n",
      "****************************************************************************************************\n",
      "Epoch: [52/1000] || Training Loss = 2.83 || Validation Loss: 2.79 || Time: 10.774584\n",
      "****************************************************************************************************\n",
      "Epoch: [53/1000] || Step: [0/1563] || Average Training Loss: 2.9070\n",
      "Epoch: [53/1000] || Step: [500/1563] || Average Training Loss: 2.8310\n",
      "Epoch: [53/1000] || Step: [1000/1563] || Average Training Loss: 2.8316\n",
      "Epoch: [53/1000] || Step: [1500/1563] || Average Training Loss: 2.8324\n",
      "Epoch: [53/1000] || Step: [0/188] || Average Validation Loss: 2.8467\n",
      "****************************************************************************************************\n",
      "Epoch: [53/1000] || Training Loss = 2.83 || Validation Loss: 2.80 || Time: 10.765605\n",
      "****************************************************************************************************\n",
      "Epoch: [54/1000] || Step: [0/1563] || Average Training Loss: 2.8812\n",
      "Epoch: [54/1000] || Step: [500/1563] || Average Training Loss: 2.8163\n",
      "Epoch: [54/1000] || Step: [1000/1563] || Average Training Loss: 2.8267\n",
      "Epoch: [54/1000] || Step: [1500/1563] || Average Training Loss: 2.8315\n",
      "Epoch: [54/1000] || Step: [0/188] || Average Validation Loss: 2.6165\n",
      "****************************************************************************************************\n",
      "Epoch: [54/1000] || Training Loss = 2.83 || Validation Loss: 2.79 || Time: 10.801043\n",
      "****************************************************************************************************\n",
      "Epoch: [55/1000] || Step: [0/1563] || Average Training Loss: 2.7270\n",
      "Epoch: [55/1000] || Step: [500/1563] || Average Training Loss: 2.8168\n",
      "Epoch: [55/1000] || Step: [1000/1563] || Average Training Loss: 2.8306\n",
      "Epoch: [55/1000] || Step: [1500/1563] || Average Training Loss: 2.8299\n",
      "Epoch: [55/1000] || Step: [0/188] || Average Validation Loss: 2.8131\n",
      "****************************************************************************************************\n",
      "Epoch: [55/1000] || Training Loss = 2.83 || Validation Loss: 2.77 || Time: 10.768478\n",
      "****************************************************************************************************\n",
      "Epoch: [56/1000] || Step: [0/1563] || Average Training Loss: 2.4829\n",
      "Epoch: [56/1000] || Step: [500/1563] || Average Training Loss: 2.8176\n",
      "Epoch: [56/1000] || Step: [1000/1563] || Average Training Loss: 2.8221\n",
      "Epoch: [56/1000] || Step: [1500/1563] || Average Training Loss: 2.8302\n",
      "Epoch: [56/1000] || Step: [0/188] || Average Validation Loss: 2.7896\n",
      "****************************************************************************************************\n",
      "Epoch: [56/1000] || Training Loss = 2.83 || Validation Loss: 2.77 || Time: 10.826862\n",
      "****************************************************************************************************\n",
      "Epoch: [57/1000] || Step: [0/1563] || Average Training Loss: 2.5901\n",
      "Epoch: [57/1000] || Step: [500/1563] || Average Training Loss: 2.8228\n",
      "Epoch: [57/1000] || Step: [1000/1563] || Average Training Loss: 2.8269\n",
      "Epoch: [57/1000] || Step: [1500/1563] || Average Training Loss: 2.8297\n",
      "Epoch: [57/1000] || Step: [0/188] || Average Validation Loss: 2.7406\n",
      "****************************************************************************************************\n",
      "Epoch: [57/1000] || Training Loss = 2.83 || Validation Loss: 2.77 || Time: 12.698827\n",
      "****************************************************************************************************\n",
      "Epoch: [58/1000] || Step: [0/1563] || Average Training Loss: 2.7426\n",
      "Epoch: [58/1000] || Step: [500/1563] || Average Training Loss: 2.8055\n",
      "Epoch: [58/1000] || Step: [1000/1563] || Average Training Loss: 2.8185\n",
      "Epoch: [58/1000] || Step: [1500/1563] || Average Training Loss: 2.8297\n",
      "Epoch: [58/1000] || Step: [0/188] || Average Validation Loss: 2.5744\n",
      "****************************************************************************************************\n",
      "Epoch: [58/1000] || Training Loss = 2.83 || Validation Loss: 2.78 || Time: 12.611680\n",
      "****************************************************************************************************\n",
      "Epoch: [59/1000] || Step: [0/1563] || Average Training Loss: 2.9831\n",
      "Epoch: [59/1000] || Step: [500/1563] || Average Training Loss: 2.8279\n",
      "Epoch: [59/1000] || Step: [1500/1563] || Average Training Loss: 2.8267\n",
      "Epoch: [59/1000] || Step: [0/188] || Average Validation Loss: 2.7945\n",
      "****************************************************************************************************\n",
      "Epoch: [59/1000] || Training Loss = 2.83 || Validation Loss: 2.78 || Time: 11.757300\n",
      "****************************************************************************************************\n",
      "Epoch: [60/1000] || Step: [0/1563] || Average Training Loss: 2.7065\n",
      "Epoch: [60/1000] || Step: [500/1563] || Average Training Loss: 2.8306\n",
      "Epoch: [60/1000] || Step: [1000/1563] || Average Training Loss: 2.8269\n",
      "Epoch: [60/1000] || Step: [1500/1563] || Average Training Loss: 2.8294\n",
      "Epoch: [60/1000] || Step: [0/188] || Average Validation Loss: 2.8086\n",
      "****************************************************************************************************\n",
      "Epoch: [60/1000] || Training Loss = 2.83 || Validation Loss: 2.78 || Time: 10.735627\n",
      "****************************************************************************************************\n",
      "Epoch: [61/1000] || Step: [0/1563] || Average Training Loss: 2.9328\n",
      "Epoch: [61/1000] || Step: [500/1563] || Average Training Loss: 2.8235\n",
      "Epoch: [61/1000] || Step: [1000/1563] || Average Training Loss: 2.8251\n",
      "Epoch: [61/1000] || Step: [1500/1563] || Average Training Loss: 2.8260\n",
      "Epoch: [61/1000] || Step: [0/188] || Average Validation Loss: 2.7192\n",
      "****************************************************************************************************\n",
      "Epoch: [61/1000] || Training Loss = 2.83 || Validation Loss: 2.78 || Time: 10.726012\n",
      "****************************************************************************************************\n",
      "Epoch: [62/1000] || Step: [0/1563] || Average Training Loss: 2.6761\n",
      "Epoch: [62/1000] || Step: [500/1563] || Average Training Loss: 2.8143\n",
      "Epoch: [62/1000] || Step: [1000/1563] || Average Training Loss: 2.8280\n",
      "Epoch: [62/1000] || Step: [1500/1563] || Average Training Loss: 2.8251\n",
      "Epoch: [62/1000] || Step: [0/188] || Average Validation Loss: 2.9334\n",
      "****************************************************************************************************\n",
      "Epoch: [62/1000] || Training Loss = 2.83 || Validation Loss: 2.79 || Time: 10.681954\n",
      "****************************************************************************************************\n",
      "Epoch: [63/1000] || Step: [0/1563] || Average Training Loss: 2.5945\n",
      "Epoch: [63/1000] || Step: [500/1563] || Average Training Loss: 2.8081\n",
      "Epoch: [63/1000] || Step: [1000/1563] || Average Training Loss: 2.8187\n",
      "Epoch: [63/1000] || Step: [0/188] || Average Validation Loss: 2.2808\n",
      "****************************************************************************************************\n",
      "Epoch: [63/1000] || Training Loss = 2.83 || Validation Loss: 2.78 || Time: 10.776330\n",
      "****************************************************************************************************\n",
      "Epoch: [64/1000] || Step: [0/1563] || Average Training Loss: 2.7776\n",
      "Epoch: [64/1000] || Step: [500/1563] || Average Training Loss: 2.8229\n",
      "Epoch: [64/1000] || Step: [1000/1563] || Average Training Loss: 2.8252\n",
      "Epoch: [64/1000] || Step: [1500/1563] || Average Training Loss: 2.8257\n",
      "Epoch: [64/1000] || Step: [0/188] || Average Validation Loss: 3.0564\n",
      "****************************************************************************************************\n",
      "Epoch: [64/1000] || Training Loss = 2.82 || Validation Loss: 2.77 || Time: 11.791102\n",
      "****************************************************************************************************\n",
      "Epoch: [65/1000] || Step: [0/1563] || Average Training Loss: 2.7270\n",
      "Epoch: [65/1000] || Step: [500/1563] || Average Training Loss: 2.8115\n",
      "Epoch: [65/1000] || Step: [1000/1563] || Average Training Loss: 2.8187\n",
      "Epoch: [65/1000] || Step: [1500/1563] || Average Training Loss: 2.8258\n",
      "Epoch: [65/1000] || Step: [0/188] || Average Validation Loss: 2.7907\n",
      "****************************************************************************************************\n",
      "Epoch: [65/1000] || Training Loss = 2.83 || Validation Loss: 2.78 || Time: 10.732460\n",
      "****************************************************************************************************\n",
      "Epoch: [66/1000] || Step: [0/1563] || Average Training Loss: 2.9143\n",
      "Epoch: [66/1000] || Step: [500/1563] || Average Training Loss: 2.8183\n",
      "Epoch: [66/1000] || Step: [1000/1563] || Average Training Loss: 2.8174\n",
      "Epoch: [66/1000] || Step: [1500/1563] || Average Training Loss: 2.8229\n",
      "Epoch: [66/1000] || Step: [0/188] || Average Validation Loss: 2.6858\n",
      "****************************************************************************************************\n",
      "Epoch: [66/1000] || Training Loss = 2.82 || Validation Loss: 2.78 || Time: 10.705797\n",
      "****************************************************************************************************\n",
      "Epoch: [67/1000] || Step: [0/1563] || Average Training Loss: 2.7602\n",
      "Epoch: [67/1000] || Step: [500/1563] || Average Training Loss: 2.8089\n",
      "Epoch: [67/1000] || Step: [1000/1563] || Average Training Loss: 2.8222\n",
      "Epoch: [67/1000] || Step: [1500/1563] || Average Training Loss: 2.8233\n",
      "Epoch: [67/1000] || Step: [0/188] || Average Validation Loss: 2.7680\n",
      "****************************************************************************************************\n",
      "Epoch: [67/1000] || Training Loss = 2.82 || Validation Loss: 2.78 || Time: 11.556264\n",
      "****************************************************************************************************\n",
      "Epoch: [68/1000] || Step: [0/1563] || Average Training Loss: 2.8191\n",
      "Epoch: [68/1000] || Step: [500/1563] || Average Training Loss: 2.8160\n",
      "Epoch: [68/1000] || Step: [1000/1563] || Average Training Loss: 2.8157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m: encoder_,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m'\u001b[39m: decoder_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m training_loss, validation_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/utils.py:74\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, criterion, optimizer, train_loader, val_loader, total_epoch, device, checkpoint_path, print_every, load_checkpoint)\u001b[0m\n\u001b[1;32m     71\u001b[0m encoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     72\u001b[0m decoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     75\u001b[0m     idx, images, captions \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     76\u001b[0m     images, captions \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), captions\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/get_loader.py:160\u001b[0m, in \u001b[0;36mMSCOCODataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# get X: Image\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# get y: Image Caption\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_deque[idx][\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Image Captioning/INM706-image-captioning/code/get_loader.py:138\u001b[0m, in \u001b[0;36mMSCOCODataset.load_img\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    136\u001b[0m img_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_deque[idx][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# convert the image to RGB to make sure all the images are 3D, because there are some images in grayscale\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_file_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_transforms(img)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder_,\n",
    "    'decoder': decoder_,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
