{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256e51d2",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff24f55",
   "metadata": {},
   "source": [
    "In this notebook, we will run models, also this notebook can be a template to run other models with different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7767e",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a880f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_loader import get_loader\n",
    "from models import Encoder, Decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd4d49",
   "metadata": {},
   "source": [
    "## Load train and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523632e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path = '../../CW/Data/train2017'\n",
    "#captions_path = '../../CW/Data/annotations_trainval2017/annotations/captions_train2017.json'\n",
    "IMAGE_PATH = '../Datasets/coco/images/train2017'\n",
    "CAPTIONS_PATH = '../Datasets/coco/annotations/captions_train2017.json'\n",
    "FREQ_THRESHOLD = 5\n",
    "CAPS_PER_IMAGE = 5\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE = True\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 512  # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1  # hidden layers in LTSM\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 10\n",
    "CHECKPOINT = '../model/model_v1'\n",
    "\n",
    "PRINT_EVERY = 500 # run print_every batches and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0113db3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of training dataloader: 50000, Lenght of testing dataloader: 188\n",
      "Lenght of vocabulary: 3387\n"
     ]
    }
   ],
   "source": [
    "train_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'train',\n",
    "    'idx2word': None,\n",
    "    'word2idx': None\n",
    "}\n",
    "\n",
    "train_loader, train_dataset = get_loader(**train_loader_params)\n",
    "\n",
    "val_loader_params = {\n",
    "    'images_path': IMAGE_PATH,\n",
    "    'captions_path': CAPTIONS_PATH,\n",
    "    'freq_threshold': FREQ_THRESHOLD,\n",
    "    'caps_per_image': 3,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': SHUFFLE,\n",
    "    'mode': 'validation',\n",
    "    'idx2word': train_dataset.vocab.idx2word,\n",
    "    'word2idx': train_dataset.vocab.word2idx\n",
    "}\n",
    "\n",
    "val_loader, val_dataset = get_loader(**val_loader_params)\n",
    "\n",
    "print(f\"Length of training dataloader: {len(train_dataset)}, Length of testing dataloader: {len(val_loader)}\")\n",
    "print(f\"Length of vocabulary: {len(train_dataset.vocab.idx2word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5622d7ab-d04b-4d5e-ae2a-fb323979833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.vocab.export_vocab('../vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e031a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(train_dataset.vocab.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb037c",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a492102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"We are using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b4ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "673155db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':1e-3, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed65aa52-7d2c-434e-b317-e3619bc82a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45158cda",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d2ab890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/10] || Step: [0/1563] || Average Training Loss: 8.1324\n",
      "Epoch: [0/10] || Step: [500/1563] || Average Training Loss: 3.8319\n",
      "Epoch: [0/10] || Step: [1000/1563] || Average Training Loss: 3.5866\n",
      "Epoch: [0/10] || Step: [1500/1563] || Average Training Loss: 3.4698\n",
      "Epoch: [0/10] || Step: [0/188] || Average Validation Loss: 3.2409\n",
      "****************************************************************************************************\n",
      "Epoch: [0/10] || Training Loss = 3.46 || Validation Loss: 3.15 || Time: 14.850750\n",
      "****************************************************************************************************\n",
      "Epoch: [1/10] || Step: [0/1563] || Average Training Loss: 3.0344\n",
      "Epoch: [1/10] || Step: [500/1563] || Average Training Loss: 3.2017\n",
      "Epoch: [1/10] || Step: [1000/1563] || Average Training Loss: 3.1969\n",
      "Epoch: [1/10] || Step: [1500/1563] || Average Training Loss: 3.1911\n",
      "Epoch: [1/10] || Step: [0/188] || Average Validation Loss: 2.9978\n",
      "****************************************************************************************************\n",
      "Epoch: [1/10] || Training Loss = 3.19 || Validation Loss: 3.09 || Time: 15.559770\n",
      "****************************************************************************************************\n",
      "Epoch: [2/10] || Step: [0/1563] || Average Training Loss: 3.2785\n",
      "Epoch: [2/10] || Step: [500/1563] || Average Training Loss: 3.1744\n",
      "Epoch: [2/10] || Step: [1000/1563] || Average Training Loss: 3.1754\n",
      "Epoch: [2/10] || Step: [1500/1563] || Average Training Loss: 3.1781\n",
      "Epoch: [2/10] || Step: [0/188] || Average Validation Loss: 2.9644\n",
      "****************************************************************************************************\n",
      "Epoch: [2/10] || Training Loss = 3.18 || Validation Loss: 3.09 || Time: 14.568631\n",
      "****************************************************************************************************\n",
      "Epoch: [3/10] || Step: [0/1563] || Average Training Loss: 3.5444\n",
      "Epoch: [3/10] || Step: [500/1563] || Average Training Loss: 3.1635\n",
      "Epoch: [3/10] || Step: [1000/1563] || Average Training Loss: 3.1629\n",
      "Epoch: [3/10] || Step: [1500/1563] || Average Training Loss: 3.1609\n",
      "Epoch: [3/10] || Step: [0/188] || Average Validation Loss: 2.9835\n",
      "****************************************************************************************************\n",
      "Epoch: [3/10] || Training Loss = 3.16 || Validation Loss: 3.06 || Time: 11.244599\n",
      "****************************************************************************************************\n",
      "Epoch: [4/10] || Step: [0/1563] || Average Training Loss: 3.2275\n",
      "Epoch: [4/10] || Step: [500/1563] || Average Training Loss: 3.1555\n",
      "Epoch: [4/10] || Step: [1000/1563] || Average Training Loss: 3.1467\n",
      "Epoch: [4/10] || Step: [1500/1563] || Average Training Loss: 3.1431\n",
      "Epoch: [4/10] || Step: [0/188] || Average Validation Loss: 3.2624\n",
      "****************************************************************************************************\n",
      "Epoch: [4/10] || Training Loss = 3.14 || Validation Loss: 3.06 || Time: 12.584878\n",
      "****************************************************************************************************\n",
      "Epoch: [5/10] || Step: [0/1563] || Average Training Loss: 3.2303\n",
      "Epoch: [5/10] || Step: [500/1563] || Average Training Loss: 3.1283\n",
      "Epoch: [5/10] || Step: [1000/1563] || Average Training Loss: 3.1332\n",
      "Epoch: [5/10] || Step: [1500/1563] || Average Training Loss: 3.1327\n",
      "Epoch: [5/10] || Step: [0/188] || Average Validation Loss: 3.2068\n",
      "****************************************************************************************************\n",
      "Epoch: [5/10] || Training Loss = 3.13 || Validation Loss: 3.06 || Time: 11.032440\n",
      "****************************************************************************************************\n",
      "Epoch: [6/10] || Step: [0/1563] || Average Training Loss: 2.8839\n",
      "Epoch: [6/10] || Step: [500/1563] || Average Training Loss: 3.1190\n",
      "Epoch: [6/10] || Step: [1000/1563] || Average Training Loss: 3.1215\n",
      "Epoch: [6/10] || Step: [1500/1563] || Average Training Loss: 3.1245\n",
      "Epoch: [6/10] || Step: [0/188] || Average Validation Loss: 3.1537\n",
      "****************************************************************************************************\n",
      "Epoch: [6/10] || Training Loss = 3.12 || Validation Loss: 3.04 || Time: 14.552815\n",
      "****************************************************************************************************\n",
      "Epoch: [7/10] || Step: [0/1563] || Average Training Loss: 3.3968\n",
      "Epoch: [7/10] || Step: [500/1563] || Average Training Loss: 3.1164\n",
      "Epoch: [7/10] || Step: [1000/1563] || Average Training Loss: 3.1138\n",
      "Epoch: [7/10] || Step: [1500/1563] || Average Training Loss: 3.1165\n",
      "Epoch: [7/10] || Step: [0/188] || Average Validation Loss: 3.0908\n",
      "****************************************************************************************************\n",
      "Epoch: [7/10] || Training Loss = 3.12 || Validation Loss: 3.04 || Time: 12.808037\n",
      "****************************************************************************************************\n",
      "Epoch: [8/10] || Step: [0/1563] || Average Training Loss: 3.2101\n",
      "Epoch: [8/10] || Step: [500/1563] || Average Training Loss: 3.1075\n",
      "Epoch: [8/10] || Step: [1000/1563] || Average Training Loss: 3.1121\n",
      "Epoch: [8/10] || Step: [1500/1563] || Average Training Loss: 3.1156\n",
      "Epoch: [8/10] || Step: [0/188] || Average Validation Loss: 3.1771\n",
      "****************************************************************************************************\n",
      "Epoch: [8/10] || Training Loss = 3.11 || Validation Loss: 3.03 || Time: 11.031873\n",
      "****************************************************************************************************\n",
      "Epoch: [9/10] || Step: [0/1563] || Average Training Loss: 3.1480\n",
      "Epoch: [9/10] || Step: [500/1563] || Average Training Loss: 3.1066\n",
      "Epoch: [9/10] || Step: [1000/1563] || Average Training Loss: 3.1101\n",
      "Epoch: [9/10] || Step: [1500/1563] || Average Training Loss: 3.1075\n",
      "Epoch: [9/10] || Step: [0/188] || Average Validation Loss: 3.1683\n",
      "****************************************************************************************************\n",
      "Epoch: [9/10] || Training Loss = 3.11 || Validation Loss: 3.03 || Time: 11.030580\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0a2c2-8bf7-4a91-9ebb-ffe9601c1ef4",
   "metadata": {},
   "source": [
    "## Try with different hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c88d1-e10a-4b3b-a017-a28e227d509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data loader\n",
    "BATCH_SIZE = 128\n",
    "CAPS_PER_IMAGE = 5 # how many captions for each image to include in data set\n",
    "\n",
    "# for encoder and decoder\n",
    "EMBED_SIZE = 1024 # dimension of vocab embedding vector\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3 #hidden layers in LTSM\n",
    "vocab_size = len(train_dataset.vocab.idx2word)\n",
    "\n",
    "# training parameters\n",
    "TOTAL_EPOCH = 10\n",
    "CHECKPOINT = '../model/model_v2'\n",
    "PRINT_EVERY = 500 # run print_every batches and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3017d8de",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../model/model_v2/model_v2_1_param.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m model_params \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../model/model_v2/model_v2_1_param.json\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m: BATCH_SIZE,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvocab_size\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mlen\u001B[39m(train_dataset\u001B[38;5;241m.\u001B[39mvocab\u001B[38;5;241m.\u001B[39midx2word)\n\u001B[1;32m      8\u001B[0m }\n\u001B[0;32m---> 10\u001B[0m \u001B[43msave_params\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/INM706-image-captioning/code/utils.py:248\u001B[0m, in \u001B[0;36msave_params\u001B[0;34m(path, batch_size, embed_size, hidden_size, num_layers, vocab_size)\u001B[0m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_params\u001B[39m(path, batch_size, embed_size, hidden_size, num_layers, vocab_size):\n\u001B[1;32m    241\u001B[0m     params \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m: batch_size,\n\u001B[1;32m    243\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124membed_size\u001B[39m\u001B[38;5;124m'\u001B[39m: embed_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    246\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvocab_size\u001B[39m\u001B[38;5;124m'\u001B[39m: vocab_size\n\u001B[1;32m    247\u001B[0m     }\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mw\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m outfile:\n\u001B[1;32m    249\u001B[0m         json\u001B[38;5;241m.\u001B[39mdump(params, outfile)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../model/model_v2/model_v2_1_param.json'"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'path': CHECKPOINT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'embed_size': EMBED_SIZE,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'vocab_size': len(train_dataset.vocab.idx2word)\n",
    "}\n",
    "\n",
    "save_params(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67378d-6e62-4b7c-9557-ea91b2f06802",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ = Encoder(embed_size=EMBED_SIZE, pretrained=True)\n",
    "decoder_ = Decoder(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=vocab_size, num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e938c-e9cc-47bd-9290-343304a18e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is a cross entropy loss and ignore the index of <PAD> since it doesn't make any difference\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "# combine the parameters of decoder and encoder\n",
    "params = list(decoder_.parameters()) + list(encoder_.embed.parameters())\n",
    "\n",
    "# Adam optimizer\n",
    "opt_pars = {'lr':3e-4, 'weight_decay':1e-3, 'betas':(0.9, 0.999), 'eps':1e-08}\n",
    "optimizer = optim.Adam(params, **opt_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d3cbe-6ae1-4619-98ac-a3f15e3f7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'encoder': encoder_,\n",
    "    'decoder': decoder_,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'total_epoch': TOTAL_EPOCH,\n",
    "    'device': device,\n",
    "    'checkpoint_path': CHECKPOINT,\n",
    "    'print_every': PRINT_EVERY,\n",
    "    'load_checkpoint': False\n",
    "}\n",
    "\n",
    "training_loss, validation_loss = train(**train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa16cd-fa96-4164-923f-8fedfc4c78d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}